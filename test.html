```html
<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NeRF：将场景表示为用于视图合成的神经辐射场 (中文翻译)</title>
    <style>
        body {
            font-family: 'Times New Roman', serif; /* 模拟学术论文常用字体 */
            line-height: 1.5;
            margin: 0 auto;
            max-width: 8.5in; /* 标准US Letter纸张宽度 */
            padding: 1in; /* 标准页边距 */
            box-sizing: border-box;
            font-size: 10pt; /* 论文常用字体大小 */
            position: relative; /* 用于绝对定位元素 */
        }
        .page-content {
            position: relative;
        }
        .page-header-left {
            position: absolute;
            left: -0.75in; /* 调整到主要内容区域之外 */
            top: 0;
            width: 0.5in;
            text-align: left;
            font-size: 8pt;
            transform: rotate(-90deg);
            transform-origin: top left;
            white-space: nowrap;
            display: flex;
            align-items: center;
            justify-content: flex-start;
        }
        .page-header-right {
            position: absolute;
            right: 0; /* 调整到内容区域的右边缘 */
            top: 0;
            font-size: 8pt;
        }

        h1 {
            font-size: 16pt;
            font-weight: bold;
            text-align: center;
            margin-bottom: 0.2em;
        }
        .authors {
            font-size: 11pt;
            text-align: center;
            margin-top: 0.5em;
            margin-bottom: 0.2em;
            line-height: 1.2;
        }
        .affiliations {
            font-size: 9pt;
            text-align: center;
            margin-bottom: 1.5em;
        }

        .section-heading {
            font-size: 12pt;
            font-weight: bold;
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }

        .abstract-block, .keywords-block {
            margin-bottom: 1.5em;
            font-size: 9pt;
        }
        .abstract-block strong, .keywords-block strong {
            font-weight: bold;
        }

        .interleaved-paragraph {
            margin-bottom: 1em; /* 原文与译文段落对之间的间距 */
        }
        .original-text, .translated-text {
            text-align: justify;
            margin-bottom: 0.5em; /* 原文与译文之间的间距 */
            text-indent: 1.5em; /* 标准段落首行缩进 */
        }
        .original-text p, .translated-text p {
            margin: 0; /* 移除div内部段落的默认边距 */
            text-indent: inherit; /* 继承父级的缩进 */
        }
        .abstract-block .original-text, .abstract-block .translated-text,
        .keywords-block .original-text, .keywords-block .translated-text {
            text-indent: 0; /* 摘要/关键词段落无缩进 */
        }

        .footnote {
            font-size: 8.5pt;
            margin-top: 1.5em;
            text-indent: 0; /* 注释通常不缩进 */
        }
        .footnote p {
            margin: 0;
        }

        /* 作者列表和标号的样式 */
        .authors sup {
            font-size: 0.7em; /* 使上标更小 */
            vertical-align: super; /* 确保上标正确对齐 */
        }
    </style>
</head>
<body>
    <div class="page-content">
        <div class="page-header-left">
            <span>arXiv:2003.08934v2 [cs.CV] 3 Aug 2020</span>
        </div>

        <h1>NeRF: Representing Scenes as<br>Neural Radiance Fields for View Synthesis</h1>

        <div class="authors">
            Ben Mildenhall<sup>1*</sup> Pratul P. Srinivasan<sup>1*</sup> Matthew Tancik<sup>1*</sup><br>
            Jonathan T. Barron<sup>2</sup> Ravi Ramamoorthi<sup>3</sup> Ren Ng<sup>1</sup>
        </div>

        <div class="affiliations">
            <sup>1</sup>UC Berkeley <sup>2</sup>Google Research <sup>3</sup>UC San Diego
        </div>

        <div class="abstract-block">
            <div class="interleaved-paragraph">
                <div class="original-text">
                    <p><b>Abstract.</b> We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, $\phi$)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.</p>
                </div>
                <div class="translated-text">
                    <p><b>摘要.</b> 我们提出了一种方法，通过使用少量输入视图优化底层连续体积场景函数，实现了复杂场景新颖视图合成的最新技术。我们的算法使用一个全连 接（非卷积）深度网络来表示场景，该网络的输入是一个单一的连续5D坐标（空间位置（x，y，z）和观察方向（θ，φ）），输出是该空间位置的体积密度和视相关辐射亮度。我们通过查询摄像机光线上的5D坐标来合成视图，并使用经典的体积渲染技术将输出的颜色和密度投影到图像中。由于体积渲染本身是可微分的，优化我们表示所需的唯一输入是具有已知摄像机姿 态的图像集。我们描述了如何有效地优化神经辐射场，以渲染具有复杂几何和外观的场景的逼真新颖视图，并展示了优于先前神经渲染和视图合成工作的成果。视图合成结果最好以视频 形式查看，因此我们强烈建议读者观看我们的补充视频以进行令人信服的比较。</p>
                </div>
            </div>
        </div>

        <div class="keywords-block">
            <div class="interleaved-paragraph">
                <div class="original-text">
                    <p><b>Keywords:</b> scene representation, view synthesis, image-based rendering, volume rendering, 3D deep learning</p>
                </div>
                <div class="translated-text">
                    <p><b>关键词:</b> 场景表示，视图合成，基于图像的渲染，体积渲染，3D深度学习</p>
                </div>
            </div>
        </div>

        <div class="section-heading">
            1 Introduction
        </div>

        <div class="interleaved-paragraph">
            <div class="original-text">
                <p>In this work, we address the long-standing problem of view synthesis in a new way by directly optimizing parameters of a continuous 5D scene representation to minimize the error of rendering a set of captured images.</p>
            </div>
            <div class="translated-text">
                <p>在这项工作中，我们通过直接优化连续5D场景表示的参数来解决视图合成的长期问题，以最小化渲染一组捕获图像的误差。</p>
            </div>
        </div>

        <div class="interleaved-paragraph">
            <div class="original-text">
                <p>We represent a static scene as a continuous 5D function that outputs the radiance emitted in each direction (θ,φ) at each point (x, y, z) in space, and a density at each point which acts like a differential opacity controlling how much radiance is accumulated by a ray passing through (x, y, z). Our method optimizes a deep fully-connected neural network without any convolutional layers (often referred to as a multilayer perceptron or MLP) to represent this function by regressing from a single 5D coordinate (x, y, z, θ, φ) to a single volume density and view-dependent RGB color. To render this neural radiance field (NeRF)</p>  
            </div>
            <div class="translated-text">
                <p>我们将静态场景表示为一个连续的5D函数，该函数在空间中每个点（x，y，z）的每个方向（θ，φ）输出辐射亮度，并在每个点输出一个密度，该密度充当微分不透 明度，控制穿过（x，y，z）的光线累积的辐射量。我们的方法优化了一个深度全连接神经网络，没有任何卷积层（通常称为多层感知器或MLP），通过从单个5D坐标（x，y，z，θ，φ）回归到一个体积密度和视相关RGB颜色来表示此函数。为了渲染这个神经辐射场（NeRF）</p>
            </div>
        </div>

        <div class="footnote">
            <div class="interleaved-paragraph">
                <div class="original-text">
                    <p>* Authors contributed equally to this work.</p>
                </div>
                <div class="translated-text">
                    <p>* 作者对这项工作贡献相同。</p>
                </div>
            </div>
        </div>

        <!--
            请注意：此HTML输出仅包含所提供的OCR文本内容及其翻译。
            如果原文包含图片，您需要手动将图片文件插入到HTML中，
            并根据原始排版调整<img>标签的位置和样式。
            在此页（第一页）的OCR文本中未检测到图片，但正文提及了图1和图2。
            这些图片应插入在它们实际出现的页面（如原文的第2页和第5页）中。
            为了保持原始排版，可能需要更复杂的CSS（如多列布局）和精确的图片尺寸/位置控制。
        -->
    </div>
</body>
</html>
```