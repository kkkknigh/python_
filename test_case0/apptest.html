<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<style>
  .original { margin-bottom: 5px; }
  .translation { margin-bottom: 20px; color: #2E86C1; }
  h1, h2, h3, h4 { color: #1A5276; }
  table { border-collapse: collapse; margin: 15px 0; }
  th, td { border: 1px solid #ddd; padding: 8px; }
</style>
</head>
<body>

<div class="original">
<h1>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1>
</div>
<div class="translation">
<h1>NeRF：将场景表示为神经辐射场以实现视图合成</h1>
</div>

<div class="original">
<p>Ben Mildenhall<sup>1</sup>*</p>
<p>Pratul P. Srinivasan<sup>1</sup></p>
<p>Matthew Tancik<sup>1</sup></p>
<p>Jonathan T. Barron<sup>2</sup></p>
<p>Ravi Ramamoorthi<sup>3</sup></p>
<p>Ren Ng<sup>1</sup></p>
<p><sup>1</sup>UC Berkeley <sup>2</sup>Google Research <sup>3</sup>UC San Diego</p>
</div>
<div class="translation">
<p>本·米尔登霍尔<sup>1</sup>*</p>
<p>普拉图尔·P·斯里尼瓦桑<sup>1</sup></p>
<p>马修·坦西克<sup>1</sup></p>
<p>乔纳森·T·巴伦<sup>2</sup></p>
<p>拉维·拉玛莫西<sup>3</sup></p>
<p>吴韧<sup>1</sup></p>
<p><sup>1</sup>加州大学伯克利分校 <sup>2</sup>谷歌研究院 <sup>3</sup>加州大学圣地亚哥分校</p>
</div>

<div class="original">
<h4>Abstract</h4>
<p>We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \((x,y,z)\) and viewing direction \((\theta,\phi)\)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.</p>
<p><strong>Keywords:</strong> scene representation, view synthesis, image-based rendering, volume rendering, 3D deep learning</p>
</div>
<div class="translation">
<h4>摘要</h4>
<p>我们提出了一种方法，通过使用一组稀疏的输入视图优化底层连续的体场景函数，在合成复杂场景的新视图方面取得了最先进的结果。我们的算法使用全连接（非卷积）深度网络表示场景，其输入是单个连续的五维坐标（空间位置 \((x,y,z)\) 和观察方向 \((\theta,\phi)\)），输出是该空间位置的体积密度和视角相关的辐射亮度。我们通过沿相机光线查询五维坐标来合成视图，并使用经典的体渲染技术将输出的颜色和密度投影成图像。由于体渲染本质上是可微的，优化我们表示所需的唯一输入是一组已知相机位姿的图像。我们描述了如何有效优化神经辐射场以渲染具有复杂几何形状和外观场景的光照真实新视图，并展示了优于先前神经渲染和视图合成工作的结果。视图合成结果最好以视频形式观看，因此我们强烈建议读者观看补充视频以获得令人信服的比较。</p>
<p><strong>关键词：</strong> 场景表示，视图合成，基于图像的渲染，体渲染，3D深度学习</p>
</div>

<div class="original">
<h2>1 Introduction</h2>
<p>In this work, we address the long-standing problem of view synthesis in a new way by directly optimizing parameters of a continuous 5D scene representation to minimize the error of rendering a set of captured images.</p>
<p>We represent a static scene as a continuous 5D function that outputs the radiance emitted in each direction \((\theta,\phi)\) at each point \((x,y,z)\) in space, and a density at each point which acts like a differential opacity controlling how much radiance is accumulated by a ray passing through \((x,y,z)\). Our method optimizes a deep fully-connected neural network without any convolutional layers (often referred to as a multilayer perceptron or MLP) to represent this function by regressing from a single 5D coordinate \((x,y,z,\theta,\phi)\) to a single volume density and view-dependent RGB color. To render this <em>neural radiance field</em> (NeRF)</p>
</div>
<div class="translation">
<h2>1 引言</h2>
<p>在这项工作中，我们通过直接优化连续五维场景表示的参数来最小化渲染一组捕获图像的误差，以一种新方式解决了视图合成这个长期存在的问题。</p>
<p>我们将静态场景表示为一个连续的五维函数，该函数输出空间中每个点 \((x,y,z)\) 沿每个方向 \((\theta,\phi)\) 发出的辐射亮度，以及每点的密度（类似于微分不透明度，控制穿过 \((x,y,z)\) 的光线累积多少辐射亮度）。我们的方法优化一个没有任何卷积层的深度全连接神经网络（通常称为多层感知器或MLP）来表示此函数，它从单个五维坐标 \((x,y,z,\theta,\phi)\) 回归出单个体积密度和视角相关的RGB颜色。为了渲染这个<em>神经辐射场</em>（NeRF）</p>
</div>

<div class="original">
<p>2 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<p><strong>Input Images</strong></p>
<p><strong>Optimize NeRF</strong></p>
<p><strong>Render new views</strong></p>
<p><strong>Fig. 1:</strong> We present a method that optimizes a continuous 5D neural radiance field representation (volume density and view-dependent color at any continuous location) of a scene from a set of input images. We use techniques from volume rendering to accumulate samples of this scene representation along rays to render the scene from any viewpoint. Here, we visualize the set of 100 input views of the synthetic Drums scene randomly captured on a surrounding hemisphere, and we show two novel views rendered from our optimized NeRF representation.</p>
</div>
<div class="translation">
<p>2 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<p><strong>输入图像</strong></p>
<p><strong>优化 NeRF</strong></p>
<p><strong>渲染新视图</strong></p>
<p><strong>图 1：</strong> 我们提出了一种方法，从一组输入图像中优化场景的连续五维神经辐射场表示（任何连续位置的体积密度和视角相关颜色）。我们使用体渲染技术沿光线累积此场景表示的样本，以从任意视点渲染场景。这里，我们可视化了在周围半球上随机捕获的合成Drums场景的100个输入视图集，并展示了从我们优化后的NeRF表示渲染出的两个新视图。</p>
</div>

<div class="original">
<p>from a particular viewpoint we: 1) march camera rays through the scene to generate a sampled set of 3D points, 2) use those points and their corresponding 2D viewing directions as input to the neural network to produce an output set of colors and densities, and 3) use classical volume rendering techniques to accumulate those colors and densities into a 2D image. Because this process is naturally differentiable, we can use gradient descent to optimize this model by minimizing the error between each observed image and the corresponding views rendered from our representation. Minimizing this error across multiple views encourages the network to predict a coherent model of the scene by assigning high volume densities and accurate colors to the locations that contain the true underlying scene content. Figure 2 visualizes this overall pipeline.</p>
</div>
<div class="translation">
<p>从特定视点出发，我们：1）在场景中步进相机光线以生成采样的3D点集，2）使用这些点及其对应的2D观察方向作为神经网络的输入，生成颜色和密度输出集，3）使用经典体渲染技术将这些颜色和密度累积成2D图像。由于这个过程本质上是可微的，我们可以使用梯度下降法通过最小化每个观测图像与从我们表示中渲染出的对应视图之间的误差来优化此模型。在多个视图上最小化此误差，促使网络通过将高体积密度和准确颜色分配给包含真实底层场景内容的位置来预测场景的一致模型。图2可视化了这个整体流程。</p>
</div>

<div class="original">
<p>We find that the basic implementation of optimizing a neural radiance field representation for a complex scene does not converge to a sufficiently high-resolution representation and is inefficient in the required number of samples per camera ray. We address these issues by transforming input 5D coordinates with a positional encoding that enables the MLP to represent higher frequency functions, and we propose a hierarchical sampling procedure to reduce the number of queries required to adequately sample this high-frequency scene representation.</p>
</div>
<div class="translation">
<p>我们发现，针对复杂场景优化神经辐射场表示的基本实现无法收敛到足够高分辨率的表示，并且在每条相机光线所需的样本数量上效率低下。我们通过使用位置编码转换输入的五维坐标来解决这些问题，该编码使MLP能够表示更高频的函数，并提出一种分层采样程序以减少充分采样此高频场景表示所需的查询次数。</p>
</div>

<div class="original">
<p>Our approach inherits the benefits of volumetric representations: both can represent complex real-world geometry and appearance and are well suited for gradient-based optimization using projected images. Crucially, our method overcomes the prohibitive storage costs of discretized voxel grids when modeling complex scenes at high-resolutions. In summary, our technical contributions are:</p>
<ul>
<li>An approach for representing continuous scenes with complex geometry and materials as 5D neural radiance fields, parameterized as basic MLP networks.</li>
<li>A differentiable rendering procedure based on classical volume rendering techniques, which we use to optimize these representations from standard RGB images. This includes a hierarchical sampling strategy to allocate the MLP’s capacity towards space with visible scene content.</li>
</ul>
</div>
<div class="translation">
<p>我们的方法继承了体表示的优势：两者都能表示复杂的真实世界几何形状和外观，并且非常适合使用投影图像进行基于梯度的优化。关键的是，我们的方法克服了在高分辨率建模复杂场景时离散体素网格的过高存储成本。总之，我们的技术贡献是：</p>
<ul>
<li>一种将具有复杂几何形状和材质的连续场景表示为五维神经辐射场的方法，参数化为基本的MLP网络。</li>
<li>一种基于经典体渲染技术的可微渲染流程，我们用它从标准RGB图像优化这些表示。这包括一种分层采样策略，用于将MLP的能力分配给具有可见场景内容的空间。</li>
</ul>
</div>

<div class="original">
<p>- A positional encoding to map each input 5D coordinate into a higher dimensional space, which enables us to successfully optimize neural radiance fields to represent high-frequency scene content.</p>
</div>
<div class="translation">
<p>- 一种位置编码，将每个输入的五维坐标映射到更高维空间，使我们能够成功优化神经辐射场以表示高频场景内容。</p>
</div>

<div class="original">
<p>We demonstrate that our resulting neural radiance field method quantitatively and qualitatively outperforms state-of-the-art view synthesis methods, including works that fit neural 3D representations to scenes as well as works that train deep convolutional networks to predict sampled volumetric representations. As far as we know, this paper presents the first continuous neural scene representation that is able to render high-resolution photorealistic novel views of real objects and scenes from RGB images captured in natural settings.</p>
</div>
<div class="translation">
<p>我们证明，我们得到的神经辐射场方法在定量和定性上都优于最先进的视图合成方法，包括将神经3D表示拟合到场景的工作以及训练深度卷积网络以预测采样体表示的工作。据我们所知，本文提出了第一个连续的神经场景表示，能够从自然环境中捕获的RGB图像渲染真实物体和场景的高分辨率光照真实新视图。</p>
</div>

<div class="original">
<h2>2 Related Work</h2>
<p>A promising recent direction in computer vision is encoding objects and scenes in the weights of an MLP that directly maps from a 3D spatial location to an implicit representation of the shape, such as the signed distance [6] at that location. However, these methods have so far been unable to reproduce realistic scenes with complex geometry with the same fidelity as techniques that represent scenes using discrete representations such as triangle meshes or voxel grids. In this section, we review these two lines of work and contrast them with our approach, which enhances the capabilities of neural scene representations to produce state-of-the-art results for rendering complex realistic scenes.</p>
</div>
<div class="translation">
<h2>2 相关工作</h2>
<p>计算机视觉中一个有前景的最新方向是将物体和场景编码在MLP的权重中，该MLP直接从3D空间位置映射到形状的隐式表示，例如该位置的有向距离[6]。然而，这些方法迄今无法以与使用三角形网格或体素网格等离散表示来表示场景的技术相同的保真度再现具有复杂几何形状的真实场景。在本节中，我们回顾了这两类工作，并将其与我们的方法进行对比，我们的方法增强了神经场景表示的能力，以产生渲染复杂真实场景的最新结果。</p>
</div>

<div class="original">
<p>A similar approach of using MLPs to map from low-dimensional coordinates to colors has also been used for representing other graphics functions such as images [44], textured materials [36, 37, 12, 31], and indirect illumination values [38].</p>
</div>
<div class="translation">
<p>使用MLP从低维坐标映射到颜色的类似方法也被用于表示其他图形函数，如图像[44]、纹理材质[36, 37, 12, 31]和间接光照值[38]。</p>
</div>

<div class="original">
<p><strong>Neural 3D shape representations</strong> Recent work has investigated the implicit representation of continuous 3D shapes as level sets by optimizing deep networks that map \(xyz\) coordinates to signed distance functions [32, 15] or occupancy fields [27, 11]. However, these models are limited by their requirement of access to ground truth 3D geometry, typically obtained from synthetic 3D shape datasets such as ShapeNet [3]. Subsequent work has relaxed this requirement of ground truth 3D shapes by formulating differentiable rendering functions that allow neural implicit shape representations to be optimized using only 2D images. Niemeyer <em>et al.</em> [29] represent surfaces as 3D occupancy fields and use a numerical method to find the surface intersection for each ray, then calculate an exact derivative using implicit differentiation. Each ray intersection location is provided as the input to a neural 3D texture field that predicts a diffuse color for that point. Sitzmann <em>et al.</em> [42] use a less direct neural 3D representation that simply outputs a feature vector and RGB color at each continuous 3D coordinate, and propose a differentiable rendering function consisting of a recurrent neural network that marches along each ray to decide where the surface is located.</p>
</div>
<div class="translation">
<p><strong>神经3D形状表示</strong> 最近的工作研究了通过优化将\(xyz\)坐标映射到有向距离函数[32, 15]或占据场[27, 11]的深度网络，将连续3D形状隐式表示为水平集。然而，这些模型受限于它们需要访问真实3D几何形状的要求，这通常从合成3D形状数据集（如ShapeNet[3]）获得。后续工作通过制定可微渲染函数放宽了对真实3D形状的要求，使得神经隐式形状表示可以仅使用2D图像进行优化。Niemeyer <em>等人</em> [29] 将表面表示为3D占据场，并使用数值方法查找每条光线的表面交点，然后使用隐式微分计算精确导数。每个光线交点位置作为神经3D纹理场的输入，该场预测该点的漫反射颜色。Sitzmann <em>等人</em> [42] 使用一种不那么直接的神经3D表示，它仅在每个连续3D坐标处输出一个特征向量和RGB颜色，并提出一个由循环神经网络组成的可微渲染函数，该网络沿每条光线步进以确定表面位置。</p>
</div>

<div class="original">
<p>Though these techniques can potentially represent complicated and high-resolution geometry, they have so far been limited to simple shapes with low geometric complexity, resulting in oversmoothed renderings. We show that an alternate strategy of optimizing networks to encode 5D radiance fields (3D volumes</p>
</div>
<div class="translation">
<p>尽管这些技术有潜力表示复杂和高分辨率的几何形状，但迄今为止它们仅限于几何复杂度低的简单形状，导致渲染结果过度平滑。我们展示了一种替代策略，即优化网络以编码五维辐射场（3D体积</p>
</div>

<div class="original">
<p>with 2D view-dependent appearance) can represent higher-resolution geometry and appearance to render photorealistic novel views of complex scenes.</p>
</div>
<div class="translation">
<p>具有2D视角相关外观），可以表示更高分辨率的几何形状和外观，以渲染复杂场景的光照真实新视图。</p>
</div>

<div class="original">
<p><strong>View synthesis and image-based rendering</strong> Given a dense sampling of views, photorealistic novel views can be reconstructed by simple light field sample interpolation techniques [21, 5, 7]. For novel view synthesis with sparser view sampling, the computer vision and graphics communities have made significant progress by predicting traditional geometry and appearance representations from observed images. One popular class of approaches uses mesh-based representations of scenes with either diffuse [48] or view-dependent [2, 8, 49] appearance. Differentiable rasterizers [4, 10, 23, 25] or pathtracers [22, 30] can directly optimize mesh representations to reproduce a set of input images using gradient descent. However, gradient-based mesh optimization based on image reprojection is often difficult, likely because of local minima or poor conditioning of the loss landscape. Furthermore, this strategy requires a template mesh with fixed topology to be provided as an initialization before optimization [22], which is typically unavailable for unconstrained real-world scenes.</p>
</div>
<div class="translation">
<p><strong>视图合成与基于图像的渲染</strong> 给定密集采样的视图，可以通过简单的光场样本插值技术[21, 5, 7]重建光照真实的新视图。对于视图采样更稀疏的新视图合成，计算机视觉和图形学界通过从观测图像预测传统几何和外观表示取得了显著进展。一类流行的方法使用基于网格的场景表示，具有漫反射[48]或视角相关[2, 8, 49]的外观。可微分光栅化器[4, 10, 23, 25]或路径追踪器[22, 30]可以直接优化网格表示，以使用梯度下降法重现一组输入图像。然而，基于图像重投影的梯度网格优化通常很困难，可能是由于局部最小值或损失景观的条件不良。此外，此策略需要在优化前提供具有固定拓扑的模板网格作为初始化[22]，这对于无约束的真实世界场景通常不可用。</p>
</div>

<div class="original">
<p>Another class of methods use volumetric representations to address the task of high-quality photorealistic view synthesis from a set of input RGB images. Volumetric approaches are able to realistically represent complex shapes and materials, are well-suited for gradient-based optimization, and tend to produce less visually distracting artifacts than mesh-based methods. Early volumetric approaches used observed images to directly color voxel grids [19, 40, 45]. More recently, several methods [9, 13, 17, 28, 33, 43, 46, 52] have used large datasets of multiple scenes to train deep networks that predict a sampled volumetric representation from a set of input images, and then use either alpha-compositing [34] or learned compositing along rays to render novel views at test time. Other works have optimized a combination of convolutional networks (CNNs) and sampled voxel grids for each specific scene, such that the CNN can compensate for discretization artifacts from low resolution voxel grids [41] or allow the predicted voxel grids to vary based on input time or animation controls [24]. While these volumetric techniques have achieved impressive results for novel view synthesis, their ability to scale to higher resolution imagery is fundamentally limited by poor time and space complexity due to their discrete sampling -- rendering higher resolution images requires a finer sampling of 3D space. We circumvent this problem by instead encoding a <em>continuous</em> volume within the parameters of a deep fully-connected neural network, which not only produces significantly higher quality renderings than prior volumetric approaches, but also requires just a fraction of the storage cost of those <em>sampled</em> volumetric representations.</p>
</div>
<div class="translation">
<p>另一类方法使用体表示来处理从一组输入RGB图像进行高质量光照真实视图合成的任务。体方法能够真实地表示复杂的形状和材质，非常适合基于梯度的优化，并且往往比基于网格的方法产生更少视觉干扰的伪影。早期的体方法使用观测图像直接为体素网格着色[19, 40, 45]。最近，一些方法[9, 13, 17, 28, 33, 43, 46, 52]使用多个场景的大型数据集训练深度网络，从一组输入图像预测采样的体表示，然后在测试时使用alpha合成[34]或沿光线学习的合成来渲染新视图。其他工作针对每个特定场景优化了卷积网络（CNN）和采样体素网格的组合，使得CNN可以补偿低分辨率体素网格的离散化伪影[41]，或允许预测的体素网格根据输入时间或动画控制而变化[24]。虽然这些体技术在视图合成方面取得了令人印象深刻的结果，但它们扩展到更高分辨率图像的能力从根本上受到离散采样导致的较差时间和空间复杂度的限制——渲染更高分辨率的图像需要对3D空间进行更精细的采样。我们通过在深度全连接神经网络的参数内编码一个<em>连续</em>体积来规避这个问题，这不仅产生了比先前体方法显著更高质量的渲染，而且只需要这些<em>采样</em>体表示的一小部分存储成本。</p>
</div>

<div class="original">
<h2>3 Neural Radiance Field Scene Representation</h2>
<p>We represent a continuous scene as a 5D vector-valued function whose input is a 3D location \(\mathbf{x}=(x,y,z)\) and 2D viewing direction \((\theta,\phi)\), and whose output is an emitted color \(\mathbf{c}=(r,g,b)\) and volume density \(\sigma\). In practice, we express</p>
</div>
<div class="translation">
<h2>3 神经辐射场场景表示</h2>
<p>我们将连续场景表示为一个五维向量值函数，其输入是三维位置 \(\mathbf{x}=(x,y,z)\) 和二维观察方向 \((\theta,\phi)\)，输出是发出的颜色 \(\mathbf{c}=(r,g,b)\) 和体积密度 \(\sigma\)。在实践中，我们将</p>
</div>

<div class="original">
<h1>NerF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1>
<p><strong>5D Input Position + Direction</strong></p>
<p><strong>Output Color + Density</strong></p>
<p><strong>Volume Rendering Rendering Loss</strong></p>
<p><strong>(a) (b) (c) (d)</strong></p>
<p><strong>Fig. 2:</strong> An overview of our neural radiance field scene representation and differentiable rendering procedure. We synthesize images by sampling 5D coordinates (location and viewing direction) along camera rays (a), feeding those locations into an MLP to produce a color and volume density (b), and using volume rendering techniques to composite these values into an image (c). This rendering function is differentiable, so we can optimize our scene representation by minimizing the residual between synthesized and ground truth observed images (d).</p>
</div>
<div class="translation">
<h1>NeRF：将场景表示为神经辐射场以实现视图合成</h1>
<p><strong>五维输入位置 + 方向</strong></p>
<p><strong>输出颜色 + 密度</strong></p>
<p><strong>体渲染 渲染损失</strong></p>
<p><strong>(a) (b) (c) (d)</strong></p>
<p><strong>图 2：</strong> 我们的神经辐射场场景表示和可微渲染流程概述。我们通过沿相机光线(a)采样五维坐标（位置和观察方向）来合成图像，将这些位置输入MLP以产生颜色和体积密度(b)，并使用体渲染技术将这些值合成到图像中(c)。此渲染函数是可微的，因此我们可以通过最小化合成图像与真实观测图像(d)之间的残差来优化场景表示。</p>
</div>

<div class="original">
<p>direction as a 3D Cartesian unit vector \(\mathbf{d}\). We approximate this continuous 5D scene representation with an MLP network \(F_{\Theta} : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)\) and optimize its weights \(\Theta\) to map from each input 5D coordinate to its corresponding volume density and directional emitted color.</p>
</div>
<div class="translation">
<p>方向表示为三维笛卡尔单位向量 \(\mathbf{d}\)。我们使用MLP网络 \(F_{\Theta} : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)\) 来近似这个连续的五维场景表示，并优化其权重 \(\Theta\) 以将每个输入的五维坐标映射到其对应的体积密度和方向发射颜色。</p>
</div>

<div class="original">
<p>We encourage the representation to be multiview consistent by restricting the network to predict the volume density \(\sigma\) as a function of only the location \(\mathbf{x}\), while allowing the RGB color \(\mathbf{c}\) to be predicted as a function of both location and viewing direction. To accomplish this, the MLP \(F_{\Theta}\) first processes the input 3D coordinate \(\mathbf{x}\) with 8 fully-connected layers (using ReLU activations and 256 channels per layer), and outputs \(\sigma\) and a 256-dimensional feature vector. This feature vector is then concatenated with the camera ray’s viewing direction and passed to one additional fully-connected layer (using a ReLU activation and 128 channels) that output the view-dependent RGB color.</p>
</div>
<div class="translation">
<p>我们通过限制网络仅根据位置 \(\mathbf{x}\) 预测体积密度 \(\sigma\)，同时允许RGB颜色 \(\mathbf{c}\) 根据位置和观察方向进行预测，来鼓励表示具有多视图一致性。为实现这一点，MLP \(F_{\Theta}\) 首先使用8个全连接层（使用ReLU激活函数，每层256个通道）处理输入的三维坐标 \(\mathbf{x}\)，并输出 \(\sigma\) 和一个256维特征向量。然后将此特征向量与相机光线的观察方向拼接，并传递到一个额外的全连接层（使用ReLU激活函数和128个通道），该层输出视角相关的RGB颜色。</p>
</div>

<div class="original">
<p>See Fig. 3 for an example of how our method uses the input viewing direction to represent non-Lambertian effects. As shown in Fig. 4, a model trained without view dependence (only \(\mathbf{x}\) as input) has difficulty representing specularities.</p>
</div>
<div class="translation">
<p>关于我们的方法如何使用输入观察方向来表示非朗伯效应的示例，请参见图3。如图4所示，在没有视角依赖（仅 \(\mathbf{x}\) 作为输入）的情况下训练的模型难以表示镜面反射。</p>
</div>

<div class="original">
<h2>4 Volume Rendering with Radiance Fields</h2>
<p>Our 5D neural radiance field represents a scene as the volume density and directional emitted radiance at any point in space. We render the color of any ray passing through the scene using principles from classical volume rendering [16]. The volume density \(\sigma(\mathbf{x})\) can be interpreted as the differential probability of a ray terminating at an infinitesimal particle at location \(\mathbf{x}\). The expected color \(C(\mathbf{r})\) of camera ray \(\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}\) with near and far bounds \(t_n\) and \(t_f\) is:</p>
<p>\[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt,\]</p>
<p>where \(T(t) = \exp\left(-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds\right)\). (1)</p>
</div>
<div class="translation">
<h2>4 使用辐射场进行体渲染</h2>
<p>我们的五维神经辐射场将场景表示为空间中任意点的体积密度和方向发射辐射亮度。我们使用经典体渲染[16]的原理来渲染穿过场景的任何光线的颜色。体积密度 \(\sigma(\mathbf{x})\) 可以解释为光线在位置 \(\mathbf{x}\) 处的无穷小粒子处终止的微分概率。具有近边界 \(t_n\) 和远边界 \(t_f\) 的相机光线 \(\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}\) 的期望颜色 \(C(\mathbf{r})\) 为：</p>
<p>\[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt,\]</p>
<p>其中 \(T(t) = \exp\left(-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds\right)\)。 (1)</p>
</div>

<div class="original">
<p>6 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<p><strong>(a) View 1 (b) View 2 (c) Radiance Distributions</strong></p>
<p><strong>Fig. 3:</strong> A visualization of view-dependent emitted radiance. Our neural radiance field representation outputs RGB color as a 5D function of both spatial position \(\mathbf{x}\) and viewing direction \(\mathbf{d}\). Here, we visualize example directional color distributions for two spatial locations in our neural representation of the <em>Ship</em> scene. In (a) and (b), we show the appearance of two fixed 3D points from two different camera positions: one on the side of the ship (orange insets) and one on the surface of the water (blue insets). Our method predicts the changing specular appearance of these two 3D points, and in (c) we show how this behavior generalizes continuously across the whole hemisphere of viewing directions.</p>
</div>
<div class="translation">
<p>6 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<p><strong>(a) 视图1 (b) 视图2 (c) 辐射分布</strong></p>
<p><strong>图 3：</strong> 视角相关发射辐射亮度的可视化。我们的神经辐射场表示将RGB颜色输出为空间位置 \(\mathbf{x}\) 和观察方向 \(\mathbf{d}\) 的五维函数。这里，我们可视化了我们神经表示的<em>Ship</em>场景中两个空间位置的示例方向颜色分布。在(a)和(b)中，我们展示了从两个不同相机位置观察到的两个固定3D点的外观：一个在船的侧面（橙色插图），一个在水面上（蓝色插图）。我们的方法预测了这两个3D点变化的镜面外观，并在(c)中展示了这种行为如何在观察方向的整个半球上连续泛化。</p>
</div>

<div class="original">
<p>The function \(T(t)\) denotes the accumulated transmittance along the ray from \(t_n\) to \(t\), i.e., the probability that the ray travels from \(t_n\) to \(t\) without hitting any other particle. Rendering a view from our continuous neural radiance field requires estimating this integral \(C(\mathbf{r})\) for a camera ray traced through each pixel of the desired virtual camera.</p>
</div>
<div class="translation">
<p>函数 \(T(t)\) 表示沿光线从 \(t_n\) 到 \(t\) 的累积透射率，即光线从 \(t_n\) 传播到 \(t\) 而不撞击任何其他粒子的概率。从我们的连续神经辐射场渲染视图需要为穿过所需虚拟相机每个像素的光线估计此积分 \(C(\mathbf{r})\)。</p>
</div>

<div class="original">
<p>We numerically estimate this continuous integral using quadrature. Deterministic quadrature, which is typically used for rendering discretized voxel grids, would effectively limit our representation’s resolution because the MLP would only be queried at a fixed discrete set of locations. Instead, we use a stratified sampling approach where we partition \([t_n,t_f]\) into \(N\) evenly-spaced bins and then draw one sample uniformly at random from within each bin:</p>
<p>\[t_i \sim U\left[t_n + \frac{i-1}{N}(t_f - t_n), \, t_n + \frac{i}{N}(t_f - t_n)\right].\]</p>
</div>
<div class="translation">
<p>我们使用求积法数值估计此连续积分。确定性求积法通常用于渲染离散体素网格，它会有效限制我们表示的分辨率，因为MLP只会在固定的离散位置集上被查询。相反，我们使用分层采样方法，将 \([t_n,t_f]\) 划分为 \(N\) 个等间距的区间，然后从每个区间内均匀随机抽取一个样本：</p>
<p>\[t_i \sim U\left[t_n + \frac{i-1}{N}(t_f - t_n), \, t_n + \frac{i}{N}(t_f - t_n)\right].\]</p>
</div>

<div class="original">
<p>Although we use a discrete set of samples to estimate the integral, stratified sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization. We use these samples to estimate \(C(\mathbf{r})\) with the quadrature rule discussed in the volume rendering review by Max [26]:</p>
<p>\[\hat{C}(\mathbf{r}) = \sum_{i=1}^{N}T_i(1-\exp(-\sigma_i\delta_i))c_i, \quad \text{where } T_i = \exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right),\]</p>
<p>where \(\delta_i = t_{i+1} - t_i\) is the distance between adjacent samples. This function for calculating \(\hat{C}(\mathbf{r})\) from the set of \((c_i,\sigma_i)\) values is trivially differentiable and reduces to traditional alpha compositing with alpha values \(\alpha_i = 1 - \exp(-\sigma_i\delta_i)\).</p>
</div>
<div class="translation">
<p>尽管我们使用离散样本集来估计积分，但分层采样使我们能够表示连续的场景，因为它导致MLP在优化过程中在连续位置被评估。我们使用这些样本，采用Max在体渲染综述[26]中讨论的求积法则来估计 \(C(\mathbf{r})\)：</p>
<p>\[\hat{C}(\mathbf{r}) = \sum_{i=1}^{N}T_i(1-\exp(-\sigma_i\delta_i))c_i, \quad \text{其中 } T_i = \exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right),\]</p>
<p>其中 \(\delta_i = t_{i+1} - t_i\) 是相邻样本之间的距离。这个根据 \((c_i,\sigma_i)\) 值集合计算 \(\hat{C}(\mathbf{r})\) 的函数是平凡可微的，并且简化为具有alpha值 \(\alpha_i = 1 - \exp(-\sigma_i\delta_i)\) 的传统alpha合成。</p>
</div>

<div class="original">
<h2>5 Optimizing a Neural Radiance Field</h2>
<p>In the previous section we have described the core components necessary for modeling a scene as a neural radiance field and rendering novel views from this representation. However, we observe that these components are not sufficient for achieving state-of-the-art quality, as demonstrated in Section 6.4). We introduce two improvements to enable representing high-resolution complex scenes. The first is a positional encoding of the input coordinates that assists the MLP in representing high-frequency functions, and the second is a hierarchical sampling procedure that allows us to efficiently sample this high-frequency representation.</p>
</div>
<div class="translation">
<h2>5 优化神经辐射场</h2>
<p>在上一节中，我们描述了将场景建模为神经辐射场并从此表示渲染新视图所需的核心组件。然而，我们观察到这些组件不足以实现最先进的质量，如第6.4节所示。我们引入了两项改进以实现高分辨率复杂场景的表示。第一项是输入坐标的位置编码，帮助MLP表示高频函数；第二项是分层采样程序，使我们能够有效地采样此高频表示。</p>
</div>

<div class="original">
<h3>Positional encoding</h3>
<p>Despite the fact that neural networks are universal function approximators [14], we found that having the network \(F_{\Theta}\) directly operate on \(xyz\theta\phi\) input coordinates results in renderings that perform poorly at representing high-frequency variation in color and geometry. This is consistent with recent work by Rahaman <em>et al.</em> [35], which shows that deep networks are biased towards learning lower frequency functions. They additionally show that mapping the inputs to a higher dimensional space using high frequency functions before passing them to the network enables better fitting of data that contains high frequency variation.</p>
</div>
<div class="translation">
<h3>位置编码</h3>
<p>尽管神经网络是通用函数逼近器[14]，但我们发现让网络 \(F_{\Theta}\) 直接在 \(xyz\theta\phi\) 输入坐标上操作会导致渲染结果在表示颜色和几何的高频变化方面表现不佳。这与Rahaman <em>等人</em> [35] 最近的工作一致，该工作表明深度网络偏向于学习低频函数。他们还表明，在将输入传递给网络之前，使用高频函数将输入映射到更高维空间，能够更好地拟合包含高频变化的数据。</p>
</div>

<div class="original">
<p>We leverage these findings in the context of neural scene representations, and show that reformulating \(F_{\Theta}\) as a composition of two functions \(F_{\Theta}=F^{\prime}_{\Theta}\circ\gamma\), one learned and one not, significantly improves performance (see Fig. 4 and Table 2). Here \(\gamma\) is a mapping from \(\mathbb{R}\) into a higher dimensional space \(\mathbb{R}^{2L}\), and \(F^{\prime}_{\Theta}\) is still simply a regular MLP. Formally, the encoding function we use is:</p>
<p>\[\gamma(p)=\left(\sin\left(2^{0}\pi p\right),\cos\left(2^{0}\pi p\right),\cdots ,\sin\left(2^{L-1}\pi p\right),\cos\left(2^{L-1}\pi p\right)\right).\] (4)</p>
</div>
<div class="translation">
<p>我们在神经场景表示的背景下利用这些发现，并表明将 \(F_{\Theta}\) 重新表述为两个函数 \(F_{\Theta}=F^{\prime}_{\Theta}\circ\gamma\) 的复合（一个可学习，一个不可学习）可显著提高性能（见图4和表2）。这里 \(\gamma\) 是从 \(\mathbb{R}\) 到更高维空间 \(\mathbb{R}^{2L}\) 的映射，而 \(F^{\prime}_{\Theta}\) 仍然只是一个常规的MLP。形式上，我们使用的编码函数是：</p>
<p>\[\gamma(p)=\left(\sin\left(2^{0}\pi p\right),\cos\left(2^{0}\pi p\right),\cdots ,\sin\left(2^{L-1}\pi p\right),\cos\left(2^{L-1}\pi p\right)\right).\] (4)</p>
</div>

<div class="original">
<p>This function \(\gamma(\cdot)\) is applied separately to each of the three coordinate values in \(\mathbf{x}\) (which are normalized to lie in \([-1,1]\)) and to the three components of the</p>
</div>
<div class="translation">
<p>此函数 \(\gamma(\cdot)\) 分别应用于 \(\mathbf{x}\) 中的三个坐标值（它们被归一化到 \([-1,1]\) 范围内）以及</p>
</div>

<div class="original">
<strong>Figure 4:</strong> Here we visualize how our full model benefits from representing view-dependent emitted radiance and from passing our input coordinates through a high-frequency positional encoding. Removing view dependence prevents the model from recreating the specular reflection on the bulldozer tread. Removing the positional encoding drastically decreases the model’s ability to represent high frequency geometry and texture, resulting in an oversmoothed appearance.
</div>
<div class="translation">
<strong>图 4：</strong> 这里我们可视化了完整模型如何受益于表示视角相关发射辐射亮度以及通过高频位置编码传递输入坐标。移除视角依赖会阻止模型重建推土机履带上的镜面反射。移除位置编码会大幅降低模型表示高频几何和纹理的能力，导致外观过度平滑。
</div>

<div class="original">
<p>Cartesian viewing direction unit vector \(\mathbf{d}\) (which by construction lie in \([-1,1]\)). In our experiments, we set \(L=10\) for \(\gamma(\mathbf{x})\) and \(L=4\) for \(\gamma(\mathbf{d})\).</p>
</div>
<div class="translation">
<p>笛卡尔观察方向单位向量 \(\mathbf{d}\) 的三个分量（根据构造位于 \([-1,1]\) 内）。在我们的实验中，对于 \(\gamma(\mathbf{x})\) 我们设置 \(L=10\)，对于 \(\gamma(\mathbf{d})\) 设置 \(L=4\)。</p>
</div>

<div class="original">
<p>A similar mapping is used in the popular Transformer architecture [47], where it is referred to as a <em>positional encoding</em>. However, Transformers use it for a different goal of providing the discrete positions of tokens in a sequence as input to an architecture that does not contain any notion of order. In contrast, we use these functions to map continuous input coordinates into a higher dimensional space to enable our MLP to more easily approximate a higher frequency function. Concurrent work on a related problem of modeling 3D protein structure from projections [51] also utilizes a similar input coordinate mapping.</p>
</div>
<div class="translation">
<p>流行的Transformer架构[47]中使用了类似的映射，在那里它被称为<em>位置编码</em>。然而，Transformer使用它是为了不同的目的：为序列中标记的离散位置提供输入给一个不包含任何顺序概念的架构。相比之下，我们使用这些函数将连续输入坐标映射到更高维空间，使我们的MLP更容易逼近更高频的函数。在从投影建模3D蛋白质结构的相关问题上，同时期的工作[51]也利用了类似的输入坐标映射。</p>
</div>

<div class="original">
<h3>Hierarchical volume sampling</h3>
<p>Our rendering strategy of densely evaluating the neural radiance field network at \(N\) query points along each camera ray is inefficient: free space and occluded regions that do not contribute to the rendered image are still sampled repeatedly. We draw inspiration from early work in volume rendering [20] and propose a hierarchical representation that increases rendering efficiency by allocating samples proportionally to their expected effect on the final rendering.</p>
</div>
<div class="translation">
<h3>分层体采样</h3>
<p>我们在每条相机光线上的 \(N\) 个查询点密集评估神经辐射场网络的渲染策略是低效的：对渲染图像没有贡献的自由空间和被遮挡区域仍然被重复采样。我们从体渲染的早期工作[20]中汲取灵感，提出了一种分层表示，通过按样本对最终渲染的预期影响比例分配样本，来提高渲染效率。</p>
</div>

<div class="original">
<p>Instead of just using a single network to represent the scene, we simultaneously optimize two networks: one "coarse" and one "fine". We first sample a set of \(N_{c}\) locations using stratified sampling, and evaluate the "coarse" network at these locations as described in Eqns. 2 and 3. Given the output of this "coarse" network, we then produce a more informed sampling of points along each ray where samples are biased towards the relevant parts of the volume. To do this, we first rewrite the alpha composited color from the coarse network \(\hat{C}_{c}(\mathbf{r})\) in Eqn. 3 as a weighted sum of all sampled colors \(c_{i}\) along the ray:</p>
<p>\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}}w_{i}c_{i}\,,\quad\ w_{i}=T_{i}(1-\exp( -\sigma_{i}\delta_{i}))\,.\] (5)</p>
</div>
<div class="translation">
<p>我们不是仅使用单个网络来表示场景，而是同时优化两个网络：一个“粗糙”网络和一个“精细”网络。我们首先使用分层采样抽取一组 \(N_{c}\) 个位置，并如公式2和3所述在这些位置评估“粗糙”网络。给定这个“粗糙”网络的输出，我们随后沿着每条光线生成一个更有信息量的点采样，其中样本偏向于体积的相关部分。为此，我们首先将公式3中来自粗糙网络 \(\hat{C}_{c}(\mathbf{r})\) 的alpha合成颜色重写为沿光线所有采样颜色 \(c_{i}\) 的加权和：</p>
<p>\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}}w_{i}c_{i}\,,\quad\ w_{i}=T_{i}(1-\exp( -\sigma_{i}\delta_{i}))\,.\] (5)</p>
</div>

<div class="original">
<p>Normalizing these weights as \(\hat{w}_{i}=w_{i}/\sum_{j=1}^{N_{c}}w_{j}\) produces a piecewise-constant PDF along the ray. We sample a second set of \(N_{f}\) locations from this distribution using inverse transform sampling, evaluate our "fine" network at the union of the first and second set of samples, and compute the final rendered color of the ray \(\hat{C}_{f}(\mathbf{r})\) using Eqn. 3 but using all \(N_{c}+N_{f}\) samples. This procedure allocates more samples to regions we expect to contain visible content. This addresses a similar goal as importance sampling, but we use the sampled values as a nonuniform discretization of the whole integration domain rather than treating each sample as an independent probabilistic estimate of the entire integral.</p>
</div>
<div class="translation">
<p>将这些权重归一化为 \(\hat{w}_{i}=w_{i}/\sum_{j=1}^{N_{c}}w_{j}\) 会沿光线产生一个分段常数概率密度函数（PDF）。我们使用逆变换采样从此分布中抽取第二组 \(N_{f}\) 个位置，在第一组和第二组样本的并集上评估我们的“精细”网络，并使用公式3但使用所有 \(N_{c}+N_{f}\) 个样本来计算光线的最终渲染颜色 \(\hat{C}_{f}(\mathbf{r})\)。此过程将更多样本分配给我们预期包含可见内容的区域。这与重要性采样的目标类似，但我们使用采样值作为整个积分域的非均匀离散化，而不是将每个样本视为整个积分的独立概率估计。</p>
</div>

<div class="original">
<h3>Implementation details</h3>
<p>We optimize a separate neural continuous volume representation network for each scene. This requires only a dataset of captured RGB images of the scene,</p>
</div>
<div class="translation">
<h3>实现细节</h3>
<p>我们为每个场景优化一个独立的神经连续体积表示网络。这只需要场景的捕获RGB图像数据集、</p>
</div>

<div class="original">
<p>the corresponding camera poses and intrinsic parameters, and scene bounds (we use ground truth camera poses, intrinsics, and bounds for synthetic data, and use the COLMAP structure-from-motion package [39] to estimate these parameters for real data). At each optimization iteration, we randomly sample a batch of camera rays from the set of all pixels in the dataset, and then follow the hierarchical sampling described in Sec. 5.2 to query \(N_c\) samples from the coarse network and \(N_c + N_f\) samples from the fine network. We then use the volume rendering procedure described in Sec. 4 to render the color of each ray from both sets of samples. Our loss is simply the total squared error between the rendered and true pixel colors for both the coarse and fine renderings:</p>
<p>\[\mathcal{L} = \sum_{\mathbf{r}\in\mathcal{R}}\left[\left\|\hat{C}_c(\mathbf{r}) - C(\mathbf{r})\right\|_2^2 + \left\|\hat{C}_f(\mathbf{r}) - C(\mathbf{r})\right\|_2^2\right]\] (6)</p>
</div>
<div class="translation">
<p>相应的相机位姿和内在参数，以及场景边界（对于合成数据我们使用真实相机位姿、内参和边界，对于真实数据我们使用COLMAP运动恢复结构包[39]来估计这些参数）。在每次优化迭代中，我们从数据集所有像素的集合中随机采样一批相机光线，然后按照第5.2节描述的分层采样，从粗糙网络查询 \(N_c\) 个样本，从精细网络查询 \(N_c + N_f\) 个样本。然后，我们使用第4节描述的体渲染流程从两组样本中渲染每条光线的颜色。我们的损失函数就是粗糙渲染和精细渲染的渲染像素颜色与真实像素颜色之间的总平方误差：</p>
<p>\[\mathcal{L} = \sum_{\mathbf{r}\in\mathcal{R}}\left[\left\|\hat{C}_c(\mathbf{r}) - C(\mathbf{r})\right\|_2^2 + \left\|\hat{C}_f(\mathbf{r}) - C(\mathbf{r})\right\|_2^2\right]\] (6)</p>
</div>

<div class="original">
<p>where \(\mathcal{R}\) is the set of rays in each batch, and \(C(\mathbf{r})\), \(\hat{C}_c(\mathbf{r})\), and \(\hat{C}_f(\mathbf{r})\) are the ground truth, coarse volume predicted, and fine volume predicted RGB colors for ray \(\mathbf{r}\) respectively. Note that even though the final rendering comes from \(\hat{C}_f(\mathbf{r})\), we also minimize the loss of \(\hat{C}_c(\mathbf{r})\) so that the weight distribution from the coarse network can be used to allocate samples in the fine network.</p>
</div>
<div class="translation">
<p>其中 \(\mathcal{R}\) 是每批中的光线集合，\(C(\mathbf{r})\), \(\hat{C}_c(\mathbf{r})\) 和 \(\hat{C}_f(\mathbf{r})\) 分别是光线 \(\mathbf{r}\) 的真实RGB颜色、粗糙体积预测的RGB颜色和精细体积预测的RGB颜色。请注意，即使最终渲染来自 \(\hat{C}_f(\mathbf{r})\)，我们也最小化 \(\hat{C}_c(\mathbf{r})\) 的损失，以便来自粗糙网络的权重分布可以用于在精细网络中分配样本。</p>
</div>

<div class="original">
<p>In our experiments, we use a batch size of 4096 rays, each sampled at \(N_c = 64\) coordinates in the coarse volume and \(N_f = 128\) additional coordinates in the fine volume. We use the Adam optimizer [18] with a learning rate that begins at \(5 \times 10^{-4}\) and decays exponentially to \(5 \times 10^{-5}\) over the course of optimization (other Adam hyperparameters are left at default values of \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), and \(\epsilon = 10^{-7}\)). The optimization for a single scene typically take around 100-300k iterations to converge on a single NVIDIA V100 GPU (about 1-2 days).</p>
</div>
<div class="translation">
<p>在我们的实验中，我们使用4096条光线作为批量大小，每条光线在粗糙体积中采样 \(N_c = 64\) 个坐标，在精细体积中额外采样 \(N_f = 128\) 个坐标。我们使用Adam优化器[18]，学习率从 \(5 \times 10^{-4}\) 开始，并在优化过程中指数衰减到 \(5 \times 10^{-5}\)（其他Adam超参数保持默认值 \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), 和 \(\epsilon = 10^{-7}\))。优化单个场景通常需要约100-300k次迭代才能在单个NVIDIA V100 GPU上收敛（约1-2天）。</p>
</div>

<div class="original">
<h2>6 Results</h2>
<p>We quantitatively (Tables 1) and qualitatively (Figs. 8 and 6) show that our method outperforms prior work, and provide extensive ablation studies to validate our design choices (Table 2). We urge the reader to view our supplementary video to better appreciate our method's significant improvement over baseline methods when rendering smooth paths of novel views.</p>
</div>
<div class="translation">
<h2>6 结果</h2>
<p>我们在定量上（表1）和定性上（图8和图6）展示了我们的方法优于先前的工作，并提供了广泛的消融研究以验证我们的设计选择（表2）。我们强烈建议读者观看我们的补充视频，以更好地理解在渲染新视图的平滑路径时，我们的方法相对于基线方法的显著改进。</p>
</div>

<div class="original">
<h3>Datasets</h3>
<p><em>Synthetic renderings of objects</em> We first show experimental results on two datasets of synthetic renderings of objects (Table 1, "Diffuse Synthetic 360" and "Realistic Synthetic 360"). The DeepVoxels [41] dataset contains four Lambertian objects with simple geometry. Each object is rendered at \(512 \times 512\) pixels from viewpoints sampled on the upper hemisphere (\(479\) as input and \(1000\) for testing). We additionally generate our own dataset containing pathtraced images of eight objects that exhibit complicated geometry and realistic non-Lambertian materials. Six are rendered from viewpoints sampled on the upper hemisphere, and two are rendered from viewpoints sampled on a full sphere. We render 100 views of each scene as input and 200 for testing, all at \(800 \times 800\) pixels.</p>
</div>
<div class="translation">
<h3>数据集</h3>
<p><em>物体的合成渲染</em> 我们首先展示在两个物体合成渲染数据集上的实验结果（表1，“Diffuse Synthetic 360” 和 “Realistic Synthetic 360”）。DeepVoxels [41] 数据集包含四个具有简单几何形状的朗伯物体。每个物体在 \(512 \times 512\) 像素分辨率下渲染，视点在上半球采样（479个作为输入，1000个用于测试）。我们额外生成了自己的数据集，包含八个物体的路径追踪图像，这些物体具有复杂的几何形状和真实的非朗伯材质。其中六个从上半球采样的视点渲染，两个从整个球体采样的视点渲染。我们为每个场景渲染100个视图作为输入，200个用于测试，分辨率均为 \(800 \times 800\) 像素。</p>
</div>

<div class="original">
<p>10 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<table>
<thead>
<tr><th>Method</th><th colspan="3">Diffuse Synthetic 360° [41]</th><th colspan="3">Realistic Synthetic 360°</th><th colspan="3">Real Forward-Facing [28]</th></tr>
<tr><th></th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th></tr>
</thead>
<tbody>
<tr><td>SRN [42]</td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr>
<tr><td>NV [24]</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>LLFF [28]</td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr>
<tr><td>Ours</td><td><strong>40.15</strong></td><td>0.991</td><td><strong>0.023</strong></td><td><strong>31.01</strong></td><td><strong>0.947</strong></td><td><strong>0.081</strong></td><td><strong>26.50</strong></td><td><strong>0.811</strong></td><td><strong>0.250</strong></td></tr>
</tbody>
</table>
<p><strong>Table 1:</strong> Our method quantitatively outperforms prior work on datasets of both synthetic and real images. We report PSNR/SSIM (higher is better) and LPIPS [50] (lower is better). The DeepVoxels [41] dataset consists of 4 diffuse objects with simple geometry. Our realistic synthetic dataset consists of pathtraced renderings of 8 geometrically complex objects with complex non-Lambertian materials. The real dataset consists of handheld forward-facing captures of 8 real-world scenes (NV cannot be evaluated on this data because it only reconstructs objects inside a bounded volume). Though LLFF achieves slightly better LPIPS, we urge readers to view our supplementary video where our method achieves better multiview consistency and produces fewer artifacts than all baselines.</p>
</div>
<div class="translation">
<p>10 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<table>
<thead>
<tr><th>方法</th><th colspan="3">Diffuse Synthetic 360° [41]</th><th colspan="3">Realistic Synthetic 360°</th><th colspan="3">Real Forward-Facing [28]</th></tr>
<tr><th></th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th></tr>
</thead>
<tbody>
<tr><td>SRN [42]</td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr>
<tr><td>NV [24]</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>LLFF [28]</td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr>
<tr><td>Ours</td><td><strong>40.15</strong></td><td>0.991</td><td><strong>0.023</strong></td><td><strong>31.01</strong></td><td><strong>0.947</strong></td><td><strong>0.081</strong></td><td><strong>26.50</strong></td><td><strong>0.811</strong></td><td><strong>0.250</strong></td></tr>
</tbody>
</table>
<p><strong>表 1：</strong> 我们的方法在合成和真实图像数据集上定量优于先前的工作。我们报告了PSNR/SSIM（越高越好）和LPIPS[50]（越低越好）。DeepVoxels[41]数据集包含4个具有简单几何形状的漫反射物体。我们的真实感合成数据集包含8个具有复杂非朗伯材质的几何复杂物体的路径追踪渲染。真实数据集包含8个真实世界场景的手持前向捕捉（NV无法在此数据上评估，因为它只重建有界体积内的物体）。尽管LLFF在LPIPS上略优，但我们强烈建议读者观看我们的补充视频，其中我们的方法实现了更好的多视图一致性，并且产生的伪影少于所有基线。</p>
</div>

<div class="original">
<p>Real images of complex scenes We show results on complex real-world scenes captured with roughly forward-facing images (Table 1, “Real Forward-Facing”). This dataset consists of 8 scenes captured with a handheld cellphone (5 taken from the LLFF paper and 3 that we capture), captured with 20 to 62 images, and hold out \( \frac{1}{8} \) of these for the test set. All images are \( 1008 \times 756 \) pixels.</p>
</div>
<div class="translation">
<p>复杂场景的真实图像 我们在使用大致前向图像捕获的复杂真实世界场景上展示了结果（表1，“Real Forward-Facing”）。该数据集包含8个使用手持手机捕获的场景（5个来自LLFF论文，3个由我们捕获），捕获了20至62张图像，并保留其中的 \( \frac{1}{8} \) 作为测试集。所有图像均为 \( 1008 \times 756 \) 像素。</p>
</div>

<div class="original">
<h3>6.2 Comparisons</h3>
<p>To evaluate our model we compare against current top-performing techniques for view synthesis, detailed below. All methods use the same set of input views to train a separate network for each scene except Local Light Field Fusion [28], which trains a single 3D convolutional network on a large dataset, then uses the same trained network to process input images of new scenes at test time.</p>
</div>
<div class="translation">
<h3>6.2 比较</h3>
<p>为了评估我们的模型，我们与当前性能最佳的视图合成技术进行比较，详情如下。所有方法都使用相同的输入视图集为每个场景训练一个单独的网络，除了局部光场融合（Local Light Field Fusion，LLFF）[28]，它在一个大型数据集上训练一个单一的3D卷积网络，然后在测试时使用相同的训练网络处理新场景的输入图像。</p>
</div>

<div class="original">
<p><strong>Neural Volumes (NV) [24]</strong> synthesizes novel views of objects that lie entirely within a bounded volume in front of a distinct background (which must be separately captured without the object of interest). It optimizes a deep 3D convolutional network to predict a discretized RGB\(\alpha\) voxel grid with \(128^3\) samples as well as a 3D warp grid with \(32^3\) samples. The algorithm renders novel views by marching camera rays through the warped voxel grid.</p>
</div>
<div class="translation">
<p><strong>神经体积（Neural Volumes, NV）[24]</strong> 合成完全位于不同背景前方有界体积内的物体的新视图（该背景必须在没有目标物体的情况下单独捕获）。它优化一个深度3D卷积网络来预测一个具有 \(128^3\) 样本的离散化RGB\(\alpha\)体素网格以及一个具有 \(32^3\) 样本的3D扭曲网格。该算法通过在扭曲的体素网格中步进相机光线来渲染新视图。</p>
</div>

<div class="original">
<p><strong>Scene Representation Networks (SRN) [42]</strong> represent a continuous scene as an opaque surface, implicitly defined by a MLP that maps each \((x,y,z)\) coordinate to a feature vector. They train a recurrent neural network to march along a ray through the scene representation by using the feature vector at any 3D coordinate to predict the next step size along the ray. The feature vector from the final step is decoded into a single color for that point on the surface. Note that SRN is a better-performing followup to DeepVoxels [41] by the same authors, which is why we do not include comparisons to DeepVoxels.</p>
</div>
<div class="translation">
<p><strong>场景表示网络（Scene Representation Networks, SRN）[42]</strong> 将连续场景表示为一个不透明表面，该表面由一个MLP隐式定义，该MLP将每个 \((x,y,z)\) 坐标映射到一个特征向量。他们训练一个循环神经网络，通过使用任何3D坐标处的特征向量预测沿光线的下一步步长，沿着光线穿过场景表示。最后一步的特征向量被解码为该表面点处的单一颜色。请注意，SRN是同一作者对DeepVoxels[41]的改进版本，性能更好，因此我们不包含与DeepVoxels的比较。</p>
</div>

<div class="original">
<h1>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1>
<p><strong>Ship<br>Lego</strong></p>
<p><strong>Microphone<br>Materials</strong></p>
<p><strong>Ground Truth  NeRF (ours)   LLFF [28]   SRN [42]   NV [24]</strong></p>
<p><strong>Fig. 5:</strong> Comparisons on test-set views for scenes from our new synthetic dataset generated with a physically-based renderer. Our method is able to recover fine details in both geometry and appearance, such as Ship’s rigging, Lego’s gear and treads, Microphone’s shiny stand and mesh grille, and Material’s non-Lambertian reflectance. LLFF exhibits banding artifacts on the Microphone stand and Material’s object edges and ghosting artifacts in Ship’s mast and inside the Lego object. SRN produces blurry and distorted renderings in every case. Neural Volumes cannot capture the details on the Microphone’s grille or Lego’s gears, and it completely fails to recover the geometry of Ship’s rigging.</p>
</div>
<div class="translation">
<h1>NeRF：将场景表示为神经辐射场以实现视图合成</h1>
<p><strong>Ship<br>Lego</strong></p>
<p><strong>Microphone<br>Materials</strong></p>
<p><strong>真实值  NeRF (我们的)   LLFF [28]   SRN [42]   NV [24]</strong></p>
<p><strong>图 5：</strong> 在我们新的基于物理渲染器生成的合成数据集场景的测试集视图上的比较。我们的方法能够恢复几何和外观的精细细节，例如Ship的索具、Lego的齿轮和履带、Microphone闪亮的支架和网状格栅，以及Material的非朗伯反射。LLFF在Microphone支架和Material物体边缘上表现出带状伪影，在Ship的桅杆和Lego物体内部表现出重影伪影。SRN在所有情况下都产生模糊和扭曲的渲染。神经体积（NV）无法捕捉Microphone格栅或Lego齿轮的细节，并且完全无法恢复Ship索具的几何形状。</p>
</div>

<div class="original">
<p>12 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<p><strong>Fern</strong></p>
<p><strong>T-Rex</strong></p>
<p><strong>Orchid</strong></p>
<p><strong>Ground Truth    NeRF (ours)    LLFF [28]    SRN [42]</strong></p>
<p><strong>Fig. 6:</strong> Comparisons on test-set views of real world scenes. LLFF is specifically designed for this use case (forward-facing captures of real scenes). Our method is able to represent fine geometry more consistently across rendered views than LLFF, as shown in Fern’s leaves and the skeleton ribs and railing in T-rex. Our method also correctly reconstructs partially occluded regions that LLFF struggles to render cleanly, such as the yellow shelves behind the leaves in the bottom Fern crop and green leaves in the background of the bottom Orchid crop. Blending between multiples renderings can also cause repeated edges in LLFF, as seen in the top Orchid crop. SRN captures the low-frequency geometry and color variation in each scene but is unable to reproduce any fine detail.</p>
</div>
<div class="translation">
<p>12 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<p><strong>Fern</strong></p>
<p><strong>T-Rex</strong></p>
<p><strong>Orchid</strong></p>
<p><strong>真实值    NeRF (我们的)    LLFF [28]    SRN [42]</strong></p>
<p><strong>图 6：</strong> 真实世界场景测试集视图上的比较。LLFF是专为这种用例设计的（真实场景的前向捕捉）。我们的方法能够在渲染视图之间比LLFF更一致地表示精细几何结构，如Fern的叶子和T-rex中的骨架肋骨和栏杆所示。我们的方法还能正确重建LLFF难以清晰渲染的部分遮挡区域，例如底部Fern裁剪图中叶子后面的黄色架子和底部Orchid裁剪图中背景的绿叶。在多个渲染之间混合也会导致LLFF中出现重复边缘，如顶部Orchid裁剪图所示。SRN捕捉了每个场景的低频几何结构和颜色变化，但无法再现任何精细细节。</p>
</div>

<div class="original">
<p><strong>Local Light Field Fusion (LLFF) [28]</strong> LLFF is designed for producing photorealistic novel views for well-sampled forward facing scenes. It uses a trained 3D convolutional network to directly predict a discretized frustum-sampled RGB\(\alpha\) grid (multiphane image or MPI [52]) for each input view, then renders novel views by alpha compositing and blending nearby MPIs into the novel viewpoint.</p>
</div>
<div class="translation">
<p><strong>局部光场融合（Local Light Field Fusion, LLFF）[28]</strong> LLFF专为采样良好的前向场景生成光照真实的新视图而设计。它使用训练好的3D卷积网络直接为每个输入视图预测一个离散的视锥采样RGB\(\alpha\)网格（多平面图像或MPI[52]），然后通过alpha合成并将附近的MPI混合到新视点来渲染新视图。</p>
</div>

<div class="original">
<h3>Discussion</h3>
<p>We thoroughly outperform both baselines that also optimize a separate network per scene (NV and SRN) in all scenarios. Furthermore, we produce qualitatively and quantitatively superior renderings compared to LLFF (across all except one metric) while using only their input images as our entire training set.</p>
</div>
<div class="translation">
<h3>讨论</h3>
<p>在所有场景中，我们都全面优于同样为每个场景优化单独网络（NV和SRN）的基线。此外，与LLFF相比，我们产生了定性和定量上更优的渲染结果（除一项指标外），同时仅使用其输入图像作为我们整个训练集。</p>
</div>

<div class="original">
<p>The SRN method produces heavily smoothed geometry and texture, and its representational power for view synthesis is limited by selecting only a single depth and color per camera ray. The NV baseline is able to capture reasonably detailed volumetric geometry and appearance, but its use of an underlying explicit \(128^{3}\) voxel grid prevents it from scaling to represent fine details at high resolutions. LLFF specifically provides a "sampling guideline" to not exceed 64 pixels of disparity between input views, so it frequently fails to estimate correct geometry in the synthetic datasets which contain up to 400-500 pixels of disparity between views. Additionally, LLFF blends between different scene representations for rendering different views, resulting in perceptually-distracting inconsistency as is apparent in our supplementary video.</p>
</div>
<div class="translation">
<p>SRN方法产生严重平滑的几何和纹理，其视图合成的表示能力受限于每条相机光线仅选择一个深度和颜色。NV基线能够捕捉相当详细的体积几何和外观，但其使用的底层显式 \(128^{3}\) 体素网格阻止了它扩展到高分辨率下表示精细细节。LLFF特别提供了“采样指南”，要求输入视图之间的视差不超64像素，因此在合成数据集（视图间视差高达400-500像素）中经常无法估计正确的几何结构。此外，LLFF在渲染不同视图时混合不同的场景表示，导致感知上分散注意力的不一致性，这在我们的补充视频中很明显。</p>
</div>

<div class="original">
<p>The biggest practical tradeoffs between these methods are time versus space. All compared single scene methods take at least 12 hours to train per scene. In contrast, LLFF can process a small input dataset in under 10 minutes. However, LLFF produces a large 3D voxel grid for every input image, resulting in enormous storage requirements (over 15GB for one "Realistic Synthetic" scene). Our method requires only 5 MB for the network weights (a relative compression of \(3000\times\) compared to LLFF), which is even less memory than the <em>input images alone</em> for a single scene from any of our datasets.</p>
</div>
<div class="translation">
<p>这些方法之间最大的实际权衡是时间与空间。所有比较的单场景方法每个场景至少需要12小时来训练。相比之下，LLFF可以在10分钟内处理一个小的输入数据集。然而，LLFF为每个输入图像生成一个大的3D体素网格，导致巨大的存储需求（一个“Realistic Synthetic”场景超过15GB）。我们的方法仅需5MB用于网络权重（相对于LLFF有 \(3000\times\) 的相对压缩），这甚至少于我们任何数据集中单个场景<em>仅输入图像</em>所需的内存。</p>
</div>

<div class="original">
<h3>Ablation studies</h3>
<p>We validate our algorithm's design choices and parameters with an extensive ablation study in Table 2. We present results on our "Realistic Synthetic 360\(^{\circ}\)" scenes. Row 9 shows our complete model as a point of reference. Row 1 shows a minimalist version of our model without positional encoding (PE), view-dependence (VD), or hierarchical sampling (H). In rows 2-4 we remove these three components one at a time from the full model, observing that positional encoding (row 2) and view-dependence (row 3) provide the largest quantitative benefit followed by hierarchical sampling (row 4). Rows 5-6 show how our performance decreases as the number of input images is reduced. Note that our method's performance using only 25 input images still exceeds NV, SRN, and LLFF across all metrics when they are provided with 100 images (see supplementary material). In rows 7-8 we validate our choice of the maximum frequency</p>
</div>
<div class="translation">
<h3>消融研究</h3>
<p>我们在表2中通过广泛的消融研究验证了我们算法的设计选择和参数。我们在“Realistic Synthetic 360\(^{\circ}\)”场景上展示结果。第9行展示了我们完整的模型作为参考点。第1行展示了一个没有位置编码（PE）、视角依赖（VD）或分层采样（H）的极简版本模型。在第2-4行，我们从完整模型中依次移除这三个组件，观察到位置编码（第2行）和视角依赖（第3行）提供了最大的定量收益，其次是分层采样（第4行）。第5-6行展示了随着输入图像数量减少，我们的性能如何下降。请注意，当仅使用25张输入图像时，我们的方法在所有指标上的性能仍然超过使用100张图像的NV、SRN和LLFF（参见补充材料）。在第7-8行，我们验证了我们对最大频率的选择</p>
</div>

<div class="original">
<p>\begin{tabular}{l|l l l l l l}  & Input & \#Im. & \(L\) & \((N_c, N_f)\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline
1) No PE, VD, H & \(xyz\) & 100 & - & (256, -) & 26.67 & 0.906 & 0.136 \\
2) No Pos. Encoding & \(xyz\theta\phi\) & 100 & - & (64, 128) & 28.77 & 0.924 & 0.108 \\
3) No View Dependence & \(xyz\) & 100 & 10 & (64, 128) & 27.66 & 0.925 & 0.117 \\
4) No Hierarchical & \(xyz\theta\phi\) & 100 & 10 & (256, -) & 30.06 & 0.938 & 0.109 \\
5) Far Fewer Images & \(xyz\theta\phi\) & 25 & 10 & (64, 128) & 27.78 & 0.925 & 0.107 \\
6) Fewer Images & \(xyz\theta\phi\) & 50 & 10 & (64, 128) & 29.79 & 0.940 & 0.096 \\
7) Fewer Frequencies & \(xyz\theta\phi\) & 100 & 5 & (64, 128) & 30.59 & 0.944 & 0.088 \\
8) More Frequencies & \(xyz\theta\phi\) & 100 & 15 & (64, 128) & 30.81 & 0.946 & 0.096 \\
9) Complete Model & \(xyz\theta\phi\) & 100 & 10 & (64, 128) & \textbf{31.01} & \textbf{0.947} & \textbf{0.081} \\ \end{tabular}</p>
<p><strong>Table 2:</strong> An ablation study of our model. Metrics are averaged over the 8 scenes from our realistic synthetic dataset. See Sec. 6.4 for detailed descriptions.</p>
</div>
<div class="translation">
<p>\begin{tabular}{l|l l l l l l}  & 输入 & \#图像数 & \(L\) & \((N_c, N_f)\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline
1) 无 PE, VD, H & \(xyz\) & 100 & - & (256, -) & 26.67 & 0.906 & 0.136 \\
2) 无位置编码 & \(xyz\theta\phi\) & 100 & - & (64, 128) & 28.77 & 0.924 & 0.108 \\
3) 无视角依赖 & \(xyz\) & 100 & 10 & (64, 128) & 27.66 & 0.925 & 0.117 \\
4) 无分层采样 & \(xyz\theta\phi\) & 100 & 10 & (256, -) & 30.06 & 0.938 & 0.109 \\
5) 极少图像 & \(xyz\theta\phi\) & 25 & 10 & (64, 128) & 27.78 & 0.925 & 0.107 \\
6) 较少图像 & \(xyz\theta\phi\) & 50 & 10 & (64, 128) & 29.79 & 0.940 & 0.096 \\
7) 较少频率 & \(xyz\theta\phi\) & 100 & 5 & (64, 128) & 30.59 & 0.944 & 0.088 \\
8) 更多频率 & \(xyz\theta\phi\) & 100 & 15 & (64, 128) & 30.81 & 0.946 & 0.096 \\
9) 完整模型 & \(xyz\theta\phi\) & 100 & 10 & (64, 128) & \textbf{31.01} & \textbf{0.947} & \textbf{0.081} \\ \end{tabular}</p>
<p><strong>表 2：</strong> 我们模型的消融研究。指标在我们真实感合成数据集的8个场景上取平均值。详细描述见第6.4节。</p>
</div>

<div class="original">
<p>\(L\) used in our positional encoding for \(\mathbf{x}\) (the maximum frequency used for \(\mathbf{d}\) is scaled proportionally). Only using 5 frequencies reduces performance, but increasing the number of frequencies from 10 to 15 does not improve performance. We believe the benefit of increasing \(L\) is limited once \(2^{L}\) exceeds the maximum frequency present in the sampled input images (roughly 1024 in our data).</p>
</div>
<div class="translation">
<p>用于 \(\mathbf{x}\) 的位置编码中的 \(L\)（用于 \(\mathbf{d}\) 的最大频率按比例缩放）。仅使用5个频率会降低性能，但将频率数量从10增加到15并不能提高性能。我们认为一旦 \(2^{L}\) 超过采样输入图像中存在的最大频率（在我们的数据中约为1024），增加 \(L\) 的收益就有限了。</p>
</div>

<div class="original">
<h2>7 Conclusion</h2>
<p>Our work directly addresses deficiencies of prior work that uses MLPs to represent objects and scenes as continuous functions. We demonstrate that representing scenes as 5D neural radiance fields (an MLP that outputs volume density and view-dependent emitted radiance as a function of 3D location and 2D viewing direction) produces better renderings than the previously-dominant approach of training deep convolutional networks to output discretized voxel representations.</p>
</div>
<div class="translation">
<h2>7 结论</h2>
<p>我们的工作直接解决了先前使用MLP将物体和场景表示为连续函数的工作的不足。我们证明了将场景表示为五维神经辐射场（一个输出体积密度和视角相关发射辐射亮度作为3D位置和2D观察方向函数的MLP），相比于之前主导的通过训练深度卷积网络输出离散体素表示的方法，能够产生更好的渲染结果。</p>
</div>

<div class="original">
<p>Although we have proposed a hierarchical sampling strategy to make rendering more sample-efficient (for both training and testing), there is still much more progress to be made in investigating techniques to efficiently optimize and render neural radiance fields. Another direction for future work is interpretability: sampled representations such as voxel grids and meshes admit reasoning about the expected quality of rendered views and failure modes, but it is unclear how to analyze these issues when we encode scenes in the weights of a deep neural network. We believe that this work makes progress towards a graphics pipeline based on real world imagery, where complex scenes could be composed of neural radiance fields optimized from images of actual objects and scenes.</p>
</div>
<div class="translation">
<p>尽管我们提出了分层采样策略以使渲染更加样本高效（对于训练和测试都是如此），但在研究高效优化和渲染神经辐射场的技术方面仍有许多进展需要取得。未来工作的另一个方向是可解释性：像体素网格和网格这样的采样表示允许对渲染视图的预期质量和故障模式进行推理，但当我们把场景编码在深度神经网络的权重中时，如何分析这些问题尚不清楚。我们相信这项工作在实现基于真实世界图像的图形流水线方面取得了进展，其中复杂场景可以由实际物体和场景图像优化而来的神经辐射场组成。</p>
</div>

<div class="original">
<p><strong>Acknowledgements</strong> We thank Kevin Cao, Guowei Frank Yang, and Nithin Raghavan for comments and discussions. RR acknowledges funding from ONR grants N000141712687 and N000142012529 and the Ronald L. Graham Chair. BM is funded by a Hertz Foundation Fellowship, and MT is funded by an NSF Graduate Fellowship. Google provided a generous donation of cloud compute credits through the BAIR Commons program. We thank the following</p>
</div>
<div class="translation">
<p><strong>致谢</strong> 我们感谢Kevin Cao、Guowei Frank Yang和Nithin Raghavan的评论和讨论。RR感谢ONR基金N000141712687、N000142012529以及Ronald L. Graham讲席的资助。BM由Hertz基金会奖学金资助，MT由NSF研究生奖学金资助。谷歌通过BAIR Commons计划慷慨捐赠了云计算积分。我们感谢以下</p>
</div>

<div class="original">
<p>Blend Swap users for the models used in our realistic synthetic dataset: gregzaal (ship), 1DInc (chair), bryanajones (drums), Herberhold (ficus), erickfree (hotdog), Heinzelnisse (lego), elbrujodelatribu (materials), and up3d.de (mic).</p>
</div>
<div class="translation">
<p>Blend Swap用户提供我们真实感合成数据集使用的模型：gregzaal (ship), 1DInc (chair), bryanajones (drums), Herberhold (ficus), erickfree (hotdog), Heinzelnisse (lego), elbrujodelatribu (materials), 以及 up3d.de (mic)。</p>
</div>

<!-- 参考文献部分因篇幅原因省略翻译，保持原文 -->
<div class="original">
<h2>References</h2>
<p>[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015)</p>
<!-- ... 其他参考文献条目 ... -->
<p>[52] Zhou, T., Tucker, R., Flynn, J., Pyffe, G., Snavely, N.: Stereo magnification: Learning view synthesis using multiplane images. ACM Transactions on Graphics (SIGGRAPH) (2018)</p>
</div>
<div class="translation">
<h2>参考文献</h2>
<p>[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow：异构系统上的大规模机器学习 (2015)</p>
<!-- ... 其他参考文献条目翻译（此处省略以节省空间）... -->
<p>[52] Zhou, T., Tucker, R., Flynn, J., Pyffe, G., Snavely, N.: 立体放大：使用多平面图像学习视图合成。 ACM Transactions on Graphics (SIGGRAPH) (2018)</p>
</div>

<!-- 附录部分省略翻译，保持原文 -->
<div class="original">
<h2>Appendix A Additional Implementation Details</h2>
<p><strong>Network Architecture</strong> Fig. 7 details our simple fully-connected architecture.</p>
<p><strong>Volume Bounds</strong> Our method renders views by querying the neural radiance field representation at continuous 5D coordinates along camera rays. For experiments with synthetic images, we scale the scene so that it lies within a cube of side length 2 centered at the origin, and only query the representation within this bounding volume. Our dataset of real images contains content that can exist anywhere between the closest point and infinity, so we use normalized device coordinates to map the depth range of these points into \([-1,1]\). This shifts all the ray origins to the near plane of the scene, maps the perspective rays of the camera to parallel rays in the transformed volume, and uses disparity (inverse depth) instead of metric depth, so all coordinates are now bounded.</p>
</div>
<div class="translation">
<h2>附录 A 附加实现细节</h2>
<p><strong>网络架构</strong> 图7详细说明了我们简单的全连接架构。</p>
<p><strong>体积边界</strong> 我们的方法通过沿相机光线在连续五维坐标上查询神经辐射场表示来渲染视图。对于合成图像的实验，我们将场景缩放，使其位于以原点为中心、边长为2的立方体内，并仅在此边界体积内查询表示。我们的真实图像数据集包含可能存在于最近点到无穷远之间任何位置的内容，因此我们使用归一化设备坐标（NDC）将这些点的深度范围映射到 \([-1,1]\)。这将所有光线起点移动到场景的近平面，将相机的透视光线映射到变换后体积中的平行光线，并使用视差（逆深度）代替度量深度，因此所有坐标现在都是有界的。</p>
</div>

<!-- ... 后续附录内容省略 ... -->

</body>
</html><!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<style>
  .original { margin-bottom: 5px; }
  .translation { margin-bottom: 20px; color: #2E86C1; }
  h1, h2, h3, h4 { color: #1A5276; }
  table { border-collapse: collapse; margin: 15px 0; }
  th, td { border: 1px solid #ddd; padding: 8px; }
</style>
</head>
<body>

<div class="original">
<h1>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1>
</div>
<div class="translation">
<h1>NeRF：将场景表示为神经辐射场以实现视图合成</h1>
</div>

<div class="original">
<p>Ben Mildenhall<sup>1</sup>*</p>
<p>Pratul P. Srinivasan<sup>1</sup></p>
<p>Matthew Tancik<sup>1</sup></p>
<p>Jonathan T. Barron<sup>2</sup></p>
<p>Ravi Ramamoorthi<sup>3</sup></p>
<p>Ren Ng<sup>1</sup></p>
<p><sup>1</sup>UC Berkeley <sup>2</sup>Google Research <sup>3</sup>UC San Diego</p>
</div>
<div class="translation">
<p>本·米尔登霍尔<sup>1</sup>*</p>
<p>普拉图尔·P·斯里尼瓦桑<sup>1</sup></p>
<p>马修·坦西克<sup>1</sup></p>
<p>乔纳森·T·巴伦<sup>2</sup></p>
<p>拉维·拉玛莫西<sup>3</sup></p>
<p>吴韧<sup>1</sup></p>
<p><sup>1</sup>加州大学伯克利分校 <sup>2</sup>谷歌研究院 <sup>3</sup>加州大学圣地亚哥分校</p>
</div>

<div class="original">
<h4>Abstract</h4>
<p>We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \((x,y,z)\) and viewing direction \((\theta,\phi)\)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.</p>
<p><strong>Keywords:</strong> scene representation, view synthesis, image-based rendering, volume rendering, 3D deep learning</p>
</div>
<div class="translation">
<h4>摘要</h4>
<p>我们提出了一种方法，通过使用一组稀疏的输入视图优化底层连续的体场景函数，在合成复杂场景的新视图方面取得了最先进的结果。我们的算法使用全连接（非卷积）深度网络表示场景，其输入是单个连续的五维坐标（空间位置 \((x,y,z)\) 和观察方向 \((\theta,\phi)\)），输出是该空间位置的体积密度和视角相关的辐射亮度。我们通过沿相机光线查询五维坐标来合成视图，并使用经典的体渲染技术将输出的颜色和密度投影成图像。由于体渲染本质上是可微的，优化我们表示所需的唯一输入是一组已知相机位姿的图像。我们描述了如何有效优化神经辐射场以渲染具有复杂几何形状和外观场景的光照真实新视图，并展示了优于先前神经渲染和视图合成工作的结果。视图合成结果最好以视频形式观看，因此我们强烈建议读者观看补充视频以获得令人信服的比较。</p>
<p><strong>关键词：</strong> 场景表示，视图合成，基于图像的渲染，体渲染，3D深度学习</p>
</div>

<div class="original">
<h2>1 Introduction</h2>
<p>In this work, we address the long-standing problem of view synthesis in a new way by directly optimizing parameters of a continuous 5D scene representation to minimize the error of rendering a set of captured images.</p>
<p>We represent a static scene as a continuous 5D function that outputs the radiance emitted in each direction \((\theta,\phi)\) at each point \((x,y,z)\) in space, and a density at each point which acts like a differential opacity controlling how much radiance is accumulated by a ray passing through \((x,y,z)\). Our method optimizes a deep fully-connected neural network without any convolutional layers (often referred to as a multilayer perceptron or MLP) to represent this function by regressing from a single 5D coordinate \((x,y,z,\theta,\phi)\) to a single volume density and view-dependent RGB color. To render this <em>neural radiance field</em> (NeRF)</p>
</div>
<div class="translation">
<h2>1 引言</h2>
<p>在这项工作中，我们通过直接优化连续五维场景表示的参数来最小化渲染一组捕获图像的误差，以一种新方式解决了视图合成这个长期存在的问题。</p>
<p>我们将静态场景表示为一个连续的五维函数，该函数输出空间中每个点 \((x,y,z)\) 沿每个方向 \((\theta,\phi)\) 发出的辐射亮度，以及每点的密度（类似于微分不透明度，控制穿过 \((x,y,z)\) 的光线累积多少辐射亮度）。我们的方法优化一个没有任何卷积层的深度全连接神经网络（通常称为多层感知器或MLP）来表示此函数，它从单个五维坐标 \((x,y,z,\theta,\phi)\) 回归出单个体积密度和视角相关的RGB颜色。为了渲染这个<em>神经辐射场</em>（NeRF）</p>
</div>

<div class="original">
<p>2 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<p><strong>Input Images</strong></p>
<p><strong>Optimize NeRF</strong></p>
<p><strong>Render new views</strong></p>
<p><strong>Fig. 1:</strong> We present a method that optimizes a continuous 5D neural radiance field representation (volume density and view-dependent color at any continuous location) of a scene from a set of input images. We use techniques from volume rendering to accumulate samples of this scene representation along rays to render the scene from any viewpoint. Here, we visualize the set of 100 input views of the synthetic Drums scene randomly captured on a surrounding hemisphere, and we show two novel views rendered from our optimized NeRF representation.</p>
</div>
<div class="translation">
<p>2 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<p><strong>输入图像</strong></p>
<p><strong>优化 NeRF</strong></p>
<p><strong>渲染新视图</strong></p>
<p><strong>图 1：</strong> 我们提出了一种方法，从一组输入图像中优化场景的连续五维神经辐射场表示（任何连续位置的体积密度和视角相关颜色）。我们使用体渲染技术沿光线累积此场景表示的样本，以从任意视点渲染场景。这里，我们可视化了在周围半球上随机捕获的合成Drums场景的100个输入视图集，并展示了从我们优化后的NeRF表示渲染出的两个新视图。</p>
</div>

<div class="original">
<p>from a particular viewpoint we: 1) march camera rays through the scene to generate a sampled set of 3D points, 2) use those points and their corresponding 2D viewing directions as input to the neural network to produce an output set of colors and densities, and 3) use classical volume rendering techniques to accumulate those colors and densities into a 2D image. Because this process is naturally differentiable, we can use gradient descent to optimize this model by minimizing the error between each observed image and the corresponding views rendered from our representation. Minimizing this error across multiple views encourages the network to predict a coherent model of the scene by assigning high volume densities and accurate colors to the locations that contain the true underlying scene content. Figure 2 visualizes this overall pipeline.</p>
</div>
<div class="translation">
<p>从特定视点出发，我们：1）在场景中步进相机光线以生成采样的3D点集，2）使用这些点及其对应的2D观察方向作为神经网络的输入，生成颜色和密度输出集，3）使用经典体渲染技术将这些颜色和密度累积成2D图像。由于这个过程本质上是可微的，我们可以使用梯度下降法通过最小化每个观测图像与从我们表示中渲染出的对应视图之间的误差来优化此模型。在多个视图上最小化此误差，促使网络通过将高体积密度和准确颜色分配给包含真实底层场景内容的位置来预测场景的一致模型。图2可视化了这个整体流程。</p>
</div>

<div class="original">
<p>We find that the basic implementation of optimizing a neural radiance field representation for a complex scene does not converge to a sufficiently high-resolution representation and is inefficient in the required number of samples per camera ray. We address these issues by transforming input 5D coordinates with a positional encoding that enables the MLP to represent higher frequency functions, and we propose a hierarchical sampling procedure to reduce the number of queries required to adequately sample this high-frequency scene representation.</p>
</div>
<div class="translation">
<p>我们发现，针对复杂场景优化神经辐射场表示的基本实现无法收敛到足够高分辨率的表示，并且在每条相机光线所需的样本数量上效率低下。我们通过使用位置编码转换输入的五维坐标来解决这些问题，该编码使MLP能够表示更高频的函数，并提出一种分层采样程序以减少充分采样此高频场景表示所需的查询次数。</p>
</div>

<div class="original">
<p>Our approach inherits the benefits of volumetric representations: both can represent complex real-world geometry and appearance and are well suited for gradient-based optimization using projected images. Crucially, our method overcomes the prohibitive storage costs of discretized voxel grids when modeling complex scenes at high-resolutions. In summary, our technical contributions are:</p>
<ul>
<li>An approach for representing continuous scenes with complex geometry and materials as 5D neural radiance fields, parameterized as basic MLP networks.</li>
<li>A differentiable rendering procedure based on classical volume rendering techniques, which we use to optimize these representations from standard RGB images. This includes a hierarchical sampling strategy to allocate the MLP’s capacity towards space with visible scene content.</li>
</ul>
</div>
<div class="translation">
<p>我们的方法继承了体表示的优势：两者都能表示复杂的真实世界几何形状和外观，并且非常适合使用投影图像进行基于梯度的优化。关键的是，我们的方法克服了在高分辨率建模复杂场景时离散体素网格的过高存储成本。总之，我们的技术贡献是：</p>
<ul>
<li>一种将具有复杂几何形状和材质的连续场景表示为五维神经辐射场的方法，参数化为基本的MLP网络。</li>
<li>一种基于经典体渲染技术的可微渲染流程，我们用它从标准RGB图像优化这些表示。这包括一种分层采样策略，用于将MLP的能力分配给具有可见场景内容的空间。</li>
</ul>
</div>

<div class="original">
<p>- A positional encoding to map each input 5D coordinate into a higher dimensional space, which enables us to successfully optimize neural radiance fields to represent high-frequency scene content.</p>
</div>
<div class="translation">
<p>- 一种位置编码，将每个输入的五维坐标映射到更高维空间，使我们能够成功优化神经辐射场以表示高频场景内容。</p>
</div>

<div class="original">
<p>We demonstrate that our resulting neural radiance field method quantitatively and qualitatively outperforms state-of-the-art view synthesis methods, including works that fit neural 3D representations to scenes as well as works that train deep convolutional networks to predict sampled volumetric representations. As far as we know, this paper presents the first continuous neural scene representation that is able to render high-resolution photorealistic novel views of real objects and scenes from RGB images captured in natural settings.</p>
</div>
<div class="translation">
<p>我们证明，我们得到的神经辐射场方法在定量和定性上都优于最先进的视图合成方法，包括将神经3D表示拟合到场景的工作以及训练深度卷积网络以预测采样体表示的工作。据我们所知，本文提出了第一个连续的神经场景表示，能够从自然环境中捕获的RGB图像渲染真实物体和场景的高分辨率光照真实新视图。</p>
</div>

<div class="original">
<h2>2 Related Work</h2>
<p>A promising recent direction in computer vision is encoding objects and scenes in the weights of an MLP that directly maps from a 3D spatial location to an implicit representation of the shape, such as the signed distance [6] at that location. However, these methods have so far been unable to reproduce realistic scenes with complex geometry with the same fidelity as techniques that represent scenes using discrete representations such as triangle meshes or voxel grids. In this section, we review these two lines of work and contrast them with our approach, which enhances the capabilities of neural scene representations to produce state-of-the-art results for rendering complex realistic scenes.</p>
</div>
<div class="translation">
<h2>2 相关工作</h2>
<p>计算机视觉中一个有前景的最新方向是将物体和场景编码在MLP的权重中，该MLP直接从3D空间位置映射到形状的隐式表示，例如该位置的有向距离[6]。然而，这些方法迄今无法以与使用三角形网格或体素网格等离散表示来表示场景的技术相同的保真度再现具有复杂几何形状的真实场景。在本节中，我们回顾了这两类工作，并将其与我们的方法进行对比，我们的方法增强了神经场景表示的能力，以产生渲染复杂真实场景的最新结果。</p>
</div>

<div class="original">
<p>A similar approach of using MLPs to map from low-dimensional coordinates to colors has also been used for representing other graphics functions such as images [44], textured materials [36, 37, 12, 31], and indirect illumination values [38].</p>
</div>
<div class="translation">
<p>使用MLP从低维坐标映射到颜色的类似方法也被用于表示其他图形函数，如图像[44]、纹理材质[36, 37, 12, 31]和间接光照值[38]。</p>
</div>

<div class="original">
<p><strong>Neural 3D shape representations</strong> Recent work has investigated the implicit representation of continuous 3D shapes as level sets by optimizing deep networks that map \(xyz\) coordinates to signed distance functions [32, 15] or occupancy fields [27, 11]. However, these models are limited by their requirement of access to ground truth 3D geometry, typically obtained from synthetic 3D shape datasets such as ShapeNet [3]. Subsequent work has relaxed this requirement of ground truth 3D shapes by formulating differentiable rendering functions that allow neural implicit shape representations to be optimized using only 2D images. Niemeyer <em>et al.</em> [29] represent surfaces as 3D occupancy fields and use a numerical method to find the surface intersection for each ray, then calculate an exact derivative using implicit differentiation. Each ray intersection location is provided as the input to a neural 3D texture field that predicts a diffuse color for that point. Sitzmann <em>et al.</em> [42] use a less direct neural 3D representation that simply outputs a feature vector and RGB color at each continuous 3D coordinate, and propose a differentiable rendering function consisting of a recurrent neural network that marches along each ray to decide where the surface is located.</p>
</div>
<div class="translation">
<p><strong>神经3D形状表示</strong> 最近的工作研究了通过优化将\(xyz\)坐标映射到有向距离函数[32, 15]或占据场[27, 11]的深度网络，将连续3D形状隐式表示为水平集。然而，这些模型受限于它们需要访问真实3D几何形状的要求，这通常从合成3D形状数据集（如ShapeNet[3]）获得。后续工作通过制定可微渲染函数放宽了对真实3D形状的要求，使得神经隐式形状表示可以仅使用2D图像进行优化。Niemeyer <em>等人</em> [29] 将表面表示为3D占据场，并使用数值方法查找每条光线的表面交点，然后使用隐式微分计算精确导数。每个光线交点位置作为神经3D纹理场的输入，该场预测该点的漫反射颜色。Sitzmann <em>等人</em> [42] 使用一种不那么直接的神经3D表示，它仅在每个连续3D坐标处输出一个特征向量和RGB颜色，并提出一个由循环神经网络组成的可微渲染函数，该网络沿每条光线步进以确定表面位置。</p>
</div>

<div class="original">
<p>Though these techniques can potentially represent complicated and high-resolution geometry, they have so far been limited to simple shapes with low geometric complexity, resulting in oversmoothed renderings. We show that an alternate strategy of optimizing networks to encode 5D radiance fields (3D volumes</p>
</div>
<div class="translation">
<p>尽管这些技术有潜力表示复杂和高分辨率的几何形状，但迄今为止它们仅限于几何复杂度低的简单形状，导致渲染结果过度平滑。我们展示了一种替代策略，即优化网络以编码五维辐射场（3D体积</p>
</div>

<div class="original">
<p>with 2D view-dependent appearance) can represent higher-resolution geometry and appearance to render photorealistic novel views of complex scenes.</p>
</div>
<div class="translation">
<p>具有2D视角相关外观），可以表示更高分辨率的几何形状和外观，以渲染复杂场景的光照真实新视图。</p>
</div>

<div class="original">
<p><strong>View synthesis and image-based rendering</strong> Given a dense sampling of views, photorealistic novel views can be reconstructed by simple light field sample interpolation techniques [21, 5, 7]. For novel view synthesis with sparser view sampling, the computer vision and graphics communities have made significant progress by predicting traditional geometry and appearance representations from observed images. One popular class of approaches uses mesh-based representations of scenes with either diffuse [48] or view-dependent [2, 8, 49] appearance. Differentiable rasterizers [4, 10, 23, 25] or pathtracers [22, 30] can directly optimize mesh representations to reproduce a set of input images using gradient descent. However, gradient-based mesh optimization based on image reprojection is often difficult, likely because of local minima or poor conditioning of the loss landscape. Furthermore, this strategy requires a template mesh with fixed topology to be provided as an initialization before optimization [22], which is typically unavailable for unconstrained real-world scenes.</p>
</div>
<div class="translation">
<p><strong>视图合成与基于图像的渲染</strong> 给定密集采样的视图，可以通过简单的光场样本插值技术[21, 5, 7]重建光照真实的新视图。对于视图采样更稀疏的新视图合成，计算机视觉和图形学界通过从观测图像预测传统几何和外观表示取得了显著进展。一类流行的方法使用基于网格的场景表示，具有漫反射[48]或视角相关[2, 8, 49]的外观。可微分光栅化器[4, 10, 23, 25]或路径追踪器[22, 30]可以直接优化网格表示，以使用梯度下降法重现一组输入图像。然而，基于图像重投影的梯度网格优化通常很困难，可能是由于局部最小值或损失景观的条件不良。此外，此策略需要在优化前提供具有固定拓扑的模板网格作为初始化[22]，这对于无约束的真实世界场景通常不可用。</p>
</div>

<div class="original">
<p>Another class of methods use volumetric representations to address the task of high-quality photorealistic view synthesis from a set of input RGB images. Volumetric approaches are able to realistically represent complex shapes and materials, are well-suited for gradient-based optimization, and tend to produce less visually distracting artifacts than mesh-based methods. Early volumetric approaches used observed images to directly color voxel grids [19, 40, 45]. More recently, several methods [9, 13, 17, 28, 33, 43, 46, 52] have used large datasets of multiple scenes to train deep networks that predict a sampled volumetric representation from a set of input images, and then use either alpha-compositing [34] or learned compositing along rays to render novel views at test time. Other works have optimized a combination of convolutional networks (CNNs) and sampled voxel grids for each specific scene, such that the CNN can compensate for discretization artifacts from low resolution voxel grids [41] or allow the predicted voxel grids to vary based on input time or animation controls [24]. While these volumetric techniques have achieved impressive results for novel view synthesis, their ability to scale to higher resolution imagery is fundamentally limited by poor time and space complexity due to their discrete sampling -- rendering higher resolution images requires a finer sampling of 3D space. We circumvent this problem by instead encoding a <em>continuous</em> volume within the parameters of a deep fully-connected neural network, which not only produces significantly higher quality renderings than prior volumetric approaches, but also requires just a fraction of the storage cost of those <em>sampled</em> volumetric representations.</p>
</div>
<div class="translation">
<p>另一类方法使用体表示来处理从一组输入RGB图像进行高质量光照真实视图合成的任务。体方法能够真实地表示复杂的形状和材质，非常适合基于梯度的优化，并且往往比基于网格的方法产生更少视觉干扰的伪影。早期的体方法使用观测图像直接为体素网格着色[19, 40, 45]。最近，一些方法[9, 13, 17, 28, 33, 43, 46, 52]使用多个场景的大型数据集训练深度网络，从一组输入图像预测采样的体表示，然后在测试时使用alpha合成[34]或沿光线学习的合成来渲染新视图。其他工作针对每个特定场景优化了卷积网络（CNN）和采样体素网格的组合，使得CNN可以补偿低分辨率体素网格的离散化伪影[41]，或允许预测的体素网格根据输入时间或动画控制而变化[24]。虽然这些体技术在视图合成方面取得了令人印象深刻的结果，但它们扩展到更高分辨率图像的能力从根本上受到离散采样导致的较差时间和空间复杂度的限制——渲染更高分辨率的图像需要对3D空间进行更精细的采样。我们通过在深度全连接神经网络的参数内编码一个<em>连续</em>体积来规避这个问题，这不仅产生了比先前体方法显著更高质量的渲染，而且只需要这些<em>采样</em>体表示的一小部分存储成本。</p>
</div>

<div class="original">
<h2>3 Neural Radiance Field Scene Representation</h2>
<p>We represent a continuous scene as a 5D vector-valued function whose input is a 3D location \(\mathbf{x}=(x,y,z)\) and 2D viewing direction \((\theta,\phi)\), and whose output is an emitted color \(\mathbf{c}=(r,g,b)\) and volume density \(\sigma\). In practice, we express</p>
</div>
<div class="translation">
<h2>3 神经辐射场场景表示</h2>
<p>我们将连续场景表示为一个五维向量值函数，其输入是三维位置 \(\mathbf{x}=(x,y,z)\) 和二维观察方向 \((\theta,\phi)\)，输出是发出的颜色 \(\mathbf{c}=(r,g,b)\) 和体积密度 \(\sigma\)。在实践中，我们将</p>
</div>

<div class="original">
<h1>NerF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1>
<p><strong>5D Input Position + Direction</strong></p>
<p><strong>Output Color + Density</strong></p>
<p><strong>Volume Rendering Rendering Loss</strong></p>
<p><strong>(a) (b) (c) (d)</strong></p>
<p><strong>Fig. 2:</strong> An overview of our neural radiance field scene representation and differentiable rendering procedure. We synthesize images by sampling 5D coordinates (location and viewing direction) along camera rays (a), feeding those locations into an MLP to produce a color and volume density (b), and using volume rendering techniques to composite these values into an image (c). This rendering function is differentiable, so we can optimize our scene representation by minimizing the residual between synthesized and ground truth observed images (d).</p>
</div>
<div class="translation">
<h1>NeRF：将场景表示为神经辐射场以实现视图合成</h1>
<p><strong>五维输入位置 + 方向</strong></p>
<p><strong>输出颜色 + 密度</strong></p>
<p><strong>体渲染 渲染损失</strong></p>
<p><strong>(a) (b) (c) (d)</strong></p>
<p><strong>图 2：</strong> 我们的神经辐射场场景表示和可微渲染流程概述。我们通过沿相机光线(a)采样五维坐标（位置和观察方向）来合成图像，将这些位置输入MLP以产生颜色和体积密度(b)，并使用体渲染技术将这些值合成到图像中(c)。此渲染函数是可微的，因此我们可以通过最小化合成图像与真实观测图像(d)之间的残差来优化场景表示。</p>
</div>

<div class="original">
<p>direction as a 3D Cartesian unit vector \(\mathbf{d}\). We approximate this continuous 5D scene representation with an MLP network \(F_{\Theta} : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)\) and optimize its weights \(\Theta\) to map from each input 5D coordinate to its corresponding volume density and directional emitted color.</p>
</div>
<div class="translation">
<p>方向表示为三维笛卡尔单位向量 \(\mathbf{d}\)。我们使用MLP网络 \(F_{\Theta} : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)\) 来近似这个连续的五维场景表示，并优化其权重 \(\Theta\) 以将每个输入的五维坐标映射到其对应的体积密度和方向发射颜色。</p>
</div>

<div class="original">
<p>We encourage the representation to be multiview consistent by restricting the network to predict the volume density \(\sigma\) as a function of only the location \(\mathbf{x}\), while allowing the RGB color \(\mathbf{c}\) to be predicted as a function of both location and viewing direction. To accomplish this, the MLP \(F_{\Theta}\) first processes the input 3D coordinate \(\mathbf{x}\) with 8 fully-connected layers (using ReLU activations and 256 channels per layer), and outputs \(\sigma\) and a 256-dimensional feature vector. This feature vector is then concatenated with the camera ray’s viewing direction and passed to one additional fully-connected layer (using a ReLU activation and 128 channels) that output the view-dependent RGB color.</p>
</div>
<div class="translation">
<p>我们通过限制网络仅根据位置 \(\mathbf{x}\) 预测体积密度 \(\sigma\)，同时允许RGB颜色 \(\mathbf{c}\) 根据位置和观察方向进行预测，来鼓励表示具有多视图一致性。为实现这一点，MLP \(F_{\Theta}\) 首先使用8个全连接层（使用ReLU激活函数，每层256个通道）处理输入的三维坐标 \(\mathbf{x}\)，并输出 \(\sigma\) 和一个256维特征向量。然后将此特征向量与相机光线的观察方向拼接，并传递到一个额外的全连接层（使用ReLU激活函数和128个通道），该层输出视角相关的RGB颜色。</p>
</div>

<div class="original">
<p>See Fig. 3 for an example of how our method uses the input viewing direction to represent non-Lambertian effects. As shown in Fig. 4, a model trained without view dependence (only \(\mathbf{x}\) as input) has difficulty representing specularities.</p>
</div>
<div class="translation">
<p>关于我们的方法如何使用输入观察方向来表示非朗伯效应的示例，请参见图3。如图4所示，在没有视角依赖（仅 \(\mathbf{x}\) 作为输入）的情况下训练的模型难以表示镜面反射。</p>
</div>

<div class="original">
<h2>4 Volume Rendering with Radiance Fields</h2>
<p>Our 5D neural radiance field represents a scene as the volume density and directional emitted radiance at any point in space. We render the color of any ray passing through the scene using principles from classical volume rendering [16]. The volume density \(\sigma(\mathbf{x})\) can be interpreted as the differential probability of a ray terminating at an infinitesimal particle at location \(\mathbf{x}\). The expected color \(C(\mathbf{r})\) of camera ray \(\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}\) with near and far bounds \(t_n\) and \(t_f\) is:</p>
<p>\[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt,\]</p>
<p>where \(T(t) = \exp\left(-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds\right)\). (1)</p>
</div>
<div class="translation">
<h2>4 使用辐射场进行体渲染</h2>
<p>我们的五维神经辐射场将场景表示为空间中任意点的体积密度和方向发射辐射亮度。我们使用经典体渲染[16]的原理来渲染穿过场景的任何光线的颜色。体积密度 \(\sigma(\mathbf{x})\) 可以解释为光线在位置 \(\mathbf{x}\) 处的无穷小粒子处终止的微分概率。具有近边界 \(t_n\) 和远边界 \(t_f\) 的相机光线 \(\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}\) 的期望颜色 \(C(\mathbf{r})\) 为：</p>
<p>\[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt,\]</p>
<p>其中 \(T(t) = \exp\left(-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds\right)\)。 (1)</p>
</div>

<div class="original">
<p>6 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<p><strong>(a) View 1 (b) View 2 (c) Radiance Distributions</strong></p>
<p><strong>Fig. 3:</strong> A visualization of view-dependent emitted radiance. Our neural radiance field representation outputs RGB color as a 5D function of both spatial position \(\mathbf{x}\) and viewing direction \(\mathbf{d}\). Here, we visualize example directional color distributions for two spatial locations in our neural representation of the <em>Ship</em> scene. In (a) and (b), we show the appearance of two fixed 3D points from two different camera positions: one on the side of the ship (orange insets) and one on the surface of the water (blue insets). Our method predicts the changing specular appearance of these two 3D points, and in (c) we show how this behavior generalizes continuously across the whole hemisphere of viewing directions.</p>
</div>
<div class="translation">
<p>6 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<p><strong>(a) 视图1 (b) 视图2 (c) 辐射分布</strong></p>
<p><strong>图 3：</strong> 视角相关发射辐射亮度的可视化。我们的神经辐射场表示将RGB颜色输出为空间位置 \(\mathbf{x}\) 和观察方向 \(\mathbf{d}\) 的五维函数。这里，我们可视化了我们神经表示的<em>Ship</em>场景中两个空间位置的示例方向颜色分布。在(a)和(b)中，我们展示了从两个不同相机位置观察到的两个固定3D点的外观：一个在船的侧面（橙色插图），一个在水面上（蓝色插图）。我们的方法预测了这两个3D点变化的镜面外观，并在(c)中展示了这种行为如何在观察方向的整个半球上连续泛化。</p>
</div>

<div class="original">
<p>The function \(T(t)\) denotes the accumulated transmittance along the ray from \(t_n\) to \(t\), i.e., the probability that the ray travels from \(t_n\) to \(t\) without hitting any other particle. Rendering a view from our continuous neural radiance field requires estimating this integral \(C(\mathbf{r})\) for a camera ray traced through each pixel of the desired virtual camera.</p>
</div>
<div class="translation">
<p>函数 \(T(t)\) 表示沿光线从 \(t_n\) 到 \(t\) 的累积透射率，即光线从 \(t_n\) 传播到 \(t\) 而不撞击任何其他粒子的概率。从我们的连续神经辐射场渲染视图需要为穿过所需虚拟相机每个像素的光线估计此积分 \(C(\mathbf{r})\)。</p>
</div>

<div class="original">
<p>We numerically estimate this continuous integral using quadrature. Deterministic quadrature, which is typically used for rendering discretized voxel grids, would effectively limit our representation’s resolution because the MLP would only be queried at a fixed discrete set of locations. Instead, we use a stratified sampling approach where we partition \([t_n,t_f]\) into \(N\) evenly-spaced bins and then draw one sample uniformly at random from within each bin:</p>
<p>\[t_i \sim U\left[t_n + \frac{i-1}{N}(t_f - t_n), \, t_n + \frac{i}{N}(t_f - t_n)\right].\]</p>
</div>
<div class="translation">
<p>我们使用求积法数值估计此连续积分。确定性求积法通常用于渲染离散体素网格，它会有效限制我们表示的分辨率，因为MLP只会在固定的离散位置集上被查询。相反，我们使用分层采样方法，将 \([t_n,t_f]\) 划分为 \(N\) 个等间距的区间，然后从每个区间内均匀随机抽取一个样本：</p>
<p>\[t_i \sim U\left[t_n + \frac{i-1}{N}(t_f - t_n), \, t_n + \frac{i}{N}(t_f - t_n)\right].\]</p>
</div>

<div class="original">
<p>Although we use a discrete set of samples to estimate the integral, stratified sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization. We use these samples to estimate \(C(\mathbf{r})\) with the quadrature rule discussed in the volume rendering review by Max [26]:</p>
<p>\[\hat{C}(\mathbf{r}) = \sum_{i=1}^{N}T_i(1-\exp(-\sigma_i\delta_i))c_i, \quad \text{where } T_i = \exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right),\]</p>
<p>where \(\delta_i = t_{i+1} - t_i\) is the distance between adjacent samples. This function for calculating \(\hat{C}(\mathbf{r})\) from the set of \((c_i,\sigma_i)\) values is trivially differentiable and reduces to traditional alpha compositing with alpha values \(\alpha_i = 1 - \exp(-\sigma_i\delta_i)\).</p>
</div>
<div class="translation">
<p>尽管我们使用离散样本集来估计积分，但分层采样使我们能够表示连续的场景，因为它导致MLP在优化过程中在连续位置被评估。我们使用这些样本，采用Max在体渲染综述[26]中讨论的求积法则来估计 \(C(\mathbf{r})\)：</p>
<p>\[\hat{C}(\mathbf{r}) = \sum_{i=1}^{N}T_i(1-\exp(-\sigma_i\delta_i))c_i, \quad \text{其中 } T_i = \exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right),\]</p>
<p>其中 \(\delta_i = t_{i+1} - t_i\) 是相邻样本之间的距离。这个根据 \((c_i,\sigma_i)\) 值集合计算 \(\hat{C}(\mathbf{r})\) 的函数是平凡可微的，并且简化为具有alpha值 \(\alpha_i = 1 - \exp(-\sigma_i\delta_i)\) 的传统alpha合成。</p>
</div>

<div class="original">
<h2>5 Optimizing a Neural Radiance Field</h2>
<p>In the previous section we have described the core components necessary for modeling a scene as a neural radiance field and rendering novel views from this representation. However, we observe that these components are not sufficient for achieving state-of-the-art quality, as demonstrated in Section 6.4). We introduce two improvements to enable representing high-resolution complex scenes. The first is a positional encoding of the input coordinates that assists the MLP in representing high-frequency functions, and the second is a hierarchical sampling procedure that allows us to efficiently sample this high-frequency representation.</p>
</div>
<div class="translation">
<h2>5 优化神经辐射场</h2>
<p>在上一节中，我们描述了将场景建模为神经辐射场并从此表示渲染新视图所需的核心组件。然而，我们观察到这些组件不足以实现最先进的质量，如第6.4节所示。我们引入了两项改进以实现高分辨率复杂场景的表示。第一项是输入坐标的位置编码，帮助MLP表示高频函数；第二项是分层采样程序，使我们能够有效地采样此高频表示。</p>
</div>

<div class="original">
<h3>Positional encoding</h3>
<p>Despite the fact that neural networks are universal function approximators [14], we found that having the network \(F_{\Theta}\) directly operate on \(xyz\theta\phi\) input coordinates results in renderings that perform poorly at representing high-frequency variation in color and geometry. This is consistent with recent work by Rahaman <em>et al.</em> [35], which shows that deep networks are biased towards learning lower frequency functions. They additionally show that mapping the inputs to a higher dimensional space using high frequency functions before passing them to the network enables better fitting of data that contains high frequency variation.</p>
</div>
<div class="translation">
<h3>位置编码</h3>
<p>尽管神经网络是通用函数逼近器[14]，但我们发现让网络 \(F_{\Theta}\) 直接在 \(xyz\theta\phi\) 输入坐标上操作会导致渲染结果在表示颜色和几何的高频变化方面表现不佳。这与Rahaman <em>等人</em> [35] 最近的工作一致，该工作表明深度网络偏向于学习低频函数。他们还表明，在将输入传递给网络之前，使用高频函数将输入映射到更高维空间，能够更好地拟合包含高频变化的数据。</p>
</div>

<div class="original">
<p>We leverage these findings in the context of neural scene representations, and show that reformulating \(F_{\Theta}\) as a composition of two functions \(F_{\Theta}=F^{\prime}_{\Theta}\circ\gamma\), one learned and one not, significantly improves performance (see Fig. 4 and Table 2). Here \(\gamma\) is a mapping from \(\mathbb{R}\) into a higher dimensional space \(\mathbb{R}^{2L}\), and \(F^{\prime}_{\Theta}\) is still simply a regular MLP. Formally, the encoding function we use is:</p>
<p>\[\gamma(p)=\left(\sin\left(2^{0}\pi p\right),\cos\left(2^{0}\pi p\right),\cdots ,\sin\left(2^{L-1}\pi p\right),\cos\left(2^{L-1}\pi p\right)\right).\] (4)</p>
</div>
<div class="translation">
<p>我们在神经场景表示的背景下利用这些发现，并表明将 \(F_{\Theta}\) 重新表述为两个函数 \(F_{\Theta}=F^{\prime}_{\Theta}\circ\gamma\) 的复合（一个可学习，一个不可学习）可显著提高性能（见图4和表2）。这里 \(\gamma\) 是从 \(\mathbb{R}\) 到更高维空间 \(\mathbb{R}^{2L}\) 的映射，而 \(F^{\prime}_{\Theta}\) 仍然只是一个常规的MLP。形式上，我们使用的编码函数是：</p>
<p>\[\gamma(p)=\left(\sin\left(2^{0}\pi p\right),\cos\left(2^{0}\pi p\right),\cdots ,\sin\left(2^{L-1}\pi p\right),\cos\left(2^{L-1}\pi p\right)\right).\] (4)</p>
</div>

<div class="original">
<p>This function \(\gamma(\cdot)\) is applied separately to each of the three coordinate values in \(\mathbf{x}\) (which are normalized to lie in \([-1,1]\)) and to the three components of the</p>
</div>
<div class="translation">
<p>此函数 \(\gamma(\cdot)\) 分别应用于 \(\mathbf{x}\) 中的三个坐标值（它们被归一化到 \([-1,1]\) 范围内）以及</p>
</div>

<div class="original">
<strong>Figure 4:</strong> Here we visualize how our full model benefits from representing view-dependent emitted radiance and from passing our input coordinates through a high-frequency positional encoding. Removing view dependence prevents the model from recreating the specular reflection on the bulldozer tread. Removing the positional encoding drastically decreases the model’s ability to represent high frequency geometry and texture, resulting in an oversmoothed appearance.
</div>
<div class="translation">
<strong>图 4：</strong> 这里我们可视化了完整模型如何受益于表示视角相关发射辐射亮度以及通过高频位置编码传递输入坐标。移除视角依赖会阻止模型重建推土机履带上的镜面反射。移除位置编码会大幅降低模型表示高频几何和纹理的能力，导致外观过度平滑。
</div>

<div class="original">
<p>Cartesian viewing direction unit vector \(\mathbf{d}\) (which by construction lie in \([-1,1]\)). In our experiments, we set \(L=10\) for \(\gamma(\mathbf{x})\) and \(L=4\) for \(\gamma(\mathbf{d})\).</p>
</div>
<div class="translation">
<p>笛卡尔观察方向单位向量 \(\mathbf{d}\) 的三个分量（根据构造位于 \([-1,1]\) 内）。在我们的实验中，对于 \(\gamma(\mathbf{x})\) 我们设置 \(L=10\)，对于 \(\gamma(\mathbf{d})\) 设置 \(L=4\)。</p>
</div>

<div class="original">
<p>A similar mapping is used in the popular Transformer architecture [47], where it is referred to as a <em>positional encoding</em>. However, Transformers use it for a different goal of providing the discrete positions of tokens in a sequence as input to an architecture that does not contain any notion of order. In contrast, we use these functions to map continuous input coordinates into a higher dimensional space to enable our MLP to more easily approximate a higher frequency function. Concurrent work on a related problem of modeling 3D protein structure from projections [51] also utilizes a similar input coordinate mapping.</p>
</div>
<div class="translation">
<p>流行的Transformer架构[47]中使用了类似的映射，在那里它被称为<em>位置编码</em>。然而，Transformer使用它是为了不同的目的：为序列中标记的离散位置提供输入给一个不包含任何顺序概念的架构。相比之下，我们使用这些函数将连续输入坐标映射到更高维空间，使我们的MLP更容易逼近更高频的函数。在从投影建模3D蛋白质结构的相关问题上，同时期的工作[51]也利用了类似的输入坐标映射。</p>
</div>

<div class="original">
<h3>Hierarchical volume sampling</h3>
<p>Our rendering strategy of densely evaluating the neural radiance field network at \(N\) query points along each camera ray is inefficient: free space and occluded regions that do not contribute to the rendered image are still sampled repeatedly. We draw inspiration from early work in volume rendering [20] and propose a hierarchical representation that increases rendering efficiency by allocating samples proportionally to their expected effect on the final rendering.</p>
</div>
<div class="translation">
<h3>分层体采样</h3>
<p>我们在每条相机光线上的 \(N\) 个查询点密集评估神经辐射场网络的渲染策略是低效的：对渲染图像没有贡献的自由空间和被遮挡区域仍然被重复采样。我们从体渲染的早期工作[20]中汲取灵感，提出了一种分层表示，通过按样本对最终渲染的预期影响比例分配样本，来提高渲染效率。</p>
</div>

<div class="original">
<p>Instead of just using a single network to represent the scene, we simultaneously optimize two networks: one "coarse" and one "fine". We first sample a set of \(N_{c}\) locations using stratified sampling, and evaluate the "coarse" network at these locations as described in Eqns. 2 and 3. Given the output of this "coarse" network, we then produce a more informed sampling of points along each ray where samples are biased towards the relevant parts of the volume. To do this, we first rewrite the alpha composited color from the coarse network \(\hat{C}_{c}(\mathbf{r})\) in Eqn. 3 as a weighted sum of all sampled colors \(c_{i}\) along the ray:</p>
<p>\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}}w_{i}c_{i}\,,\quad\ w_{i}=T_{i}(1-\exp( -\sigma_{i}\delta_{i}))\,.\] (5)</p>
</div>
<div class="translation">
<p>我们不是仅使用单个网络来表示场景，而是同时优化两个网络：一个“粗糙”网络和一个“精细”网络。我们首先使用分层采样抽取一组 \(N_{c}\) 个位置，并如公式2和3所述在这些位置评估“粗糙”网络。给定这个“粗糙”网络的输出，我们随后沿着每条光线生成一个更有信息量的点采样，其中样本偏向于体积的相关部分。为此，我们首先将公式3中来自粗糙网络 \(\hat{C}_{c}(\mathbf{r})\) 的alpha合成颜色重写为沿光线所有采样颜色 \(c_{i}\) 的加权和：</p>
<p>\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}}w_{i}c_{i}\,,\quad\ w_{i}=T_{i}(1-\exp( -\sigma_{i}\delta_{i}))\,.\] (5)</p>
</div>

<div class="original">
<p>Normalizing these weights as \(\hat{w}_{i}=w_{i}/\sum_{j=1}^{N_{c}}w_{j}\) produces a piecewise-constant PDF along the ray. We sample a second set of \(N_{f}\) locations from this distribution using inverse transform sampling, evaluate our "fine" network at the union of the first and second set of samples, and compute the final rendered color of the ray \(\hat{C}_{f}(\mathbf{r})\) using Eqn. 3 but using all \(N_{c}+N_{f}\) samples. This procedure allocates more samples to regions we expect to contain visible content. This addresses a similar goal as importance sampling, but we use the sampled values as a nonuniform discretization of the whole integration domain rather than treating each sample as an independent probabilistic estimate of the entire integral.</p>
</div>
<div class="translation">
<p>将这些权重归一化为 \(\hat{w}_{i}=w_{i}/\sum_{j=1}^{N_{c}}w_{j}\) 会沿光线产生一个分段常数概率密度函数（PDF）。我们使用逆变换采样从此分布中抽取第二组 \(N_{f}\) 个位置，在第一组和第二组样本的并集上评估我们的“精细”网络，并使用公式3但使用所有 \(N_{c}+N_{f}\) 个样本来计算光线的最终渲染颜色 \(\hat{C}_{f}(\mathbf{r})\)。此过程将更多样本分配给我们预期包含可见内容的区域。这与重要性采样的目标类似，但我们使用采样值作为整个积分域的非均匀离散化，而不是将每个样本视为整个积分的独立概率估计。</p>
</div>

<div class="original">
<h3>Implementation details</h3>
<p>We optimize a separate neural continuous volume representation network for each scene. This requires only a dataset of captured RGB images of the scene,</p>
</div>
<div class="translation">
<h3>实现细节</h3>
<p>我们为每个场景优化一个独立的神经连续体积表示网络。这只需要场景的捕获RGB图像数据集、</p>
</div>

<div class="original">
<p>the corresponding camera poses and intrinsic parameters, and scene bounds (we use ground truth camera poses, intrinsics, and bounds for synthetic data, and use the COLMAP structure-from-motion package [39] to estimate these parameters for real data). At each optimization iteration, we randomly sample a batch of camera rays from the set of all pixels in the dataset, and then follow the hierarchical sampling described in Sec. 5.2 to query \(N_c\) samples from the coarse network and \(N_c + N_f\) samples from the fine network. We then use the volume rendering procedure described in Sec. 4 to render the color of each ray from both sets of samples. Our loss is simply the total squared error between the rendered and true pixel colors for both the coarse and fine renderings:</p>
<p>\[\mathcal{L} = \sum_{\mathbf{r}\in\mathcal{R}}\left[\left\|\hat{C}_c(\mathbf{r}) - C(\mathbf{r})\right\|_2^2 + \left\|\hat{C}_f(\mathbf{r}) - C(\mathbf{r})\right\|_2^2\right]\] (6)</p>
</div>
<div class="translation">
<p>相应的相机位姿和内在参数，以及场景边界（对于合成数据我们使用真实相机位姿、内参和边界，对于真实数据我们使用COLMAP运动恢复结构包[39]来估计这些参数）。在每次优化迭代中，我们从数据集所有像素的集合中随机采样一批相机光线，然后按照第5.2节描述的分层采样，从粗糙网络查询 \(N_c\) 个样本，从精细网络查询 \(N_c + N_f\) 个样本。然后，我们使用第4节描述的体渲染流程从两组样本中渲染每条光线的颜色。我们的损失函数就是粗糙渲染和精细渲染的渲染像素颜色与真实像素颜色之间的总平方误差：</p>
<p>\[\mathcal{L} = \sum_{\mathbf{r}\in\mathcal{R}}\left[\left\|\hat{C}_c(\mathbf{r}) - C(\mathbf{r})\right\|_2^2 + \left\|\hat{C}_f(\mathbf{r}) - C(\mathbf{r})\right\|_2^2\right]\] (6)</p>
</div>

<div class="original">
<p>where \(\mathcal{R}\) is the set of rays in each batch, and \(C(\mathbf{r})\), \(\hat{C}_c(\mathbf{r})\), and \(\hat{C}_f(\mathbf{r})\) are the ground truth, coarse volume predicted, and fine volume predicted RGB colors for ray \(\mathbf{r}\) respectively. Note that even though the final rendering comes from \(\hat{C}_f(\mathbf{r})\), we also minimize the loss of \(\hat{C}_c(\mathbf{r})\) so that the weight distribution from the coarse network can be used to allocate samples in the fine network.</p>
</div>
<div class="translation">
<p>其中 \(\mathcal{R}\) 是每批中的光线集合，\(C(\mathbf{r})\), \(\hat{C}_c(\mathbf{r})\) 和 \(\hat{C}_f(\mathbf{r})\) 分别是光线 \(\mathbf{r}\) 的真实RGB颜色、粗糙体积预测的RGB颜色和精细体积预测的RGB颜色。请注意，即使最终渲染来自 \(\hat{C}_f(\mathbf{r})\)，我们也最小化 \(\hat{C}_c(\mathbf{r})\) 的损失，以便来自粗糙网络的权重分布可以用于在精细网络中分配样本。</p>
</div>

<div class="original">
<p>In our experiments, we use a batch size of 4096 rays, each sampled at \(N_c = 64\) coordinates in the coarse volume and \(N_f = 128\) additional coordinates in the fine volume. We use the Adam optimizer [18] with a learning rate that begins at \(5 \times 10^{-4}\) and decays exponentially to \(5 \times 10^{-5}\) over the course of optimization (other Adam hyperparameters are left at default values of \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), and \(\epsilon = 10^{-7}\)). The optimization for a single scene typically take around 100-300k iterations to converge on a single NVIDIA V100 GPU (about 1-2 days).</p>
</div>
<div class="translation">
<p>在我们的实验中，我们使用4096条光线作为批量大小，每条光线在粗糙体积中采样 \(N_c = 64\) 个坐标，在精细体积中额外采样 \(N_f = 128\) 个坐标。我们使用Adam优化器[18]，学习率从 \(5 \times 10^{-4}\) 开始，并在优化过程中指数衰减到 \(5 \times 10^{-5}\)（其他Adam超参数保持默认值 \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), 和 \(\epsilon = 10^{-7}\))。优化单个场景通常需要约100-300k次迭代才能在单个NVIDIA V100 GPU上收敛（约1-2天）。</p>
</div>

<div class="original">
<h2>6 Results</h2>
<p>We quantitatively (Tables 1) and qualitatively (Figs. 8 and 6) show that our method outperforms prior work, and provide extensive ablation studies to validate our design choices (Table 2). We urge the reader to view our supplementary video to better appreciate our method's significant improvement over baseline methods when rendering smooth paths of novel views.</p>
</div>
<div class="translation">
<h2>6 结果</h2>
<p>我们在定量上（表1）和定性上（图8和图6）展示了我们的方法优于先前的工作，并提供了广泛的消融研究以验证我们的设计选择（表2）。我们强烈建议读者观看我们的补充视频，以更好地理解在渲染新视图的平滑路径时，我们的方法相对于基线方法的显著改进。</p>
</div>

<div class="original">
<h3>Datasets</h3>
<p><em>Synthetic renderings of objects</em> We first show experimental results on two datasets of synthetic renderings of objects (Table 1, "Diffuse Synthetic 360" and "Realistic Synthetic 360"). The DeepVoxels [41] dataset contains four Lambertian objects with simple geometry. Each object is rendered at \(512 \times 512\) pixels from viewpoints sampled on the upper hemisphere (\(479\) as input and \(1000\) for testing). We additionally generate our own dataset containing pathtraced images of eight objects that exhibit complicated geometry and realistic non-Lambertian materials. Six are rendered from viewpoints sampled on the upper hemisphere, and two are rendered from viewpoints sampled on a full sphere. We render 100 views of each scene as input and 200 for testing, all at \(800 \times 800\) pixels.</p>
</div>
<div class="translation">
<h3>数据集</h3>
<p><em>物体的合成渲染</em> 我们首先展示在两个物体合成渲染数据集上的实验结果（表1，“Diffuse Synthetic 360” 和 “Realistic Synthetic 360”）。DeepVoxels [41] 数据集包含四个具有简单几何形状的朗伯物体。每个物体在 \(512 \times 512\) 像素分辨率下渲染，视点在上半球采样（479个作为输入，1000个用于测试）。我们额外生成了自己的数据集，包含八个物体的路径追踪图像，这些物体具有复杂的几何形状和真实的非朗伯材质。其中六个从上半球采样的视点渲染，两个从整个球体采样的视点渲染。我们为每个场景渲染100个视图作为输入，200个用于测试，分辨率均为 \(800 \times 800\) 像素。</p>
</div>

<div class="original">
<p>10 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<table>
<thead>
<tr><th>Method</th><th colspan="3">Diffuse Synthetic 360° [41]</th><th colspan="3">Realistic Synthetic 360°</th><th colspan="3">Real Forward-Facing [28]</th></tr>
<tr><th></th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th></tr>
</thead>
<tbody>
<tr><td>SRN [42]</td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr>
<tr><td>NV [24]</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>LLFF [28]</td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr>
<tr><td>Ours</td><td><strong>40.15</strong></td><td>0.991</td><td><strong>0.023</strong></td><td><strong>31.01</strong></td><td><strong>0.947</strong></td><td><strong>0.081</strong></td><td><strong>26.50</strong></td><td><strong>0.811</strong></td><td><strong>0.250</strong></td></tr>
</tbody>
</table>
<p><strong>Table 1:</strong> Our method quantitatively outperforms prior work on datasets of both synthetic and real images. We report PSNR/SSIM (higher is better) and LPIPS [50] (lower is better). The DeepVoxels [41] dataset consists of 4 diffuse objects with simple geometry. Our realistic synthetic dataset consists of pathtraced renderings of 8 geometrically complex objects with complex non-Lambertian materials. The real dataset consists of handheld forward-facing captures of 8 real-world scenes (NV cannot be evaluated on this data because it only reconstructs objects inside a bounded volume). Though LLFF achieves slightly better LPIPS, we urge readers to view our supplementary video where our method achieves better multiview consistency and produces fewer artifacts than all baselines.</p>
</div>
<div class="translation">
<p>10 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<table>
<thead>
<tr><th>方法</th><th colspan="3">Diffuse Synthetic 360° [41]</th><th colspan="3">Realistic Synthetic 360°</th><th colspan="3">Real Forward-Facing [28]</th></tr>
<tr><th></th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th></tr>
</thead>
<tbody>
<tr><td>SRN [42]</td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr>
<tr><td>NV [24]</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>LLFF [28]</td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr>
<tr><td>Ours</td><td><strong>40.15</strong></td><td>0.991</td><td><strong>0.023</strong></td><td><strong>31.01</strong></td><td><strong>0.947</strong></td><td><strong>0.081</strong></td><td><strong>26.50</strong></td><td><strong>0.811</strong></td><td><strong>0.250</strong></td></tr>
</tbody>
</table>
<p><strong>表 1：</strong> 我们的方法在合成和真实图像数据集上定量优于先前的工作。我们报告了PSNR/SSIM（越高越好）和LPIPS[50]（越低越好）。DeepVoxels[41]数据集包含4个具有简单几何形状的漫反射物体。我们的真实感合成数据集包含8个具有复杂非朗伯材质的几何复杂物体的路径追踪渲染。真实数据集包含8个真实世界场景的手持前向捕捉（NV无法在此数据上评估，因为它只重建有界体积内的物体）。尽管LLFF在LPIPS上略优，但我们强烈建议读者观看我们的补充视频，其中我们的方法实现了更好的多视图一致性，并且产生的伪影少于所有基线。</p>
</div>

<div class="original">
<p>Real images of complex scenes We show results on complex real-world scenes captured with roughly forward-facing images (Table 1, “Real Forward-Facing”). This dataset consists of 8 scenes captured with a handheld cellphone (5 taken from the LLFF paper and 3 that we capture), captured with 20 to 62 images, and hold out \( \frac{1}{8} \) of these for the test set. All images are \( 1008 \times 756 \) pixels.</p>
</div>
<div class="translation">
<p>复杂场景的真实图像 我们在使用大致前向图像捕获的复杂真实世界场景上展示了结果（表1，“Real Forward-Facing”）。该数据集包含8个使用手持手机捕获的场景（5个来自LLFF论文，3个由我们捕获），捕获了20至62张图像，并保留其中的 \( \frac{1}{8} \) 作为测试集。所有图像均为 \( 1008 \times 756 \) 像素。</p>
</div>

<div class="original">
<h3>6.2 Comparisons</h3>
<p>To evaluate our model we compare against current top-performing techniques for view synthesis, detailed below. All methods use the same set of input views to train a separate network for each scene except Local Light Field Fusion [28], which trains a single 3D convolutional network on a large dataset, then uses the same trained network to process input images of new scenes at test time.</p>
</div>
<div class="translation">
<h3>6.2 比较</h3>
<p>为了评估我们的模型，我们与当前性能最佳的视图合成技术进行比较，详情如下。所有方法都使用相同的输入视图集为每个场景训练一个单独的网络，除了局部光场融合（Local Light Field Fusion，LLFF）[28]，它在一个大型数据集上训练一个单一的3D卷积网络，然后在测试时使用相同的训练网络处理新场景的输入图像。</p>
</div>

<div class="original">
<p><strong>Neural Volumes (NV) [24]</strong> synthesizes novel views of objects that lie entirely within a bounded volume in front of a distinct background (which must be separately captured without the object of interest). It optimizes a deep 3D convolutional network to predict a discretized RGB\(\alpha\) voxel grid with \(128^3\) samples as well as a 3D warp grid with \(32^3\) samples. The algorithm renders novel views by marching camera rays through the warped voxel grid.</p>
</div>
<div class="translation">
<p><strong>神经体积（Neural Volumes, NV）[24]</strong> 合成完全位于不同背景前方有界体积内的物体的新视图（该背景必须在没有目标物体的情况下单独捕获）。它优化一个深度3D卷积网络来预测一个具有 \(128^3\) 样本的离散化RGB\(\alpha\)体素网格以及一个具有 \(32^3\) 样本的3D扭曲网格。该算法通过在扭曲的体素网格中步进相机光线来渲染新视图。</p>
</div>

<div class="original">
<p><strong>Scene Representation Networks (SRN) [42]</strong> represent a continuous scene as an opaque surface, implicitly defined by a MLP that maps each \((x,y,z)\) coordinate to a feature vector. They train a recurrent neural network to march along a ray through the scene representation by using the feature vector at any 3D coordinate to predict the next step size along the ray. The feature vector from the final step is decoded into a single color for that point on the surface. Note that SRN is a better-performing followup to DeepVoxels [41] by the same authors, which is why we do not include comparisons to DeepVoxels.</p>
</div>
<div class="translation">
<p><strong>场景表示网络（Scene Representation Networks, SRN）[42]</strong> 将连续场景表示为一个不透明表面，该表面由一个MLP隐式定义，该MLP将每个 \((x,y,z)\) 坐标映射到一个特征向量。他们训练一个循环神经网络，通过使用任何3D坐标处的特征向量预测沿光线的下一步步长，沿着光线穿过场景表示。最后一步的特征向量被解码为该表面点处的单一颜色。请注意，SRN是同一作者对DeepVoxels[41]的改进版本，性能更好，因此我们不包含与DeepVoxels的比较。</p>
</div>

<div class="original">
<h1>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1>
<p><strong>Ship<br>Lego</strong></p>
<p><strong>Microphone<br>Materials</strong></p>
<p><strong>Ground Truth  NeRF (ours)   LLFF [28]   SRN [42]   NV [24]</strong></p>
<p><strong>Fig. 5:</strong> Comparisons on test-set views for scenes from our new synthetic dataset generated with a physically-based renderer. Our method is able to recover fine details in both geometry and appearance, such as Ship’s rigging, Lego’s gear and treads, Microphone’s shiny stand and mesh grille, and Material’s non-Lambertian reflectance. LLFF exhibits banding artifacts on the Microphone stand and Material’s object edges and ghosting artifacts in Ship’s mast and inside the Lego object. SRN produces blurry and distorted renderings in every case. Neural Volumes cannot capture the details on the Microphone’s grille or Lego’s gears, and it completely fails to recover the geometry of Ship’s rigging.</p>
</div>
<div class="translation">
<h1>NeRF：将场景表示为神经辐射场以实现视图合成</h1>
<p><strong>Ship<br>Lego</strong></p>
<p><strong>Microphone<br>Materials</strong></p>
<p><strong>真实值  NeRF (我们的)   LLFF [28]   SRN [42]   NV [24]</strong></p>
<p><strong>图 5：</strong> 在我们新的基于物理渲染器生成的合成数据集场景的测试集视图上的比较。我们的方法能够恢复几何和外观的精细细节，例如Ship的索具、Lego的齿轮和履带、Microphone闪亮的支架和网状格栅，以及Material的非朗伯反射。LLFF在Microphone支架和Material物体边缘上表现出带状伪影，在Ship的桅杆和Lego物体内部表现出重影伪影。SRN在所有情况下都产生模糊和扭曲的渲染。神经体积（NV）无法捕捉Microphone格栅或Lego齿轮的细节，并且完全无法恢复Ship索具的几何形状。</p>
</div>

<div class="original">
<p>12 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<p><strong>Fern</strong></p>
<p><strong>T-Rex</strong></p>
<p><strong>Orchid</strong></p>
<p><strong>Ground Truth    NeRF (ours)    LLFF [28]    SRN [42]</strong></p>
<p><strong>Fig. 6:</strong> Comparisons on test-set views of real world scenes. LLFF is specifically designed for this use case (forward-facing captures of real scenes). Our method is able to represent fine geometry more consistently across rendered views than LLFF, as shown in Fern’s leaves and the skeleton ribs and railing in T-rex. Our method also correctly reconstructs partially occluded regions that LLFF struggles to render cleanly, such as the yellow shelves behind the leaves in the bottom Fern crop and green leaves in the background of the bottom Orchid crop. Blending between multiples renderings can also cause repeated edges in LLFF, as seen in the top Orchid crop. SRN captures the low-frequency geometry and color variation in each scene but is unable to reproduce any fine detail.</p>
</div>
<div class="translation">
<p>12 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<p><strong>Fern</strong></p>
<p><strong>T-Rex</strong></p>
<p><strong>Orchid</strong></p>
<p><strong>真实值    NeRF (我们的)    LLFF [28]    SRN [42]</strong></p>
<p><strong>图 6：</strong> 真实世界场景测试集视图上的比较。LLFF是专为这种用例设计的（真实场景的前向捕捉）。我们的方法能够在渲染视图之间比LLFF更一致地表示精细几何结构，如Fern的叶子和T-rex中的骨架肋骨和栏杆所示。我们的方法还能正确重建LLFF难以清晰渲染的部分遮挡区域，例如底部Fern裁剪图中叶子后面的黄色架子和底部Orchid裁剪图中背景的绿叶。在多个渲染之间混合也会导致LLFF中出现重复边缘，如顶部Orchid裁剪图所示。SRN捕捉了每个场景的低频几何结构和颜色变化，但无法再现任何精细细节。</p>
</div>

<div class="original">
<p><strong>Local Light Field Fusion (LLFF) [28]</strong> LLFF is designed for producing photorealistic novel views for well-sampled forward facing scenes. It uses a trained 3D convolutional network to directly predict a discretized frustum-sampled RGB\(\alpha\) grid (multiphane image or MPI [52]) for each input view, then renders novel views by alpha compositing and blending nearby MPIs into the novel viewpoint.</p>
</div>
<div class="translation">
<p><strong>局部光场融合（Local Light Field Fusion, LLFF）[28]</strong> LLFF专为采样良好的前向场景生成光照真实的新视图而设计。它使用训练好的3D卷积网络直接为每个输入视图预测一个离散的视锥采样RGB\(\alpha\)网格（多平面图像或MPI[52]），然后通过alpha合成并将附近的MPI混合到新视点来渲染新视图。</p>
</div>

<div class="original">
<h3>Discussion</h3>
<p>We thoroughly outperform both baselines that also optimize a separate network per scene (NV and SRN) in all scenarios. Furthermore, we produce qualitatively and quantitatively superior renderings compared to LLFF (across all except one metric) while using only their input images as our entire training set.</p>
</div>
<div class="translation">
<h3>讨论</h3>
<p>在所有场景中，我们都全面优于同样为每个场景优化单独网络（NV和SRN）的基线。此外，与LLFF相比，我们产生了定性和定量上更优的渲染结果（除一项指标外），同时仅使用其输入图像作为我们整个训练集。</p>
</div>

<div class="original">
<p>The SRN method produces heavily smoothed geometry and texture, and its representational power for view synthesis is limited by selecting only a single depth and color per camera ray. The NV baseline is able to capture reasonably detailed volumetric geometry and appearance, but its use of an underlying explicit \(128^{3}\) voxel grid prevents it from scaling to represent fine details at high resolutions. LLFF specifically provides a "sampling guideline" to not exceed 64 pixels of disparity between input views, so it frequently fails to estimate correct geometry in the synthetic datasets which contain up to 400-500 pixels of disparity between views. Additionally, LLFF blends between different scene representations for rendering different views, resulting in perceptually-distracting inconsistency as is apparent in our supplementary video.</p>
</div>
<div class="translation">
<p>SRN方法产生严重平滑的几何和纹理，其视图合成的表示能力受限于每条相机光线仅选择一个深度和颜色。NV基线能够捕捉相当详细的体积几何和外观，但其使用的底层显式 \(128^{3}\) 体素网格阻止了它扩展到高分辨率下表示精细细节。LLFF特别提供了“采样指南”，要求输入视图之间的视差不超64像素，因此在合成数据集（视图间视差高达400-500像素）中经常无法估计正确的几何结构。此外，LLFF在渲染不同视图时混合不同的场景表示，导致感知上分散注意力的不一致性，这在我们的补充视频中很明显。</p>
</div>

<div class="original">
<p>The biggest practical tradeoffs between these methods are time versus space. All compared single scene methods take at least 12 hours to train per scene. In contrast, LLFF can process a small input dataset in under 10 minutes. However, LLFF produces a large 3D voxel grid for every input image, resulting in enormous storage requirements (over 15GB for one "Realistic Synthetic" scene). Our method requires only 5 MB for the network weights (a relative compression of \(3000\times\) compared to LLFF), which is even less memory than the <em>input images alone</em> for a single scene from any of our datasets.</p>
</div>
<div class="translation">
<p>这些方法之间最大的实际权衡是时间与空间。所有比较的单场景方法每个场景至少需要12小时来训练。相比之下，LLFF可以在10分钟内处理一个小的输入数据集。然而，LLFF为每个输入图像生成一个大的3D体素网格，导致巨大的存储需求（一个“Realistic Synthetic”场景超过15GB）。我们的方法仅需5MB用于网络权重（相对于LLFF有 \(3000\times\) 的相对压缩），这甚至少于我们任何数据集中单个场景<em>仅输入图像</em>所需的内存。</p>
</div>

<div class="original">
<h3>Ablation studies</h3>
<p>We validate our algorithm's design choices and parameters with an extensive ablation study in Table 2. We present results on our "Realistic Synthetic 360\(^{\circ}\)" scenes. Row 9 shows our complete model as a point of reference. Row 1 shows a minimalist version of our model without positional encoding (PE), view-dependence (VD), or hierarchical sampling (H). In rows 2-4 we remove these three components one at a time from the full model, observing that positional encoding (row 2) and view-dependence (row 3) provide the largest quantitative benefit followed by hierarchical sampling (row 4). Rows 5-6 show how our performance decreases as the number of input images is reduced. Note that our method's performance using only 25 input images still exceeds NV, SRN, and LLFF across all metrics when they are provided with 100 images (see supplementary material). In rows 7-8 we validate our choice of the maximum frequency</p>
</div>
<div class="translation">
<h3>消融研究</h3>
<p>我们在表2中通过广泛的消融研究验证了我们算法的设计选择和参数。我们在“Realistic Synthetic 360\(^{\circ}\)”场景上展示结果。第9行展示了我们完整的模型作为参考点。第1行展示了一个没有位置编码（PE）、视角依赖（VD）或分层采样（H）的极简版本模型。在第2-4行，我们从完整模型中依次移除这三个组件，观察到位置编码（第2行）和视角依赖（第3行）提供了最大的定量收益，其次是分层采样（第4行）。第5-6行展示了随着输入图像数量减少，我们的性能如何下降。请注意，当仅使用25张输入图像时，我们的方法在所有指标上的性能仍然超过使用100张图像的NV、SRN和LLFF（参见补充材料）。在第7-8行，我们验证了我们对最大频率的选择</p>
</div>

<div class="original">
<p>\begin{tabular}{l|l l l l l l}  & Input & \#Im. & \(L\) & \((N_c, N_f)\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline
1) No PE, VD, H & \(xyz\) & 100 & - & (256, -) & 26.67 & 0.906 & 0.136 \\
2) No Pos. Encoding & \(xyz\theta\phi\) & 100 & - & (64, 128) & 28.77 & 0.924 & 0.108 \\
3) No View Dependence & \(xyz\) & 100 & 10 & (64, 128) & 27.66 & 0.925 & 0.117 \\
4) No Hierarchical & \(xyz\theta\phi\) & 100 & 10 & (256, -) & 30.06 & 0.938 & 0.109 \\
5) Far Fewer Images & \(xyz\theta\phi\) & 25 & 10 & (64, 128) & 27.78 & 0.925 & 0.107 \\
6) Fewer Images & \(xyz\theta\phi\) & 50 & 10 & (64, 128) & 29.79 & 0.940 & 0.096 \\
7) Fewer Frequencies & \(xyz\theta\phi\) & 100 & 5 & (64, 128) & 30.59 & 0.944 & 0.088 \\
8) More Frequencies & \(xyz\theta\phi\) & 100 & 15 & (64, 128) & 30.81 & 0.946 & 0.096 \\
9) Complete Model & \(xyz\theta\phi\) & 100 & 10 & (64, 128) & \textbf{31.01} & \textbf{0.947} & \textbf{0.081} \\ \end{tabular}</p>
<p><strong>Table 2:</strong> An ablation study of our model. Metrics are averaged over the 8 scenes from our realistic synthetic dataset. See Sec. 6.4 for detailed descriptions.</p>
</div>
<div class="translation">
<p>\begin{tabular}{l|l l l l l l}  & 输入 & \#图像数 & \(L\) & \((N_c, N_f)\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline
1) 无 PE, VD, H & \(xyz\) & 100 & - & (256, -) & 26.67 & 0.906 & 0.136 \\
2) 无位置编码 & \(xyz\theta\phi\) & 100 & - & (64, 128) & 28.77 & 0.924 & 0.108 \\
3) 无视角依赖 & \(xyz\) & 100 & 10 & (64, 128) & 27.66 & 0.925 & 0.117 \\
4) 无分层采样 & \(xyz\theta\phi\) & 100 & 10 & (256, -) & 30.06 & 0.938 & 0.109 \\
5) 极少图像 & \(xyz\theta\phi\) & 25 & 10 & (64, 128) & 27.78 & 0.925 & 0.107 \\
6) 较少图像 & \(xyz\theta\phi\) & 50 & 10 & (64, 128) & 29.79 & 0.940 & 0.096 \\
7) 较少频率 & \(xyz\theta\phi\) & 100 & 5 & (64, 128) & 30.59 & 0.944 & 0.088 \\
8) 更多频率 & \(xyz\theta\phi\) & 100 & 15 & (64, 128) & 30.81 & 0.946 & 0.096 \\
9) 完整模型 & \(xyz\theta\phi\) & 100 & 10 & (64, 128) & \textbf{31.01} & \textbf{0.947} & \textbf{0.081} \\ \end{tabular}</p>
<p><strong>表 2：</strong> 我们模型的消融研究。指标在我们真实感合成数据集的8个场景上取平均值。详细描述见第6.4节。</p>
</div>

<div class="original">
<p>\(L\) used in our positional encoding for \(\mathbf{x}\) (the maximum frequency used for \(\mathbf{d}\) is scaled proportionally). Only using 5 frequencies reduces performance, but increasing the number of frequencies from 10 to 15 does not improve performance. We believe the benefit of increasing \(L\) is limited once \(2^{L}\) exceeds the maximum frequency present in the sampled input images (roughly 1024 in our data).</p>
</div>
<div class="translation">
<p>用于 \(\mathbf{x}\) 的位置编码中的 \(L\)（用于 \(\mathbf{d}\) 的最大频率按比例缩放）。仅使用5个频率会降低性能，但将频率数量从10增加到15并不能提高性能。我们认为一旦 \(2^{L}\) 超过采样输入图像中存在的最大频率（在我们的数据中约为1024），增加 \(L\) 的收益就有限了。</p>
</div>

<div class="original">
<h2>7 Conclusion</h2>
<p>Our work directly addresses deficiencies of prior work that uses MLPs to represent objects and scenes as continuous functions. We demonstrate that representing scenes as 5D neural radiance fields (an MLP that outputs volume density and view-dependent emitted radiance as a function of 3D location and 2D viewing direction) produces better renderings than the previously-dominant approach of training deep convolutional networks to output discretized voxel representations.</p>
</div>
<div class="translation">
<h2>7 结论</h2>
<p>我们的工作直接解决了先前使用MLP将物体和场景表示为连续函数的工作的不足。我们证明了将场景表示为五维神经辐射场（一个输出体积密度和视角相关发射辐射亮度作为3D位置和2D观察方向函数的MLP），相比于之前主导的通过训练深度卷积网络输出离散体素表示的方法，能够产生更好的渲染结果。</p>
</div>

<div class="original">
<p>Although we have proposed a hierarchical sampling strategy to make rendering more sample-efficient (for both training and testing), there is still much more progress to be made in investigating techniques to efficiently optimize and render neural radiance fields. Another direction for future work is interpretability: sampled representations such as voxel grids and meshes admit reasoning about the expected quality of rendered views and failure modes, but it is unclear how to analyze these issues when we encode scenes in the weights of a deep neural network. We believe that this work makes progress towards a graphics pipeline based on real world imagery, where complex scenes could be composed of neural radiance fields optimized from images of actual objects and scenes.</p>
</div>
<div class="translation">
<p>尽管我们提出了分层采样策略以使渲染更加样本高效（对于训练和测试都是如此），但在研究高效优化和渲染神经辐射场的技术方面仍有许多进展需要取得。未来工作的另一个方向是可解释性：像体素网格和网格这样的采样表示允许对渲染视图的预期质量和故障模式进行推理，但当我们把场景编码在深度神经网络的权重中时，如何分析这些问题尚不清楚。我们相信这项工作在实现基于真实世界图像的图形流水线方面取得了进展，其中复杂场景可以由实际物体和场景图像优化而来的神经辐射场组成。</p>
</div>

<div class="original">
<p><strong>Acknowledgements</strong> We thank Kevin Cao, Guowei Frank Yang, and Nithin Raghavan for comments and discussions. RR acknowledges funding from ONR grants N000141712687 and N000142012529 and the Ronald L. Graham Chair. BM is funded by a Hertz Foundation Fellowship, and MT is funded by an NSF Graduate Fellowship. Google provided a generous donation of cloud compute credits through the BAIR Commons program. We thank the following</p>
</div>
<div class="translation">
<p><strong>致谢</strong> 我们感谢Kevin Cao、Guowei Frank Yang和Nithin Raghavan的评论和讨论。RR感谢ONR基金N000141712687、N000142012529以及Ronald L. Graham讲席的资助。BM由Hertz基金会奖学金资助，MT由NSF研究生奖学金资助。谷歌通过BAIR Commons计划慷慨捐赠了云计算积分。我们感谢以下</p>
</div>

<div class="original">
<p>Blend Swap users for the models used in our realistic synthetic dataset: gregzaal (ship), 1DInc (chair), bryanajones (drums), Herberhold (ficus), erickfree (hotdog), Heinzelnisse (lego), elbrujodelatribu (materials), and up3d.de (mic).</p>
</div>
<div class="translation">
<p>Blend Swap用户提供我们真实感合成数据集使用的模型：gregzaal (ship), 1DInc (chair), bryanajones (drums), Herberhold (ficus), erickfree (hotdog), Heinzelnisse (lego), elbrujodelatribu (materials), 以及 up3d.de (mic)。</p>
</div>

<!-- 参考文献部分因篇幅原因省略翻译，保持原文 -->
<div class="original">
<h2>References</h2>
<p>[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015)</p>
<!-- ... 其他参考文献条目 ... -->
<p>[52] Zhou, T., Tucker, R., Flynn, J., Pyffe, G., Snavely, N.: Stereo magnification: Learning view synthesis using multiplane images. ACM Transactions on Graphics (SIGGRAPH) (2018)</p>
</div>
<div class="translation">
<h2>参考文献</h2>
<p>[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow：异构系统上的大规模机器学习 (2015)</p>
<!-- ... 其他参考文献条目翻译（此处省略以节省空间）... -->
<p>[52] Zhou, T., Tucker, R., Flynn, J., Pyffe, G., Snavely, N.: 立体放大：使用多平面图像学习视图合成。 ACM Transactions on Graphics (SIGGRAPH) (2018)</p>
</div>

<!-- 附录部分省略翻译，保持原文 -->
<div class="original">
<h2>Appendix A Additional Implementation Details</h2>
<p><strong>Network Architecture</strong> Fig. 7 details our simple fully-connected architecture.</p>
<p><strong>Volume Bounds</strong> Our method renders views by querying the neural radiance field representation at continuous 5D coordinates along camera rays. For experiments with synthetic images, we scale the scene so that it lies within a cube of side length 2 centered at the origin, and only query the representation within this bounding volume. Our dataset of real images contains content that can exist anywhere between the closest point and infinity, so we use normalized device coordinates to map the depth range of these points into \([-1,1]\). This shifts all the ray origins to the near plane of the scene, maps the perspective rays of the camera to parallel rays in the transformed volume, and uses disparity (inverse depth) instead of metric depth, so all coordinates are now bounded.</p>
</div>
<div class="translation">
<h2>附录 A 附加实现细节</h2>
<p><strong>网络架构</strong> 图7详细说明了我们简单的全连接架构。</p>
<p><strong>体积边界</strong> 我们的方法通过沿相机光线在连续五维坐标上查询神经辐射场表示来渲染视图。对于合成图像的实验，我们将场景缩放，使其位于以原点为中心、边长为2的立方体内，并仅在此边界体积内查询表示。我们的真实图像数据集包含可能存在于最近点到无穷远之间任何位置的内容，因此我们使用归一化设备坐标（NDC）将这些点的深度范围映射到 \([-1,1]\)。这将所有光线起点移动到场景的近平面，将相机的透视光线映射到变换后体积中的平行光线，并使用视差（逆深度）代替度量深度，因此所有坐标现在都是有界的。</p>
</div>

<!-- ... 后续附录内容省略 ... -->

</body>
</html><!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<style>
  .original { margin-bottom: 5px; }
  .translation { margin-bottom: 20px; color: #2E86C1; }
  h1, h2, h3, h4 { color: #1A5276; }
  table { border-collapse: collapse; margin: 15px 0; }
  th, td { border: 1px solid #ddd; padding: 8px; }
</style>
</head>
<body>

<div class="original">
<h1>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1>
</div>
<div class="translation">
<h1>NeRF：将场景表示为神经辐射场以实现视图合成</h1>
</div>

<div class="original">
<p>Ben Mildenhall<sup>1</sup>*</p>
<p>Pratul P. Srinivasan<sup>1</sup></p>
<p>Matthew Tancik<sup>1</sup></p>
<p>Jonathan T. Barron<sup>2</sup></p>
<p>Ravi Ramamoorthi<sup>3</sup></p>
<p>Ren Ng<sup>1</sup></p>
<p><sup>1</sup>UC Berkeley <sup>2</sup>Google Research <sup>3</sup>UC San Diego</p>
</div>
<div class="translation">
<p>本·米尔登霍尔<sup>1</sup>*</p>
<p>普拉图尔·P·斯里尼瓦桑<sup>1</sup></p>
<p>马修·坦西克<sup>1</sup></p>
<p>乔纳森·T·巴伦<sup>2</sup></p>
<p>拉维·拉玛莫西<sup>3</sup></p>
<p>吴韧<sup>1</sup></p>
<p><sup>1</sup>加州大学伯克利分校 <sup>2</sup>谷歌研究院 <sup>3</sup>加州大学圣地亚哥分校</p>
</div>

<div class="original">
<h4>Abstract</h4>
<p>We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \((x,y,z)\) and viewing direction \((\theta,\phi)\)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.</p>
<p><strong>Keywords:</strong> scene representation, view synthesis, image-based rendering, volume rendering, 3D deep learning</p>
</div>
<div class="translation">
<h4>摘要</h4>
<p>我们提出了一种方法，通过使用一组稀疏的输入视图优化底层连续的体场景函数，在合成复杂场景的新视图方面取得了最先进的结果。我们的算法使用全连接（非卷积）深度网络表示场景，其输入是单个连续的五维坐标（空间位置 \((x,y,z)\) 和观察方向 \((\theta,\phi)\)），输出是该空间位置的体积密度和视角相关的辐射亮度。我们通过沿相机光线查询五维坐标来合成视图，并使用经典的体渲染技术将输出的颜色和密度投影成图像。由于体渲染本质上是可微的，优化我们表示所需的唯一输入是一组已知相机位姿的图像。我们描述了如何有效优化神经辐射场以渲染具有复杂几何形状和外观场景的光照真实新视图，并展示了优于先前神经渲染和视图合成工作的结果。视图合成结果最好以视频形式观看，因此我们强烈建议读者观看补充视频以获得令人信服的比较。</p>
<p><strong>关键词：</strong> 场景表示，视图合成，基于图像的渲染，体渲染，3D深度学习</p>
</div>

<div class="original">
<h2>1 Introduction</h2>
<p>In this work, we address the long-standing problem of view synthesis in a new way by directly optimizing parameters of a continuous 5D scene representation to minimize the error of rendering a set of captured images.</p>
<p>We represent a static scene as a continuous 5D function that outputs the radiance emitted in each direction \((\theta,\phi)\) at each point \((x,y,z)\) in space, and a density at each point which acts like a differential opacity controlling how much radiance is accumulated by a ray passing through \((x,y,z)\). Our method optimizes a deep fully-connected neural network without any convolutional layers (often referred to as a multilayer perceptron or MLP) to represent this function by regressing from a single 5D coordinate \((x,y,z,\theta,\phi)\) to a single volume density and view-dependent RGB color. To render this <em>neural radiance field</em> (NeRF)</p>
</div>
<div class="translation">
<h2>1 引言</h2>
<p>在这项工作中，我们通过直接优化连续五维场景表示的参数来最小化渲染一组捕获图像的误差，以一种新方式解决了视图合成这个长期存在的问题。</p>
<p>我们将静态场景表示为一个连续的五维函数，该函数输出空间中每个点 \((x,y,z)\) 沿每个方向 \((\theta,\phi)\) 发出的辐射亮度，以及每点的密度（类似于微分不透明度，控制穿过 \((x,y,z)\) 的光线累积多少辐射亮度）。我们的方法优化一个没有任何卷积层的深度全连接神经网络（通常称为多层感知器或MLP）来表示此函数，它从单个五维坐标 \((x,y,z,\theta,\phi)\) 回归出单个体积密度和视角相关的RGB颜色。为了渲染这个<em>神经辐射场</em>（NeRF）</p>
</div>

<div class="original">
<p>2 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<p><strong>Input Images</strong></p>
<p><strong>Optimize NeRF</strong></p>
<p><strong>Render new views</strong></p>
<p><strong>Fig. 1:</strong> We present a method that optimizes a continuous 5D neural radiance field representation (volume density and view-dependent color at any continuous location) of a scene from a set of input images. We use techniques from volume rendering to accumulate samples of this scene representation along rays to render the scene from any viewpoint. Here, we visualize the set of 100 input views of the synthetic Drums scene randomly captured on a surrounding hemisphere, and we show two novel views rendered from our optimized NeRF representation.</p>
</div>
<div class="translation">
<p>2 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<p><strong>输入图像</strong></p>
<p><strong>优化 NeRF</strong></p>
<p><strong>渲染新视图</strong></p>
<p><strong>图 1：</strong> 我们提出了一种方法，从一组输入图像中优化场景的连续五维神经辐射场表示（任何连续位置的体积密度和视角相关颜色）。我们使用体渲染技术沿光线累积此场景表示的样本，以从任意视点渲染场景。这里，我们可视化了在周围半球上随机捕获的合成Drums场景的100个输入视图集，并展示了从我们优化后的NeRF表示渲染出的两个新视图。</p>
</div>

<div class="original">
<p>from a particular viewpoint we: 1) march camera rays through the scene to generate a sampled set of 3D points, 2) use those points and their corresponding 2D viewing directions as input to the neural network to produce an output set of colors and densities, and 3) use classical volume rendering techniques to accumulate those colors and densities into a 2D image. Because this process is naturally differentiable, we can use gradient descent to optimize this model by minimizing the error between each observed image and the corresponding views rendered from our representation. Minimizing this error across multiple views encourages the network to predict a coherent model of the scene by assigning high volume densities and accurate colors to the locations that contain the true underlying scene content. Figure 2 visualizes this overall pipeline.</p>
</div>
<div class="translation">
<p>从特定视点出发，我们：1）在场景中步进相机光线以生成采样的3D点集，2）使用这些点及其对应的2D观察方向作为神经网络的输入，生成颜色和密度输出集，3）使用经典体渲染技术将这些颜色和密度累积成2D图像。由于这个过程本质上是可微的，我们可以使用梯度下降法通过最小化每个观测图像与从我们表示中渲染出的对应视图之间的误差来优化此模型。在多个视图上最小化此误差，促使网络通过将高体积密度和准确颜色分配给包含真实底层场景内容的位置来预测场景的一致模型。图2可视化了这个整体流程。</p>
</div>

<div class="original">
<p>We find that the basic implementation of optimizing a neural radiance field representation for a complex scene does not converge to a sufficiently high-resolution representation and is inefficient in the required number of samples per camera ray. We address these issues by transforming input 5D coordinates with a positional encoding that enables the MLP to represent higher frequency functions, and we propose a hierarchical sampling procedure to reduce the number of queries required to adequately sample this high-frequency scene representation.</p>
</div>
<div class="translation">
<p>我们发现，针对复杂场景优化神经辐射场表示的基本实现无法收敛到足够高分辨率的表示，并且在每条相机光线所需的样本数量上效率低下。我们通过使用位置编码转换输入的五维坐标来解决这些问题，该编码使MLP能够表示更高频的函数，并提出一种分层采样程序以减少充分采样此高频场景表示所需的查询次数。</p>
</div>

<div class="original">
<p>Our approach inherits the benefits of volumetric representations: both can represent complex real-world geometry and appearance and are well suited for gradient-based optimization using projected images. Crucially, our method overcomes the prohibitive storage costs of discretized voxel grids when modeling complex scenes at high-resolutions. In summary, our technical contributions are:</p>
<ul>
<li>An approach for representing continuous scenes with complex geometry and materials as 5D neural radiance fields, parameterized as basic MLP networks.</li>
<li>A differentiable rendering procedure based on classical volume rendering techniques, which we use to optimize these representations from standard RGB images. This includes a hierarchical sampling strategy to allocate the MLP’s capacity towards space with visible scene content.</li>
</ul>
</div>
<div class="translation">
<p>我们的方法继承了体表示的优势：两者都能表示复杂的真实世界几何形状和外观，并且非常适合使用投影图像进行基于梯度的优化。关键的是，我们的方法克服了在高分辨率建模复杂场景时离散体素网格的过高存储成本。总之，我们的技术贡献是：</p>
<ul>
<li>一种将具有复杂几何形状和材质的连续场景表示为五维神经辐射场的方法，参数化为基本的MLP网络。</li>
<li>一种基于经典体渲染技术的可微渲染流程，我们用它从标准RGB图像优化这些表示。这包括一种分层采样策略，用于将MLP的能力分配给具有可见场景内容的空间。</li>
</ul>
</div>

<div class="original">
<p>- A positional encoding to map each input 5D coordinate into a higher dimensional space, which enables us to successfully optimize neural radiance fields to represent high-frequency scene content.</p>
</div>
<div class="translation">
<p>- 一种位置编码，将每个输入的五维坐标映射到更高维空间，使我们能够成功优化神经辐射场以表示高频场景内容。</p>
</div>

<div class="original">
<p>We demonstrate that our resulting neural radiance field method quantitatively and qualitatively outperforms state-of-the-art view synthesis methods, including works that fit neural 3D representations to scenes as well as works that train deep convolutional networks to predict sampled volumetric representations. As far as we know, this paper presents the first continuous neural scene representation that is able to render high-resolution photorealistic novel views of real objects and scenes from RGB images captured in natural settings.</p>
</div>
<div class="translation">
<p>我们证明，我们得到的神经辐射场方法在定量和定性上都优于最先进的视图合成方法，包括将神经3D表示拟合到场景的工作以及训练深度卷积网络以预测采样体表示的工作。据我们所知，本文提出了第一个连续的神经场景表示，能够从自然环境中捕获的RGB图像渲染真实物体和场景的高分辨率光照真实新视图。</p>
</div>

<div class="original">
<h2>2 Related Work</h2>
<p>A promising recent direction in computer vision is encoding objects and scenes in the weights of an MLP that directly maps from a 3D spatial location to an implicit representation of the shape, such as the signed distance [6] at that location. However, these methods have so far been unable to reproduce realistic scenes with complex geometry with the same fidelity as techniques that represent scenes using discrete representations such as triangle meshes or voxel grids. In this section, we review these two lines of work and contrast them with our approach, which enhances the capabilities of neural scene representations to produce state-of-the-art results for rendering complex realistic scenes.</p>
</div>
<div class="translation">
<h2>2 相关工作</h2>
<p>计算机视觉中一个有前景的最新方向是将物体和场景编码在MLP的权重中，该MLP直接从3D空间位置映射到形状的隐式表示，例如该位置的有向距离[6]。然而，这些方法迄今无法以与使用三角形网格或体素网格等离散表示来表示场景的技术相同的保真度再现具有复杂几何形状的真实场景。在本节中，我们回顾了这两类工作，并将其与我们的方法进行对比，我们的方法增强了神经场景表示的能力，以产生渲染复杂真实场景的最新结果。</p>
</div>

<div class="original">
<p>A similar approach of using MLPs to map from low-dimensional coordinates to colors has also been used for representing other graphics functions such as images [44], textured materials [36, 37, 12, 31], and indirect illumination values [38].</p>
</div>
<div class="translation">
<p>使用MLP从低维坐标映射到颜色的类似方法也被用于表示其他图形函数，如图像[44]、纹理材质[36, 37, 12, 31]和间接光照值[38]。</p>
</div>

<div class="original">
<p><strong>Neural 3D shape representations</strong> Recent work has investigated the implicit representation of continuous 3D shapes as level sets by optimizing deep networks that map \(xyz\) coordinates to signed distance functions [32, 15] or occupancy fields [27, 11]. However, these models are limited by their requirement of access to ground truth 3D geometry, typically obtained from synthetic 3D shape datasets such as ShapeNet [3]. Subsequent work has relaxed this requirement of ground truth 3D shapes by formulating differentiable rendering functions that allow neural implicit shape representations to be optimized using only 2D images. Niemeyer <em>et al.</em> [29] represent surfaces as 3D occupancy fields and use a numerical method to find the surface intersection for each ray, then calculate an exact derivative using implicit differentiation. Each ray intersection location is provided as the input to a neural 3D texture field that predicts a diffuse color for that point. Sitzmann <em>et al.</em> [42] use a less direct neural 3D representation that simply outputs a feature vector and RGB color at each continuous 3D coordinate, and propose a differentiable rendering function consisting of a recurrent neural network that marches along each ray to decide where the surface is located.</p>
</div>
<div class="translation">
<p><strong>神经3D形状表示</strong> 最近的工作研究了通过优化将\(xyz\)坐标映射到有向距离函数[32, 15]或占据场[27, 11]的深度网络，将连续3D形状隐式表示为水平集。然而，这些模型受限于它们需要访问真实3D几何形状的要求，这通常从合成3D形状数据集（如ShapeNet[3]）获得。后续工作通过制定可微渲染函数放宽了对真实3D形状的要求，使得神经隐式形状表示可以仅使用2D图像进行优化。Niemeyer <em>等人</em> [29] 将表面表示为3D占据场，并使用数值方法查找每条光线的表面交点，然后使用隐式微分计算精确导数。每个光线交点位置作为神经3D纹理场的输入，该场预测该点的漫反射颜色。Sitzmann <em>等人</em> [42] 使用一种不那么直接的神经3D表示，它仅在每个连续3D坐标处输出一个特征向量和RGB颜色，并提出一个由循环神经网络组成的可微渲染函数，该网络沿每条光线步进以确定表面位置。</p>
</div>

<div class="original">
<p>Though these techniques can potentially represent complicated and high-resolution geometry, they have so far been limited to simple shapes with low geometric complexity, resulting in oversmoothed renderings. We show that an alternate strategy of optimizing networks to encode 5D radiance fields (3D volumes</p>
</div>
<div class="translation">
<p>尽管这些技术有潜力表示复杂和高分辨率的几何形状，但迄今为止它们仅限于几何复杂度低的简单形状，导致渲染结果过度平滑。我们展示了一种替代策略，即优化网络以编码五维辐射场（3D体积</p>
</div>

<div class="original">
<p>with 2D view-dependent appearance) can represent higher-resolution geometry and appearance to render photorealistic novel views of complex scenes.</p>
</div>
<div class="translation">
<p>具有2D视角相关外观），可以表示更高分辨率的几何形状和外观，以渲染复杂场景的光照真实新视图。</p>
</div>

<div class="original">
<p><strong>View synthesis and image-based rendering</strong> Given a dense sampling of views, photorealistic novel views can be reconstructed by simple light field sample interpolation techniques [21, 5, 7]. For novel view synthesis with sparser view sampling, the computer vision and graphics communities have made significant progress by predicting traditional geometry and appearance representations from observed images. One popular class of approaches uses mesh-based representations of scenes with either diffuse [48] or view-dependent [2, 8, 49] appearance. Differentiable rasterizers [4, 10, 23, 25] or pathtracers [22, 30] can directly optimize mesh representations to reproduce a set of input images using gradient descent. However, gradient-based mesh optimization based on image reprojection is often difficult, likely because of local minima or poor conditioning of the loss landscape. Furthermore, this strategy requires a template mesh with fixed topology to be provided as an initialization before optimization [22], which is typically unavailable for unconstrained real-world scenes.</p>
</div>
<div class="translation">
<p><strong>视图合成与基于图像的渲染</strong> 给定密集采样的视图，可以通过简单的光场样本插值技术[21, 5, 7]重建光照真实的新视图。对于视图采样更稀疏的新视图合成，计算机视觉和图形学界通过从观测图像预测传统几何和外观表示取得了显著进展。一类流行的方法使用基于网格的场景表示，具有漫反射[48]或视角相关[2, 8, 49]的外观。可微分光栅化器[4, 10, 23, 25]或路径追踪器[22, 30]可以直接优化网格表示，以使用梯度下降法重现一组输入图像。然而，基于图像重投影的梯度网格优化通常很困难，可能是由于局部最小值或损失景观的条件不良。此外，此策略需要在优化前提供具有固定拓扑的模板网格作为初始化[22]，这对于无约束的真实世界场景通常不可用。</p>
</div>

<div class="original">
<p>Another class of methods use volumetric representations to address the task of high-quality photorealistic view synthesis from a set of input RGB images. Volumetric approaches are able to realistically represent complex shapes and materials, are well-suited for gradient-based optimization, and tend to produce less visually distracting artifacts than mesh-based methods. Early volumetric approaches used observed images to directly color voxel grids [19, 40, 45]. More recently, several methods [9, 13, 17, 28, 33, 43, 46, 52] have used large datasets of multiple scenes to train deep networks that predict a sampled volumetric representation from a set of input images, and then use either alpha-compositing [34] or learned compositing along rays to render novel views at test time. Other works have optimized a combination of convolutional networks (CNNs) and sampled voxel grids for each specific scene, such that the CNN can compensate for discretization artifacts from low resolution voxel grids [41] or allow the predicted voxel grids to vary based on input time or animation controls [24]. While these volumetric techniques have achieved impressive results for novel view synthesis, their ability to scale to higher resolution imagery is fundamentally limited by poor time and space complexity due to their discrete sampling -- rendering higher resolution images requires a finer sampling of 3D space. We circumvent this problem by instead encoding a <em>continuous</em> volume within the parameters of a deep fully-connected neural network, which not only produces significantly higher quality renderings than prior volumetric approaches, but also requires just a fraction of the storage cost of those <em>sampled</em> volumetric representations.</p>
</div>
<div class="translation">
<p>另一类方法使用体表示来处理从一组输入RGB图像进行高质量光照真实视图合成的任务。体方法能够真实地表示复杂的形状和材质，非常适合基于梯度的优化，并且往往比基于网格的方法产生更少视觉干扰的伪影。早期的体方法使用观测图像直接为体素网格着色[19, 40, 45]。最近，一些方法[9, 13, 17, 28, 33, 43, 46, 52]使用多个场景的大型数据集训练深度网络，从一组输入图像预测采样的体表示，然后在测试时使用alpha合成[34]或沿光线学习的合成来渲染新视图。其他工作针对每个特定场景优化了卷积网络（CNN）和采样体素网格的组合，使得CNN可以补偿低分辨率体素网格的离散化伪影[41]，或允许预测的体素网格根据输入时间或动画控制而变化[24]。虽然这些体技术在视图合成方面取得了令人印象深刻的结果，但它们扩展到更高分辨率图像的能力从根本上受到离散采样导致的较差时间和空间复杂度的限制——渲染更高分辨率的图像需要对3D空间进行更精细的采样。我们通过在深度全连接神经网络的参数内编码一个<em>连续</em>体积来规避这个问题，这不仅产生了比先前体方法显著更高质量的渲染，而且只需要这些<em>采样</em>体表示的一小部分存储成本。</p>
</div>

<div class="original">
<h2>3 Neural Radiance Field Scene Representation</h2>
<p>We represent a continuous scene as a 5D vector-valued function whose input is a 3D location \(\mathbf{x}=(x,y,z)\) and 2D viewing direction \((\theta,\phi)\), and whose output is an emitted color \(\mathbf{c}=(r,g,b)\) and volume density \(\sigma\). In practice, we express</p>
</div>
<div class="translation">
<h2>3 神经辐射场场景表示</h2>
<p>我们将连续场景表示为一个五维向量值函数，其输入是三维位置 \(\mathbf{x}=(x,y,z)\) 和二维观察方向 \((\theta,\phi)\)，输出是发出的颜色 \(\mathbf{c}=(r,g,b)\) 和体积密度 \(\sigma\)。在实践中，我们将</p>
</div>

<div class="original">
<h1>NerF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1>
<p><strong>5D Input Position + Direction</strong></p>
<p><strong>Output Color + Density</strong></p>
<p><strong>Volume Rendering Rendering Loss</strong></p>
<p><strong>(a) (b) (c) (d)</strong></p>
<p><strong>Fig. 2:</strong> An overview of our neural radiance field scene representation and differentiable rendering procedure. We synthesize images by sampling 5D coordinates (location and viewing direction) along camera rays (a), feeding those locations into an MLP to produce a color and volume density (b), and using volume rendering techniques to composite these values into an image (c). This rendering function is differentiable, so we can optimize our scene representation by minimizing the residual between synthesized and ground truth observed images (d).</p>
</div>
<div class="translation">
<h1>NeRF：将场景表示为神经辐射场以实现视图合成</h1>
<p><strong>五维输入位置 + 方向</strong></p>
<p><strong>输出颜色 + 密度</strong></p>
<p><strong>体渲染 渲染损失</strong></p>
<p><strong>(a) (b) (c) (d)</strong></p>
<p><strong>图 2：</strong> 我们的神经辐射场场景表示和可微渲染流程概述。我们通过沿相机光线(a)采样五维坐标（位置和观察方向）来合成图像，将这些位置输入MLP以产生颜色和体积密度(b)，并使用体渲染技术将这些值合成到图像中(c)。此渲染函数是可微的，因此我们可以通过最小化合成图像与真实观测图像(d)之间的残差来优化场景表示。</p>
</div>

<div class="original">
<p>direction as a 3D Cartesian unit vector \(\mathbf{d}\). We approximate this continuous 5D scene representation with an MLP network \(F_{\Theta} : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)\) and optimize its weights \(\Theta\) to map from each input 5D coordinate to its corresponding volume density and directional emitted color.</p>
</div>
<div class="translation">
<p>方向表示为三维笛卡尔单位向量 \(\mathbf{d}\)。我们使用MLP网络 \(F_{\Theta} : (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)\) 来近似这个连续的五维场景表示，并优化其权重 \(\Theta\) 以将每个输入的五维坐标映射到其对应的体积密度和方向发射颜色。</p>
</div>

<div class="original">
<p>We encourage the representation to be multiview consistent by restricting the network to predict the volume density \(\sigma\) as a function of only the location \(\mathbf{x}\), while allowing the RGB color \(\mathbf{c}\) to be predicted as a function of both location and viewing direction. To accomplish this, the MLP \(F_{\Theta}\) first processes the input 3D coordinate \(\mathbf{x}\) with 8 fully-connected layers (using ReLU activations and 256 channels per layer), and outputs \(\sigma\) and a 256-dimensional feature vector. This feature vector is then concatenated with the camera ray’s viewing direction and passed to one additional fully-connected layer (using a ReLU activation and 128 channels) that output the view-dependent RGB color.</p>
</div>
<div class="translation">
<p>我们通过限制网络仅根据位置 \(\mathbf{x}\) 预测体积密度 \(\sigma\)，同时允许RGB颜色 \(\mathbf{c}\) 根据位置和观察方向进行预测，来鼓励表示具有多视图一致性。为实现这一点，MLP \(F_{\Theta}\) 首先使用8个全连接层（使用ReLU激活函数，每层256个通道）处理输入的三维坐标 \(\mathbf{x}\)，并输出 \(\sigma\) 和一个256维特征向量。然后将此特征向量与相机光线的观察方向拼接，并传递到一个额外的全连接层（使用ReLU激活函数和128个通道），该层输出视角相关的RGB颜色。</p>
</div>

<div class="original">
<p>See Fig. 3 for an example of how our method uses the input viewing direction to represent non-Lambertian effects. As shown in Fig. 4, a model trained without view dependence (only \(\mathbf{x}\) as input) has difficulty representing specularities.</p>
</div>
<div class="translation">
<p>关于我们的方法如何使用输入观察方向来表示非朗伯效应的示例，请参见图3。如图4所示，在没有视角依赖（仅 \(\mathbf{x}\) 作为输入）的情况下训练的模型难以表示镜面反射。</p>
</div>

<div class="original">
<h2>4 Volume Rendering with Radiance Fields</h2>
<p>Our 5D neural radiance field represents a scene as the volume density and directional emitted radiance at any point in space. We render the color of any ray passing through the scene using principles from classical volume rendering [16]. The volume density \(\sigma(\mathbf{x})\) can be interpreted as the differential probability of a ray terminating at an infinitesimal particle at location \(\mathbf{x}\). The expected color \(C(\mathbf{r})\) of camera ray \(\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}\) with near and far bounds \(t_n\) and \(t_f\) is:</p>
<p>\[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt,\]</p>
<p>where \(T(t) = \exp\left(-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds\right)\). (1)</p>
</div>
<div class="translation">
<h2>4 使用辐射场进行体渲染</h2>
<p>我们的五维神经辐射场将场景表示为空间中任意点的体积密度和方向发射辐射亮度。我们使用经典体渲染[16]的原理来渲染穿过场景的任何光线的颜色。体积密度 \(\sigma(\mathbf{x})\) 可以解释为光线在位置 \(\mathbf{x}\) 处的无穷小粒子处终止的微分概率。具有近边界 \(t_n\) 和远边界 \(t_f\) 的相机光线 \(\mathbf{r}(t) = \mathbf{o} + t\mathbf{d}\) 的期望颜色 \(C(\mathbf{r})\) 为：</p>
<p>\[C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t), \mathbf{d})dt,\]</p>
<p>其中 \(T(t) = \exp\left(-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds\right)\)。 (1)</p>
</div>

<div class="original">
<p>6 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<p><strong>(a) View 1 (b) View 2 (c) Radiance Distributions</strong></p>
<p><strong>Fig. 3:</strong> A visualization of view-dependent emitted radiance. Our neural radiance field representation outputs RGB color as a 5D function of both spatial position \(\mathbf{x}\) and viewing direction \(\mathbf{d}\). Here, we visualize example directional color distributions for two spatial locations in our neural representation of the <em>Ship</em> scene. In (a) and (b), we show the appearance of two fixed 3D points from two different camera positions: one on the side of the ship (orange insets) and one on the surface of the water (blue insets). Our method predicts the changing specular appearance of these two 3D points, and in (c) we show how this behavior generalizes continuously across the whole hemisphere of viewing directions.</p>
</div>
<div class="translation">
<p>6 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<p><strong>(a) 视图1 (b) 视图2 (c) 辐射分布</strong></p>
<p><strong>图 3：</strong> 视角相关发射辐射亮度的可视化。我们的神经辐射场表示将RGB颜色输出为空间位置 \(\mathbf{x}\) 和观察方向 \(\mathbf{d}\) 的五维函数。这里，我们可视化了我们神经表示的<em>Ship</em>场景中两个空间位置的示例方向颜色分布。在(a)和(b)中，我们展示了从两个不同相机位置观察到的两个固定3D点的外观：一个在船的侧面（橙色插图），一个在水面上（蓝色插图）。我们的方法预测了这两个3D点变化的镜面外观，并在(c)中展示了这种行为如何在观察方向的整个半球上连续泛化。</p>
</div>

<div class="original">
<p>The function \(T(t)\) denotes the accumulated transmittance along the ray from \(t_n\) to \(t\), i.e., the probability that the ray travels from \(t_n\) to \(t\) without hitting any other particle. Rendering a view from our continuous neural radiance field requires estimating this integral \(C(\mathbf{r})\) for a camera ray traced through each pixel of the desired virtual camera.</p>
</div>
<div class="translation">
<p>函数 \(T(t)\) 表示沿光线从 \(t_n\) 到 \(t\) 的累积透射率，即光线从 \(t_n\) 传播到 \(t\) 而不撞击任何其他粒子的概率。从我们的连续神经辐射场渲染视图需要为穿过所需虚拟相机每个像素的光线估计此积分 \(C(\mathbf{r})\)。</p>
</div>

<div class="original">
<p>We numerically estimate this continuous integral using quadrature. Deterministic quadrature, which is typically used for rendering discretized voxel grids, would effectively limit our representation’s resolution because the MLP would only be queried at a fixed discrete set of locations. Instead, we use a stratified sampling approach where we partition \([t_n,t_f]\) into \(N\) evenly-spaced bins and then draw one sample uniformly at random from within each bin:</p>
<p>\[t_i \sim U\left[t_n + \frac{i-1}{N}(t_f - t_n), \, t_n + \frac{i}{N}(t_f - t_n)\right].\]</p>
</div>
<div class="translation">
<p>我们使用求积法数值估计此连续积分。确定性求积法通常用于渲染离散体素网格，它会有效限制我们表示的分辨率，因为MLP只会在固定的离散位置集上被查询。相反，我们使用分层采样方法，将 \([t_n,t_f]\) 划分为 \(N\) 个等间距的区间，然后从每个区间内均匀随机抽取一个样本：</p>
<p>\[t_i \sim U\left[t_n + \frac{i-1}{N}(t_f - t_n), \, t_n + \frac{i}{N}(t_f - t_n)\right].\]</p>
</div>

<div class="original">
<p>Although we use a discrete set of samples to estimate the integral, stratified sampling enables us to represent a continuous scene representation because it results in the MLP being evaluated at continuous positions over the course of optimization. We use these samples to estimate \(C(\mathbf{r})\) with the quadrature rule discussed in the volume rendering review by Max [26]:</p>
<p>\[\hat{C}(\mathbf{r}) = \sum_{i=1}^{N}T_i(1-\exp(-\sigma_i\delta_i))c_i, \quad \text{where } T_i = \exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right),\]</p>
<p>where \(\delta_i = t_{i+1} - t_i\) is the distance between adjacent samples. This function for calculating \(\hat{C}(\mathbf{r})\) from the set of \((c_i,\sigma_i)\) values is trivially differentiable and reduces to traditional alpha compositing with alpha values \(\alpha_i = 1 - \exp(-\sigma_i\delta_i)\).</p>
</div>
<div class="translation">
<p>尽管我们使用离散样本集来估计积分，但分层采样使我们能够表示连续的场景，因为它导致MLP在优化过程中在连续位置被评估。我们使用这些样本，采用Max在体渲染综述[26]中讨论的求积法则来估计 \(C(\mathbf{r})\)：</p>
<p>\[\hat{C}(\mathbf{r}) = \sum_{i=1}^{N}T_i(1-\exp(-\sigma_i\delta_i))c_i, \quad \text{其中 } T_i = \exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right),\]</p>
<p>其中 \(\delta_i = t_{i+1} - t_i\) 是相邻样本之间的距离。这个根据 \((c_i,\sigma_i)\) 值集合计算 \(\hat{C}(\mathbf{r})\) 的函数是平凡可微的，并且简化为具有alpha值 \(\alpha_i = 1 - \exp(-\sigma_i\delta_i)\) 的传统alpha合成。</p>
</div>

<div class="original">
<h2>5 Optimizing a Neural Radiance Field</h2>
<p>In the previous section we have described the core components necessary for modeling a scene as a neural radiance field and rendering novel views from this representation. However, we observe that these components are not sufficient for achieving state-of-the-art quality, as demonstrated in Section 6.4). We introduce two improvements to enable representing high-resolution complex scenes. The first is a positional encoding of the input coordinates that assists the MLP in representing high-frequency functions, and the second is a hierarchical sampling procedure that allows us to efficiently sample this high-frequency representation.</p>
</div>
<div class="translation">
<h2>5 优化神经辐射场</h2>
<p>在上一节中，我们描述了将场景建模为神经辐射场并从此表示渲染新视图所需的核心组件。然而，我们观察到这些组件不足以实现最先进的质量，如第6.4节所示。我们引入了两项改进以实现高分辨率复杂场景的表示。第一项是输入坐标的位置编码，帮助MLP表示高频函数；第二项是分层采样程序，使我们能够有效地采样此高频表示。</p>
</div>

<div class="original">
<h3>Positional encoding</h3>
<p>Despite the fact that neural networks are universal function approximators [14], we found that having the network \(F_{\Theta}\) directly operate on \(xyz\theta\phi\) input coordinates results in renderings that perform poorly at representing high-frequency variation in color and geometry. This is consistent with recent work by Rahaman <em>et al.</em> [35], which shows that deep networks are biased towards learning lower frequency functions. They additionally show that mapping the inputs to a higher dimensional space using high frequency functions before passing them to the network enables better fitting of data that contains high frequency variation.</p>
</div>
<div class="translation">
<h3>位置编码</h3>
<p>尽管神经网络是通用函数逼近器[14]，但我们发现让网络 \(F_{\Theta}\) 直接在 \(xyz\theta\phi\) 输入坐标上操作会导致渲染结果在表示颜色和几何的高频变化方面表现不佳。这与Rahaman <em>等人</em> [35] 最近的工作一致，该工作表明深度网络偏向于学习低频函数。他们还表明，在将输入传递给网络之前，使用高频函数将输入映射到更高维空间，能够更好地拟合包含高频变化的数据。</p>
</div>

<div class="original">
<p>We leverage these findings in the context of neural scene representations, and show that reformulating \(F_{\Theta}\) as a composition of two functions \(F_{\Theta}=F^{\prime}_{\Theta}\circ\gamma\), one learned and one not, significantly improves performance (see Fig. 4 and Table 2). Here \(\gamma\) is a mapping from \(\mathbb{R}\) into a higher dimensional space \(\mathbb{R}^{2L}\), and \(F^{\prime}_{\Theta}\) is still simply a regular MLP. Formally, the encoding function we use is:</p>
<p>\[\gamma(p)=\left(\sin\left(2^{0}\pi p\right),\cos\left(2^{0}\pi p\right),\cdots ,\sin\left(2^{L-1}\pi p\right),\cos\left(2^{L-1}\pi p\right)\right).\] (4)</p>
</div>
<div class="translation">
<p>我们在神经场景表示的背景下利用这些发现，并表明将 \(F_{\Theta}\) 重新表述为两个函数 \(F_{\Theta}=F^{\prime}_{\Theta}\circ\gamma\) 的复合（一个可学习，一个不可学习）可显著提高性能（见图4和表2）。这里 \(\gamma\) 是从 \(\mathbb{R}\) 到更高维空间 \(\mathbb{R}^{2L}\) 的映射，而 \(F^{\prime}_{\Theta}\) 仍然只是一个常规的MLP。形式上，我们使用的编码函数是：</p>
<p>\[\gamma(p)=\left(\sin\left(2^{0}\pi p\right),\cos\left(2^{0}\pi p\right),\cdots ,\sin\left(2^{L-1}\pi p\right),\cos\left(2^{L-1}\pi p\right)\right).\] (4)</p>
</div>

<div class="original">
<p>This function \(\gamma(\cdot)\) is applied separately to each of the three coordinate values in \(\mathbf{x}\) (which are normalized to lie in \([-1,1]\)) and to the three components of the</p>
</div>
<div class="translation">
<p>此函数 \(\gamma(\cdot)\) 分别应用于 \(\mathbf{x}\) 中的三个坐标值（它们被归一化到 \([-1,1]\) 范围内）以及</p>
</div>

<div class="original">
<strong>Figure 4:</strong> Here we visualize how our full model benefits from representing view-dependent emitted radiance and from passing our input coordinates through a high-frequency positional encoding. Removing view dependence prevents the model from recreating the specular reflection on the bulldozer tread. Removing the positional encoding drastically decreases the model’s ability to represent high frequency geometry and texture, resulting in an oversmoothed appearance.
</div>
<div class="translation">
<strong>图 4：</strong> 这里我们可视化了完整模型如何受益于表示视角相关发射辐射亮度以及通过高频位置编码传递输入坐标。移除视角依赖会阻止模型重建推土机履带上的镜面反射。移除位置编码会大幅降低模型表示高频几何和纹理的能力，导致外观过度平滑。
</div>

<div class="original">
<p>Cartesian viewing direction unit vector \(\mathbf{d}\) (which by construction lie in \([-1,1]\)). In our experiments, we set \(L=10\) for \(\gamma(\mathbf{x})\) and \(L=4\) for \(\gamma(\mathbf{d})\).</p>
</div>
<div class="translation">
<p>笛卡尔观察方向单位向量 \(\mathbf{d}\) 的三个分量（根据构造位于 \([-1,1]\) 内）。在我们的实验中，对于 \(\gamma(\mathbf{x})\) 我们设置 \(L=10\)，对于 \(\gamma(\mathbf{d})\) 设置 \(L=4\)。</p>
</div>

<div class="original">
<p>A similar mapping is used in the popular Transformer architecture [47], where it is referred to as a <em>positional encoding</em>. However, Transformers use it for a different goal of providing the discrete positions of tokens in a sequence as input to an architecture that does not contain any notion of order. In contrast, we use these functions to map continuous input coordinates into a higher dimensional space to enable our MLP to more easily approximate a higher frequency function. Concurrent work on a related problem of modeling 3D protein structure from projections [51] also utilizes a similar input coordinate mapping.</p>
</div>
<div class="translation">
<p>流行的Transformer架构[47]中使用了类似的映射，在那里它被称为<em>位置编码</em>。然而，Transformer使用它是为了不同的目的：为序列中标记的离散位置提供输入给一个不包含任何顺序概念的架构。相比之下，我们使用这些函数将连续输入坐标映射到更高维空间，使我们的MLP更容易逼近更高频的函数。在从投影建模3D蛋白质结构的相关问题上，同时期的工作[51]也利用了类似的输入坐标映射。</p>
</div>

<div class="original">
<h3>Hierarchical volume sampling</h3>
<p>Our rendering strategy of densely evaluating the neural radiance field network at \(N\) query points along each camera ray is inefficient: free space and occluded regions that do not contribute to the rendered image are still sampled repeatedly. We draw inspiration from early work in volume rendering [20] and propose a hierarchical representation that increases rendering efficiency by allocating samples proportionally to their expected effect on the final rendering.</p>
</div>
<div class="translation">
<h3>分层体采样</h3>
<p>我们在每条相机光线上的 \(N\) 个查询点密集评估神经辐射场网络的渲染策略是低效的：对渲染图像没有贡献的自由空间和被遮挡区域仍然被重复采样。我们从体渲染的早期工作[20]中汲取灵感，提出了一种分层表示，通过按样本对最终渲染的预期影响比例分配样本，来提高渲染效率。</p>
</div>

<div class="original">
<p>Instead of just using a single network to represent the scene, we simultaneously optimize two networks: one "coarse" and one "fine". We first sample a set of \(N_{c}\) locations using stratified sampling, and evaluate the "coarse" network at these locations as described in Eqns. 2 and 3. Given the output of this "coarse" network, we then produce a more informed sampling of points along each ray where samples are biased towards the relevant parts of the volume. To do this, we first rewrite the alpha composited color from the coarse network \(\hat{C}_{c}(\mathbf{r})\) in Eqn. 3 as a weighted sum of all sampled colors \(c_{i}\) along the ray:</p>
<p>\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}}w_{i}c_{i}\,,\quad\ w_{i}=T_{i}(1-\exp( -\sigma_{i}\delta_{i}))\,.\] (5)</p>
</div>
<div class="translation">
<p>我们不是仅使用单个网络来表示场景，而是同时优化两个网络：一个“粗糙”网络和一个“精细”网络。我们首先使用分层采样抽取一组 \(N_{c}\) 个位置，并如公式2和3所述在这些位置评估“粗糙”网络。给定这个“粗糙”网络的输出，我们随后沿着每条光线生成一个更有信息量的点采样，其中样本偏向于体积的相关部分。为此，我们首先将公式3中来自粗糙网络 \(\hat{C}_{c}(\mathbf{r})\) 的alpha合成颜色重写为沿光线所有采样颜色 \(c_{i}\) 的加权和：</p>
<p>\[\hat{C}_{c}(\mathbf{r})=\sum_{i=1}^{N_{c}}w_{i}c_{i}\,,\quad\ w_{i}=T_{i}(1-\exp( -\sigma_{i}\delta_{i}))\,.\] (5)</p>
</div>

<div class="original">
<p>Normalizing these weights as \(\hat{w}_{i}=w_{i}/\sum_{j=1}^{N_{c}}w_{j}\) produces a piecewise-constant PDF along the ray. We sample a second set of \(N_{f}\) locations from this distribution using inverse transform sampling, evaluate our "fine" network at the union of the first and second set of samples, and compute the final rendered color of the ray \(\hat{C}_{f}(\mathbf{r})\) using Eqn. 3 but using all \(N_{c}+N_{f}\) samples. This procedure allocates more samples to regions we expect to contain visible content. This addresses a similar goal as importance sampling, but we use the sampled values as a nonuniform discretization of the whole integration domain rather than treating each sample as an independent probabilistic estimate of the entire integral.</p>
</div>
<div class="translation">
<p>将这些权重归一化为 \(\hat{w}_{i}=w_{i}/\sum_{j=1}^{N_{c}}w_{j}\) 会沿光线产生一个分段常数概率密度函数（PDF）。我们使用逆变换采样从此分布中抽取第二组 \(N_{f}\) 个位置，在第一组和第二组样本的并集上评估我们的“精细”网络，并使用公式3但使用所有 \(N_{c}+N_{f}\) 个样本来计算光线的最终渲染颜色 \(\hat{C}_{f}(\mathbf{r})\)。此过程将更多样本分配给我们预期包含可见内容的区域。这与重要性采样的目标类似，但我们使用采样值作为整个积分域的非均匀离散化，而不是将每个样本视为整个积分的独立概率估计。</p>
</div>

<div class="original">
<h3>Implementation details</h3>
<p>We optimize a separate neural continuous volume representation network for each scene. This requires only a dataset of captured RGB images of the scene,</p>
</div>
<div class="translation">
<h3>实现细节</h3>
<p>我们为每个场景优化一个独立的神经连续体积表示网络。这只需要场景的捕获RGB图像数据集、</p>
</div>

<div class="original">
<p>the corresponding camera poses and intrinsic parameters, and scene bounds (we use ground truth camera poses, intrinsics, and bounds for synthetic data, and use the COLMAP structure-from-motion package [39] to estimate these parameters for real data). At each optimization iteration, we randomly sample a batch of camera rays from the set of all pixels in the dataset, and then follow the hierarchical sampling described in Sec. 5.2 to query \(N_c\) samples from the coarse network and \(N_c + N_f\) samples from the fine network. We then use the volume rendering procedure described in Sec. 4 to render the color of each ray from both sets of samples. Our loss is simply the total squared error between the rendered and true pixel colors for both the coarse and fine renderings:</p>
<p>\[\mathcal{L} = \sum_{\mathbf{r}\in\mathcal{R}}\left[\left\|\hat{C}_c(\mathbf{r}) - C(\mathbf{r})\right\|_2^2 + \left\|\hat{C}_f(\mathbf{r}) - C(\mathbf{r})\right\|_2^2\right]\] (6)</p>
</div>
<div class="translation">
<p>相应的相机位姿和内在参数，以及场景边界（对于合成数据我们使用真实相机位姿、内参和边界，对于真实数据我们使用COLMAP运动恢复结构包[39]来估计这些参数）。在每次优化迭代中，我们从数据集所有像素的集合中随机采样一批相机光线，然后按照第5.2节描述的分层采样，从粗糙网络查询 \(N_c\) 个样本，从精细网络查询 \(N_c + N_f\) 个样本。然后，我们使用第4节描述的体渲染流程从两组样本中渲染每条光线的颜色。我们的损失函数就是粗糙渲染和精细渲染的渲染像素颜色与真实像素颜色之间的总平方误差：</p>
<p>\[\mathcal{L} = \sum_{\mathbf{r}\in\mathcal{R}}\left[\left\|\hat{C}_c(\mathbf{r}) - C(\mathbf{r})\right\|_2^2 + \left\|\hat{C}_f(\mathbf{r}) - C(\mathbf{r})\right\|_2^2\right]\] (6)</p>
</div>

<div class="original">
<p>where \(\mathcal{R}\) is the set of rays in each batch, and \(C(\mathbf{r})\), \(\hat{C}_c(\mathbf{r})\), and \(\hat{C}_f(\mathbf{r})\) are the ground truth, coarse volume predicted, and fine volume predicted RGB colors for ray \(\mathbf{r}\) respectively. Note that even though the final rendering comes from \(\hat{C}_f(\mathbf{r})\), we also minimize the loss of \(\hat{C}_c(\mathbf{r})\) so that the weight distribution from the coarse network can be used to allocate samples in the fine network.</p>
</div>
<div class="translation">
<p>其中 \(\mathcal{R}\) 是每批中的光线集合，\(C(\mathbf{r})\), \(\hat{C}_c(\mathbf{r})\) 和 \(\hat{C}_f(\mathbf{r})\) 分别是光线 \(\mathbf{r}\) 的真实RGB颜色、粗糙体积预测的RGB颜色和精细体积预测的RGB颜色。请注意，即使最终渲染来自 \(\hat{C}_f(\mathbf{r})\)，我们也最小化 \(\hat{C}_c(\mathbf{r})\) 的损失，以便来自粗糙网络的权重分布可以用于在精细网络中分配样本。</p>
</div>

<div class="original">
<p>In our experiments, we use a batch size of 4096 rays, each sampled at \(N_c = 64\) coordinates in the coarse volume and \(N_f = 128\) additional coordinates in the fine volume. We use the Adam optimizer [18] with a learning rate that begins at \(5 \times 10^{-4}\) and decays exponentially to \(5 \times 10^{-5}\) over the course of optimization (other Adam hyperparameters are left at default values of \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), and \(\epsilon = 10^{-7}\)). The optimization for a single scene typically take around 100-300k iterations to converge on a single NVIDIA V100 GPU (about 1-2 days).</p>
</div>
<div class="translation">
<p>在我们的实验中，我们使用4096条光线作为批量大小，每条光线在粗糙体积中采样 \(N_c = 64\) 个坐标，在精细体积中额外采样 \(N_f = 128\) 个坐标。我们使用Adam优化器[18]，学习率从 \(5 \times 10^{-4}\) 开始，并在优化过程中指数衰减到 \(5 \times 10^{-5}\)（其他Adam超参数保持默认值 \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), 和 \(\epsilon = 10^{-7}\))。优化单个场景通常需要约100-300k次迭代才能在单个NVIDIA V100 GPU上收敛（约1-2天）。</p>
</div>

<div class="original">
<h2>6 Results</h2>
<p>We quantitatively (Tables 1) and qualitatively (Figs. 8 and 6) show that our method outperforms prior work, and provide extensive ablation studies to validate our design choices (Table 2). We urge the reader to view our supplementary video to better appreciate our method's significant improvement over baseline methods when rendering smooth paths of novel views.</p>
</div>
<div class="translation">
<h2>6 结果</h2>
<p>我们在定量上（表1）和定性上（图8和图6）展示了我们的方法优于先前的工作，并提供了广泛的消融研究以验证我们的设计选择（表2）。我们强烈建议读者观看我们的补充视频，以更好地理解在渲染新视图的平滑路径时，我们的方法相对于基线方法的显著改进。</p>
</div>

<div class="original">
<h3>Datasets</h3>
<p><em>Synthetic renderings of objects</em> We first show experimental results on two datasets of synthetic renderings of objects (Table 1, "Diffuse Synthetic 360" and "Realistic Synthetic 360"). The DeepVoxels [41] dataset contains four Lambertian objects with simple geometry. Each object is rendered at \(512 \times 512\) pixels from viewpoints sampled on the upper hemisphere (\(479\) as input and \(1000\) for testing). We additionally generate our own dataset containing pathtraced images of eight objects that exhibit complicated geometry and realistic non-Lambertian materials. Six are rendered from viewpoints sampled on the upper hemisphere, and two are rendered from viewpoints sampled on a full sphere. We render 100 views of each scene as input and 200 for testing, all at \(800 \times 800\) pixels.</p>
</div>
<div class="translation">
<h3>数据集</h3>
<p><em>物体的合成渲染</em> 我们首先展示在两个物体合成渲染数据集上的实验结果（表1，“Diffuse Synthetic 360” 和 “Realistic Synthetic 360”）。DeepVoxels [41] 数据集包含四个具有简单几何形状的朗伯物体。每个物体在 \(512 \times 512\) 像素分辨率下渲染，视点在上半球采样（479个作为输入，1000个用于测试）。我们额外生成了自己的数据集，包含八个物体的路径追踪图像，这些物体具有复杂的几何形状和真实的非朗伯材质。其中六个从上半球采样的视点渲染，两个从整个球体采样的视点渲染。我们为每个场景渲染100个视图作为输入，200个用于测试，分辨率均为 \(800 \times 800\) 像素。</p>
</div>

<div class="original">
<p>10 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<table>
<thead>
<tr><th>Method</th><th colspan="3">Diffuse Synthetic 360° [41]</th><th colspan="3">Realistic Synthetic 360°</th><th colspan="3">Real Forward-Facing [28]</th></tr>
<tr><th></th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th></tr>
</thead>
<tbody>
<tr><td>SRN [42]</td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr>
<tr><td>NV [24]</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>LLFF [28]</td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr>
<tr><td>Ours</td><td><strong>40.15</strong></td><td>0.991</td><td><strong>0.023</strong></td><td><strong>31.01</strong></td><td><strong>0.947</strong></td><td><strong>0.081</strong></td><td><strong>26.50</strong></td><td><strong>0.811</strong></td><td><strong>0.250</strong></td></tr>
</tbody>
</table>
<p><strong>Table 1:</strong> Our method quantitatively outperforms prior work on datasets of both synthetic and real images. We report PSNR/SSIM (higher is better) and LPIPS [50] (lower is better). The DeepVoxels [41] dataset consists of 4 diffuse objects with simple geometry. Our realistic synthetic dataset consists of pathtraced renderings of 8 geometrically complex objects with complex non-Lambertian materials. The real dataset consists of handheld forward-facing captures of 8 real-world scenes (NV cannot be evaluated on this data because it only reconstructs objects inside a bounded volume). Though LLFF achieves slightly better LPIPS, we urge readers to view our supplementary video where our method achieves better multiview consistency and produces fewer artifacts than all baselines.</p>
</div>
<div class="translation">
<p>10 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<table>
<thead>
<tr><th>方法</th><th colspan="3">Diffuse Synthetic 360° [41]</th><th colspan="3">Realistic Synthetic 360°</th><th colspan="3">Real Forward-Facing [28]</th></tr>
<tr><th></th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th><th>PSNR↑</th><th>SSIM↑</th><th>LPIPS↓</th></tr>
</thead>
<tbody>
<tr><td>SRN [42]</td><td>33.20</td><td>0.963</td><td>0.073</td><td>22.26</td><td>0.846</td><td>0.170</td><td>22.84</td><td>0.668</td><td>0.378</td></tr>
<tr><td>NV [24]</td><td>29.62</td><td>0.929</td><td>0.099</td><td>26.05</td><td>0.893</td><td>0.160</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>LLFF [28]</td><td>34.38</td><td>0.985</td><td>0.048</td><td>24.88</td><td>0.911</td><td>0.114</td><td>24.13</td><td>0.798</td><td>0.212</td></tr>
<tr><td>Ours</td><td><strong>40.15</strong></td><td>0.991</td><td><strong>0.023</strong></td><td><strong>31.01</strong></td><td><strong>0.947</strong></td><td><strong>0.081</strong></td><td><strong>26.50</strong></td><td><strong>0.811</strong></td><td><strong>0.250</strong></td></tr>
</tbody>
</table>
<p><strong>表 1：</strong> 我们的方法在合成和真实图像数据集上定量优于先前的工作。我们报告了PSNR/SSIM（越高越好）和LPIPS[50]（越低越好）。DeepVoxels[41]数据集包含4个具有简单几何形状的漫反射物体。我们的真实感合成数据集包含8个具有复杂非朗伯材质的几何复杂物体的路径追踪渲染。真实数据集包含8个真实世界场景的手持前向捕捉（NV无法在此数据上评估，因为它只重建有界体积内的物体）。尽管LLFF在LPIPS上略优，但我们强烈建议读者观看我们的补充视频，其中我们的方法实现了更好的多视图一致性，并且产生的伪影少于所有基线。</p>
</div>

<div class="original">
<p>Real images of complex scenes We show results on complex real-world scenes captured with roughly forward-facing images (Table 1, “Real Forward-Facing”). This dataset consists of 8 scenes captured with a handheld cellphone (5 taken from the LLFF paper and 3 that we capture), captured with 20 to 62 images, and hold out \( \frac{1}{8} \) of these for the test set. All images are \( 1008 \times 756 \) pixels.</p>
</div>
<div class="translation">
<p>复杂场景的真实图像 我们在使用大致前向图像捕获的复杂真实世界场景上展示了结果（表1，“Real Forward-Facing”）。该数据集包含8个使用手持手机捕获的场景（5个来自LLFF论文，3个由我们捕获），捕获了20至62张图像，并保留其中的 \( \frac{1}{8} \) 作为测试集。所有图像均为 \( 1008 \times 756 \) 像素。</p>
</div>

<div class="original">
<h3>6.2 Comparisons</h3>
<p>To evaluate our model we compare against current top-performing techniques for view synthesis, detailed below. All methods use the same set of input views to train a separate network for each scene except Local Light Field Fusion [28], which trains a single 3D convolutional network on a large dataset, then uses the same trained network to process input images of new scenes at test time.</p>
</div>
<div class="translation">
<h3>6.2 比较</h3>
<p>为了评估我们的模型，我们与当前性能最佳的视图合成技术进行比较，详情如下。所有方法都使用相同的输入视图集为每个场景训练一个单独的网络，除了局部光场融合（Local Light Field Fusion，LLFF）[28]，它在一个大型数据集上训练一个单一的3D卷积网络，然后在测试时使用相同的训练网络处理新场景的输入图像。</p>
</div>

<div class="original">
<p><strong>Neural Volumes (NV) [24]</strong> synthesizes novel views of objects that lie entirely within a bounded volume in front of a distinct background (which must be separately captured without the object of interest). It optimizes a deep 3D convolutional network to predict a discretized RGB\(\alpha\) voxel grid with \(128^3\) samples as well as a 3D warp grid with \(32^3\) samples. The algorithm renders novel views by marching camera rays through the warped voxel grid.</p>
</div>
<div class="translation">
<p><strong>神经体积（Neural Volumes, NV）[24]</strong> 合成完全位于不同背景前方有界体积内的物体的新视图（该背景必须在没有目标物体的情况下单独捕获）。它优化一个深度3D卷积网络来预测一个具有 \(128^3\) 样本的离散化RGB\(\alpha\)体素网格以及一个具有 \(32^3\) 样本的3D扭曲网格。该算法通过在扭曲的体素网格中步进相机光线来渲染新视图。</p>
</div>

<div class="original">
<p><strong>Scene Representation Networks (SRN) [42]</strong> represent a continuous scene as an opaque surface, implicitly defined by a MLP that maps each \((x,y,z)\) coordinate to a feature vector. They train a recurrent neural network to march along a ray through the scene representation by using the feature vector at any 3D coordinate to predict the next step size along the ray. The feature vector from the final step is decoded into a single color for that point on the surface. Note that SRN is a better-performing followup to DeepVoxels [41] by the same authors, which is why we do not include comparisons to DeepVoxels.</p>
</div>
<div class="translation">
<p><strong>场景表示网络（Scene Representation Networks, SRN）[42]</strong> 将连续场景表示为一个不透明表面，该表面由一个MLP隐式定义，该MLP将每个 \((x,y,z)\) 坐标映射到一个特征向量。他们训练一个循环神经网络，通过使用任何3D坐标处的特征向量预测沿光线的下一步步长，沿着光线穿过场景表示。最后一步的特征向量被解码为该表面点处的单一颜色。请注意，SRN是同一作者对DeepVoxels[41]的改进版本，性能更好，因此我们不包含与DeepVoxels的比较。</p>
</div>

<div class="original">
<h1>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</h1>
<p><strong>Ship<br>Lego</strong></p>
<p><strong>Microphone<br>Materials</strong></p>
<p><strong>Ground Truth  NeRF (ours)   LLFF [28]   SRN [42]   NV [24]</strong></p>
<p><strong>Fig. 5:</strong> Comparisons on test-set views for scenes from our new synthetic dataset generated with a physically-based renderer. Our method is able to recover fine details in both geometry and appearance, such as Ship’s rigging, Lego’s gear and treads, Microphone’s shiny stand and mesh grille, and Material’s non-Lambertian reflectance. LLFF exhibits banding artifacts on the Microphone stand and Material’s object edges and ghosting artifacts in Ship’s mast and inside the Lego object. SRN produces blurry and distorted renderings in every case. Neural Volumes cannot capture the details on the Microphone’s grille or Lego’s gears, and it completely fails to recover the geometry of Ship’s rigging.</p>
</div>
<div class="translation">
<h1>NeRF：将场景表示为神经辐射场以实现视图合成</h1>
<p><strong>Ship<br>Lego</strong></p>
<p><strong>Microphone<br>Materials</strong></p>
<p><strong>真实值  NeRF (我们的)   LLFF [28]   SRN [42]   NV [24]</strong></p>
<p><strong>图 5：</strong> 在我们新的基于物理渲染器生成的合成数据集场景的测试集视图上的比较。我们的方法能够恢复几何和外观的精细细节，例如Ship的索具、Lego的齿轮和履带、Microphone闪亮的支架和网状格栅，以及Material的非朗伯反射。LLFF在Microphone支架和Material物体边缘上表现出带状伪影，在Ship的桅杆和Lego物体内部表现出重影伪影。SRN在所有情况下都产生模糊和扭曲的渲染。神经体积（NV）无法捕捉Microphone格栅或Lego齿轮的细节，并且完全无法恢复Ship索具的几何形状。</p>
</div>

<div class="original">
<p>12 B. Mildenhall, P. P. Srinivasan, M. Tancik et al.</p>
<p><strong>Fern</strong></p>
<p><strong>T-Rex</strong></p>
<p><strong>Orchid</strong></p>
<p><strong>Ground Truth    NeRF (ours)    LLFF [28]    SRN [42]</strong></p>
<p><strong>Fig. 6:</strong> Comparisons on test-set views of real world scenes. LLFF is specifically designed for this use case (forward-facing captures of real scenes). Our method is able to represent fine geometry more consistently across rendered views than LLFF, as shown in Fern’s leaves and the skeleton ribs and railing in T-rex. Our method also correctly reconstructs partially occluded regions that LLFF struggles to render cleanly, such as the yellow shelves behind the leaves in the bottom Fern crop and green leaves in the background of the bottom Orchid crop. Blending between multiples renderings can also cause repeated edges in LLFF, as seen in the top Orchid crop. SRN captures the low-frequency geometry and color variation in each scene but is unable to reproduce any fine detail.</p>
</div>
<div class="translation">
<p>12 B. 米尔登霍尔, P. P. 斯里尼瓦桑, M. 坦西克 等</p>
<p><strong>Fern</strong></p>
<p><strong>T-Rex</strong></p>
<p><strong>Orchid</strong></p>
<p><strong>真实值    NeRF (我们的)    LLFF [28]    SRN [42]</strong></p>
<p><strong>图 6：</strong> 真实世界场景测试集视图上的比较。LLFF是专为这种用例设计的（真实场景的前向捕捉）。我们的方法能够在渲染视图之间比LLFF更一致地表示精细几何结构，如Fern的叶子和T-rex中的骨架肋骨和栏杆所示。我们的方法还能正确重建LLFF难以清晰渲染的部分遮挡区域，例如底部Fern裁剪图中叶子后面的黄色架子和底部Orchid裁剪图中背景的绿叶。在多个渲染之间混合也会导致LLFF中出现重复边缘，如顶部Orchid裁剪图所示。SRN捕捉了每个场景的低频几何结构和颜色变化，但无法再现任何精细细节。</p>
</div>

<div class="original">
<p><strong>Local Light Field Fusion (LLFF) [28]</strong> LLFF is designed for producing photorealistic novel views for well-sampled forward facing scenes. It uses a trained 3D convolutional network to directly predict a discretized frustum-sampled RGB\(\alpha\) grid (multiphane image or MPI [52]) for each input view, then renders novel views by alpha compositing and blending nearby MPIs into the novel viewpoint.</p>
</div>
<div class="translation">
<p><strong>局部光场融合（Local Light Field Fusion, LLFF）[28]</strong> LLFF专为采样良好的前向场景生成光照真实的新视图而设计。它使用训练好的3D卷积网络直接为每个输入视图预测一个离散的视锥采样RGB\(\alpha\)网格（多平面图像或MPI[52]），然后通过alpha合成并将附近的MPI混合到新视点来渲染新视图。</p>
</div>

<div class="original">
<h3>Discussion</h3>
<p>We thoroughly outperform both baselines that also optimize a separate network per scene (NV and SRN) in all scenarios. Furthermore, we produce qualitatively and quantitatively superior renderings compared to LLFF (across all except one metric) while using only their input images as our entire training set.</p>
</div>
<div class="translation">
<h3>讨论</h3>
<p>在所有场景中，我们都全面优于同样为每个场景优化单独网络（NV和SRN）的基线。此外，与LLFF相比，我们产生了定性和定量上更优的渲染结果（除一项指标外），同时仅使用其输入图像作为我们整个训练集。</p>
</div>

<div class="original">
<p>The SRN method produces heavily smoothed geometry and texture, and its representational power for view synthesis is limited by selecting only a single depth and color per camera ray. The NV baseline is able to capture reasonably detailed volumetric geometry and appearance, but its use of an underlying explicit \(128^{3}\) voxel grid prevents it from scaling to represent fine details at high resolutions. LLFF specifically provides a "sampling guideline" to not exceed 64 pixels of disparity between input views, so it frequently fails to estimate correct geometry in the synthetic datasets which contain up to 400-500 pixels of disparity between views. Additionally, LLFF blends between different scene representations for rendering different views, resulting in perceptually-distracting inconsistency as is apparent in our supplementary video.</p>
</div>
<div class="translation">
<p>SRN方法产生严重平滑的几何和纹理，其视图合成的表示能力受限于每条相机光线仅选择一个深度和颜色。NV基线能够捕捉相当详细的体积几何和外观，但其使用的底层显式 \(128^{3}\) 体素网格阻止了它扩展到高分辨率下表示精细细节。LLFF特别提供了“采样指南”，要求输入视图之间的视差不超64像素，因此在合成数据集（视图间视差高达400-500像素）中经常无法估计正确的几何结构。此外，LLFF在渲染不同视图时混合不同的场景表示，导致感知上分散注意力的不一致性，这在我们的补充视频中很明显。</p>
</div>

<div class="original">
<p>The biggest practical tradeoffs between these methods are time versus space. All compared single scene methods take at least 12 hours to train per scene. In contrast, LLFF can process a small input dataset in under 10 minutes. However, LLFF produces a large 3D voxel grid for every input image, resulting in enormous storage requirements (over 15GB for one "Realistic Synthetic" scene). Our method requires only 5 MB for the network weights (a relative compression of \(3000\times\) compared to LLFF), which is even less memory than the <em>input images alone</em> for a single scene from any of our datasets.</p>
</div>
<div class="translation">
<p>这些方法之间最大的实际权衡是时间与空间。所有比较的单场景方法每个场景至少需要12小时来训练。相比之下，LLFF可以在10分钟内处理一个小的输入数据集。然而，LLFF为每个输入图像生成一个大的3D体素网格，导致巨大的存储需求（一个“Realistic Synthetic”场景超过15GB）。我们的方法仅需5MB用于网络权重（相对于LLFF有 \(3000\times\) 的相对压缩），这甚至少于我们任何数据集中单个场景<em>仅输入图像</em>所需的内存。</p>
</div>

<div class="original">
<h3>Ablation studies</h3>
<p>We validate our algorithm's design choices and parameters with an extensive ablation study in Table 2. We present results on our "Realistic Synthetic 360\(^{\circ}\)" scenes. Row 9 shows our complete model as a point of reference. Row 1 shows a minimalist version of our model without positional encoding (PE), view-dependence (VD), or hierarchical sampling (H). In rows 2-4 we remove these three components one at a time from the full model, observing that positional encoding (row 2) and view-dependence (row 3) provide the largest quantitative benefit followed by hierarchical sampling (row 4). Rows 5-6 show how our performance decreases as the number of input images is reduced. Note that our method's performance using only 25 input images still exceeds NV, SRN, and LLFF across all metrics when they are provided with 100 images (see supplementary material). In rows 7-8 we validate our choice of the maximum frequency</p>
</div>
<div class="translation">
<h3>消融研究</h3>
<p>我们在表2中通过广泛的消融研究验证了我们算法的设计选择和参数。我们在“Realistic Synthetic 360\(^{\circ}\)”场景上展示结果。第9行展示了我们完整的模型作为参考点。第1行展示了一个没有位置编码（PE）、视角依赖（VD）或分层采样（H）的极简版本模型。在第2-4行，我们从完整模型中依次移除这三个组件，观察到位置编码（第2行）和视角依赖（第3行）提供了最大的定量收益，其次是分层采样（第4行）。第5-6行展示了随着输入图像数量减少，我们的性能如何下降。请注意，当仅使用25张输入图像时，我们的方法在所有指标上的性能仍然超过使用100张图像的NV、SRN和LLFF（参见补充材料）。在第7-8行，我们验证了我们对最大频率的选择</p>
</div>

<div class="original">
<p>\begin{tabular}{l|l l l l l l}  & Input & \#Im. & \(L\) & \((N_c, N_f)\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline
1) No PE, VD, H & \(xyz\) & 100 & - & (256, -) & 26.67 & 0.906 & 0.136 \\
2) No Pos. Encoding & \(xyz\theta\phi\) & 100 & - & (64, 128) & 28.77 & 0.924 & 0.108 \\
3) No View Dependence & \(xyz\) & 100 & 10 & (64, 128) & 27.66 & 0.925 & 0.117 \\
4) No Hierarchical & \(xyz\theta\phi\) & 100 & 10 & (256, -) & 30.06 & 0.938 & 0.109 \\
5) Far Fewer Images & \(xyz\theta\phi\) & 25 & 10 & (64, 128) & 27.78 & 0.925 & 0.107 \\
6) Fewer Images & \(xyz\theta\phi\) & 50 & 10 & (64, 128) & 29.79 & 0.940 & 0.096 \\
7) Fewer Frequencies & \(xyz\theta\phi\) & 100 & 5 & (64, 128) & 30.59 & 0.944 & 0.088 \\
8) More Frequencies & \(xyz\theta\phi\) & 100 & 15 & (64, 128) & 30.81 & 0.946 & 0.096 \\
9) Complete Model & \(xyz\theta\phi\) & 100 & 10 & (64, 128) & \textbf{31.01} & \textbf{0.947} & \textbf{0.081} \\ \end{tabular}</p>
<p><strong>Table 2:</strong> An ablation study of our model. Metrics are averaged over the 8 scenes from our realistic synthetic dataset. See Sec. 6.4 for detailed descriptions.</p>
</div>
<div class="translation">
<p>\begin{tabular}{l|l l l l l l}  & 输入 & \#图像数 & \(L\) & \((N_c, N_f)\) & PSNR\(\uparrow\) & SSIM\(\uparrow\) & LPIPS\(\downarrow\) \\ \hline
1) 无 PE, VD, H & \(xyz\) & 100 & - & (256, -) & 26.67 & 0.906 & 0.136 \\
2) 无位置编码 & \(xyz\theta\phi\) & 100 & - & (64, 128) & 28.77 & 0.924 & 0.108 \\
3) 无视角依赖 & \(xyz\) & 100 & 10 & (64, 128) & 27.66 & 0.925 & 0.117 \\
4) 无分层采样 & \(xyz\theta\phi\) & 100 & 10 & (256, -) & 30.06 & 0.938 & 0.109 \\
5) 极少图像 & \(xyz\theta\phi\) & 25 & 10 & (64, 128) & 27.78 & 0.925 & 0.107 \\
6) 较少图像 & \(xyz\theta\phi\) & 50 & 10 & (64, 128) & 29.79 & 0.940 & 0.096 \\
7) 较少频率 & \(xyz\theta\phi\) & 100 & 5 & (64, 128) & 30.59 & 0.944 & 0.088 \\
8) 更多频率 & \(xyz\theta\phi\) & 100 & 15 & (64, 128) & 30.81 & 0.946 & 0.096 \\
9) 完整模型 & \(xyz\theta\phi\) & 100 & 10 & (64, 128) & \textbf{31.01} & \textbf{0.947} & \textbf{0.081} \\ \end{tabular}</p>
<p><strong>表 2：</strong> 我们模型的消融研究。指标在我们真实感合成数据集的8个场景上取平均值。详细描述见第6.4节。</p>
</div>

<div class="original">
<p>\(L\) used in our positional encoding for \(\mathbf{x}\) (the maximum frequency used for \(\mathbf{d}\) is scaled proportionally). Only using 5 frequencies reduces performance, but increasing the number of frequencies from 10 to 15 does not improve performance. We believe the benefit of increasing \(L\) is limited once \(2^{L}\) exceeds the maximum frequency present in the sampled input images (roughly 1024 in our data).</p>
</div>
<div class="translation">
<p>用于 \(\mathbf{x}\) 的位置编码中的 \(L\)（用于 \(\mathbf{d}\) 的最大频率按比例缩放）。仅使用5个频率会降低性能，但将频率数量从10增加到15并不能提高性能。我们认为一旦 \(2^{L}\) 超过采样输入图像中存在的最大频率（在我们的数据中约为1024），增加 \(L\) 的收益就有限了。</p>
</div>

<div class="original">
<h2>7 Conclusion</h2>
<p>Our work directly addresses deficiencies of prior work that uses MLPs to represent objects and scenes as continuous functions. We demonstrate that representing scenes as 5D neural radiance fields (an MLP that outputs volume density and view-dependent emitted radiance as a function of 3D location and 2D viewing direction) produces better renderings than the previously-dominant approach of training deep convolutional networks to output discretized voxel representations.</p>
</div>
<div class="translation">
<h2>7 结论</h2>
<p>我们的工作直接解决了先前使用MLP将物体和场景表示为连续函数的工作的不足。我们证明了将场景表示为五维神经辐射场（一个输出体积密度和视角相关发射辐射亮度作为3D位置和2D观察方向函数的MLP），相比于之前主导的通过训练深度卷积网络输出离散体素表示的方法，能够产生更好的渲染结果。</p>
</div>

<div class="original">
<p>Although we have proposed a hierarchical sampling strategy to make rendering more sample-efficient (for both training and testing), there is still much more progress to be made in investigating techniques to efficiently optimize and render neural radiance fields. Another direction for future work is interpretability: sampled representations such as voxel grids and meshes admit reasoning about the expected quality of rendered views and failure modes, but it is unclear how to analyze these issues when we encode scenes in the weights of a deep neural network. We believe that this work makes progress towards a graphics pipeline based on real world imagery, where complex scenes could be composed of neural radiance fields optimized from images of actual objects and scenes.</p>
</div>
<div class="translation">
<p>尽管我们提出了分层采样策略以使渲染更加样本高效（对于训练和测试都是如此），但在研究高效优化和渲染神经辐射场的技术方面仍有许多进展需要取得。未来工作的另一个方向是可解释性：像体素网格和网格这样的采样表示允许对渲染视图的预期质量和故障模式进行推理，但当我们把场景编码在深度神经网络的权重中时，如何分析这些问题尚不清楚。我们相信这项工作在实现基于真实世界图像的图形流水线方面取得了进展，其中复杂场景可以由实际物体和场景图像优化而来的神经辐射场组成。</p>
</div>

<div class="original">
<p><strong>Acknowledgements</strong> We thank Kevin Cao, Guowei Frank Yang, and Nithin Raghavan for comments and discussions. RR acknowledges funding from ONR grants N000141712687 and N000142012529 and the Ronald L. Graham Chair. BM is funded by a Hertz Foundation Fellowship, and MT is funded by an NSF Graduate Fellowship. Google provided a generous donation of cloud compute credits through the BAIR Commons program. We thank the following</p>
</div>
<div class="translation">
<p><strong>致谢</strong> 我们感谢Kevin Cao、Guowei Frank Yang和Nithin Raghavan的评论和讨论。RR感谢ONR基金N000141712687、N000142012529以及Ronald L. Graham讲席的资助。BM由Hertz基金会奖学金资助，MT由NSF研究生奖学金资助。谷歌通过BAIR Commons计划慷慨捐赠了云计算积分。我们感谢以下</p>
</div>

<div class="original">
<p>Blend Swap users for the models used in our realistic synthetic dataset: gregzaal (ship), 1DInc (chair), bryanajones (drums), Herberhold (ficus), erickfree (hotdog), Heinzelnisse (lego), elbrujodelatribu (materials), and up3d.de (mic).</p>
</div>
<div class="translation">
<p>Blend Swap用户提供我们真实感合成数据集使用的模型：gregzaal (ship), 1DInc (chair), bryanajones (drums), Herberhold (ficus), erickfree (hotdog), Heinzelnisse (lego), elbrujodelatribu (materials), 以及 up3d.de (mic)。</p>
</div>

<!-- 参考文献部分因篇幅原因省略翻译，保持原文 -->
<div class="original">
<h2>References</h2>
<p>[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015)</p>
<!-- ... 其他参考文献条目 ... -->
<p>[52] Zhou, T., Tucker, R., Flynn, J., Pyffe, G., Snavely, N.: Stereo magnification: Learning view synthesis using multiplane images. ACM Transactions on Graphics (SIGGRAPH) (2018)</p>
</div>
<div class="translation">
<h2>参考文献</h2>
<p>[1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow：异构系统上的大规模机器学习 (2015)</p>
<!-- ... 其他参考文献条目翻译（此处省略以节省空间）... -->
<p>[52] Zhou, T., Tucker, R., Flynn, J., Pyffe, G., Snavely, N.: 立体放大：使用多平面图像学习视图合成。 ACM Transactions on Graphics (SIGGRAPH) (2018)</p>
</div>

<!-- 附录部分省略翻译，保持原文 -->
<div class="original">
<h2>Appendix A Additional Implementation Details</h2>
<p><strong>Network Architecture</strong> Fig. 7 details our simple fully-connected architecture.</p>
<p><strong>Volume Bounds</strong> Our method renders views by querying the neural radiance field representation at continuous 5D coordinates along camera rays. For experiments with synthetic images, we scale the scene so that it lies within a cube of side length 2 centered at the origin, and only query the representation within this bounding volume. Our dataset of real images contains content that can exist anywhere between the closest point and infinity, so we use normalized device coordinates to map the depth range of these points into \([-1,1]\). This shifts all the ray origins to the near plane of the scene, maps the perspective rays of the camera to parallel rays in the transformed volume, and uses disparity (inverse depth) instead of metric depth, so all coordinates are now bounded.</p>
</div>
<div class="translation">
<h2>附录 A 附加实现细节</h2>
<p><strong>网络架构</strong> 图7详细说明了我们简单的全连接架构。</p>
<p><strong>体积边界</strong> 我们的方法通过沿相机光线在连续五维坐标上查询神经辐射场表示来渲染视图。对于合成图像的实验，我们将场景缩放，使其位于以原点为中心、边长为2的立方体内，并仅在此边界体积内查询表示。我们的真实图像数据集包含可能存在于最近点到无穷远之间任何位置的内容，因此我们使用归一化设备坐标（NDC）将这些点的深度范围映射到 \([-1,1]\)。这将所有光线起点移动到场景的近平面，将相机的透视光线映射到变换后体积中的平行光线，并使用视差（逆深度）代替度量深度，因此所有坐标现在都是有界的。</p>
</div>

<!-- ... 后续附录内容省略 ... -->

</body>
</html>